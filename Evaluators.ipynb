{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7aa4d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    0.2 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fedec771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5dbff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c16c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional, TransMotionDatasetConditional,VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b9dad",
   "metadata": {},
   "source": [
    "## VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "# train_loader = DATALoader(train_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8806cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.evaluator_dataset import EvaluatorMotionCollator, EvaluatorVarLenMotionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf013804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 80/1910 [00:00<00:02, 795.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 200 - 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:02<00:00, 745.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = EvaluatorVarLenMotionDataset(data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" ,  split = \"train\" ,num_stages = 6 ,min_length_seconds=10, max_length_seconds=40)\n",
    "collate_fn = EvaluatorMotionCollator()\n",
    "\n",
    "aist_loader = DATALoader(aist_ds,10,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1fbb6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in aist_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99ae543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200, 263])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1d50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {\n",
    "    \"mBR\" : \"Break\",\n",
    "    \"mPO\" : \"Pop\",\n",
    "    \"mLO\" : \"Lock\",\n",
    "    \"mMH\" : \"Middle Hip-hop\",\n",
    "    \"mLH\" : \"LA style Hip-hop\",\n",
    "    \"mHO\" : \"House\",    \n",
    "    \"mWA\" : \"Waack\",\n",
    "    \"mKR\" : \"Krump\",\n",
    "    \"mJS\" : \"Street Jazz\",\n",
    "    \"mJB\" : \"Ballet Jazz\",\n",
    "}\n",
    "\n",
    "joint_index_genre_mapping = {\n",
    "    \"Break\":[],\n",
    "    \"Pop\":[],\n",
    "    \"Lock\":[],\n",
    "    \"Middle Hip-hop\":[],\n",
    "    \"LA style Hip-hop\":[],\n",
    "    \"House\":[],    \n",
    "    \"Waack\":[],\n",
    "    \"Krump\":[],\n",
    "    \"Street Jazz\":[],\n",
    "    \"Ballet Jazz\":[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f1c587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de40b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f05b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b6797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbec54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d757551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class MotionEncoderBiGRUCo(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(MotionEncoderBiGRUCo, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.input_emb = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        self.input_emb.apply(self.init_weight)\n",
    "        self.output_net.apply(self.init_weight)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden = nn.Parameter(torch.randn((2, 1, self.hidden_size), requires_grad=True))\n",
    "        \n",
    "    def init_weight(self, m):\n",
    "        if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose1d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            # m.bias.data.fill_(0.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # input(batch_size, seq_len, dim)\n",
    "    def forward(self, inputs, m_lens):\n",
    "        num_samples = inputs.shape[0]\n",
    "\n",
    "        input_embs = self.input_emb(inputs)\n",
    "        hidden = self.hidden.repeat(1, num_samples, 1)\n",
    "\n",
    "        cap_lens = m_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(input_embs, cap_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        gru_seq, gru_last = self.gru(emb, hidden)\n",
    "\n",
    "        gru_last = torch.cat([gru_last[0], gru_last[1]], dim=-1)\n",
    "\n",
    "        return F.normalize(self.output_net(gru_last), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ec89046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [ True, False,  True,  True],\n",
       "        [ True,  True, False,  True],\n",
       "        [ True,  True,  True, False]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.eye(2 * 2, 2 * 2, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3722fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.loss import CLIPLoss,InfoNceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "43bea444",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionEncoderBiGRUCo(263,768,128,\"cuda\")\n",
    "audioEncoder = MotionEncoderBiGRUCo(128,768,128,\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f975e9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['motion', 'motion_lengths', 'motion_mask', 'names', 'condition', 'condition_mask'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4b20fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = motionEncoder(batch[\"motion\"]*batch[\"motion_mask\"][:,:,None] , batch[\"motion_lengths\"])\n",
    "ec = audioEncoder(batch[\"condition\"]*batch[\"condition_mask\"][:,:,None] ,batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b80de62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f7b42483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9851, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_loss(em , ec ,  torch.ones(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef5c84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "infonce = InfoNceLoss()\n",
    "clip = CLIPLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "34480e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9726, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infonce(em,ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00e7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172cafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba8bf708",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ec.shape[0]\n",
    "'''Positive pairs'''\n",
    "pos_labels = torch.zeros(batch_size)\n",
    "loss_pos = contrastive_loss(ec, em, pos_labels)\n",
    "\n",
    "'''Negative Pairs, shifting index'''\n",
    "neg_labels = torch.ones(batch_size)\n",
    "shift = np.random.randint(0, batch_size-1)\n",
    "new_idx = np.arange(shift, batch_size + shift) % batch_size\n",
    "mis_motion_embedding = em.clone()[new_idx]\n",
    "loss_neg = contrastive_loss(ec, mis_motion_embedding, neg_labels)\n",
    "loss = loss_pos + loss_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "745881fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(209.3174, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0ca19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = torch.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/text_mot_match/model/finest.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdc37a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10724"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eed64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
