{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbaf470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74024dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19169f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e1d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional, TransMotionDatasetConditional,VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0e92d",
   "metadata": {},
   "source": [
    "## VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce71275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741341ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "# train_loader = DATALoader(train_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aca8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.evaluator_dataset import EvaluatorMotionCollator, EvaluatorVarLenMotionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac6215e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 89/1910 [00:00<00:02, 884.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 200 - 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:02<00:00, 816.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = EvaluatorVarLenMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , num_stages = 6 ,min_length_seconds=10, max_length_seconds=40)\n",
    "collate_fn = EvaluatorMotionCollator()\n",
    "\n",
    "aist_loader = DATALoader(aist_ds,10,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4974d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in aist_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6385174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 174, 263])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8f1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {\n",
    "    \"mBR\" : \"Break\",\n",
    "    \"mPO\" : \"Pop\",\n",
    "    \"mLO\" : \"Lock\",\n",
    "    \"mMH\" : \"Middle Hip-hop\",\n",
    "    \"mLH\" : \"LA style Hip-hop\",\n",
    "    \"mHO\" : \"House\",    \n",
    "    \"mWA\" : \"Waack\",\n",
    "    \"mKR\" : \"Krump\",\n",
    "    \"mJS\" : \"Street Jazz\",\n",
    "    \"mJB\" : \"Ballet Jazz\",\n",
    "}\n",
    "\n",
    "joint_index_genre_mapping = {\n",
    "    \"Break\":[],\n",
    "    \"Pop\":[],\n",
    "    \"Lock\":[],\n",
    "    \"Middle Hip-hop\":[],\n",
    "    \"LA style Hip-hop\":[],\n",
    "    \"House\":[],    \n",
    "    \"Waack\":[],\n",
    "    \"Krump\":[],\n",
    "    \"Street Jazz\":[],\n",
    "    \"Ballet Jazz\":[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee8a752f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a6ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f7cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccdec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf09b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60387e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class MotionEncoderBiGRUCo(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device):\n",
    "        super(MotionEncoderBiGRUCo, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.input_emb = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "        self.input_emb.apply(self.init_weight)\n",
    "        self.output_net.apply(self.init_weight)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden = nn.Parameter(torch.randn((2, 1, self.hidden_size), requires_grad=True))\n",
    "        \n",
    "    def init_weight(self, m):\n",
    "        if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose1d):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            # m.bias.data.fill_(0.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # input(batch_size, seq_len, dim)\n",
    "    def forward(self, inputs, m_lens):\n",
    "        num_samples = inputs.shape[0]\n",
    "\n",
    "        input_embs = self.input_emb(inputs)\n",
    "        hidden = self.hidden.repeat(1, num_samples, 1)\n",
    "\n",
    "        cap_lens = m_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(input_embs, cap_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        gru_seq, gru_last = self.gru(emb, hidden)\n",
    "\n",
    "        gru_last = torch.cat([gru_last[0], gru_last[1]], dim=-1)\n",
    "\n",
    "        return self.output_net(gru_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13969318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7201b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=10.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "  \n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c285094",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionEncoderBiGRUCo(263,768,128,\"cuda\")\n",
    "audioEncoder = MotionEncoderBiGRUCo(128,768,128,\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9275c2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['motion', 'motion_lengths', 'motion_mask', 'names', 'condition', 'condition_mask'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f1c679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = motionEncoder(batch[\"motion\"]*batch[\"motion_mask\"][:,:,None] , batch[\"motion_lengths\"])\n",
    "ec = audioEncoder(batch[\"condition\"]*batch[\"condition_mask\"][:,:,None] ,batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "805dcf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_loss = ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2ed8886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9462d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5d0c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ec.shape[0]\n",
    "'''Positive pairs'''\n",
    "pos_labels = torch.zeros(batch_size)\n",
    "loss_pos = contrastive_loss(ec, em, pos_labels)\n",
    "\n",
    "'''Negative Pairs, shifting index'''\n",
    "neg_labels = torch.ones(batch_size)\n",
    "shift = np.random.randint(0, batch_size-1)\n",
    "new_idx = np.arange(shift, batch_size + shift) % batch_size\n",
    "mis_motion_embedding = em.clone()[new_idx]\n",
    "loss_neg = contrastive_loss(ec, mis_motion_embedding, neg_labels)\n",
    "loss = loss_pos + loss_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7477d80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(209.3174, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
