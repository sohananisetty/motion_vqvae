{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa4d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedec771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5dbff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c16c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional, TransMotionDatasetConditional,VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "1fbb6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in aist_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "99ae543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 263])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "a1d50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.eval_modules import AISTEncoderBiGRUCo\n",
    "\n",
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_e = get_cfg_defaults()\n",
    "cfg_e.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/extractors/small/aist_extractor.yaml\")\n",
    "\n",
    "\n",
    "motion_extractor = AISTEncoderBiGRUCo(cfg_e.extractor.motion_input_size,cfg_e.extractor.hidden_size,cfg_e.extractor.output_size)\n",
    "music_extractor =  AISTEncoderBiGRUCo(cfg_e.extractor.music_input_size,cfg_e.extractor.hidden_size,cfg_e.extractor.output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "c3f1c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25000.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "chk = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/extractors/small/extractors.pt\")\n",
    "print(chk[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "e2de40b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AISTEncoderBiGRUCo(\n",
       "  (input_emb): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (gru): GRU(256, 256, batch_first=True, bidirectional=True)\n",
       "  (output_net): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_extractor.load_state_dict(chk[\"motion_extractor\"])\n",
    "music_extractor.load_state_dict(chk[\"music_extractor\"])\n",
    "motion_extractor.eval()\n",
    "music_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264fdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "79f05b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:02<00:00, 803.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.evaluator_dataset import EvaluatorMotionCollator, EvaluatorVarLenMotionDataset,EvaluatorMotionDataset\n",
    "# aist_ds = EvaluatorVarLenMotionDataset(split = \"test\" ,data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" ,num_stages = 6 ,min_length_seconds=10, max_length_seconds=40)\n",
    "aist_ds = EvaluatorMotionDataset(split = \"train\" ,data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" ,window_size = -1 )\n",
    "\n",
    "collate_fn = EvaluatorMotionCollator()\n",
    "\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2dae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "56c27c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_musics_list = glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "c6328b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/scratch/sanisetty3/music_motion/AIST/music/mWA0.npy'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gt_musics_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "700d3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_encodings = []\n",
    "music_encodings_name = []\n",
    "for i in (gt_musics_list):\n",
    "    musc = torch.Tensor(np.load(i))\n",
    "    ec = music_extractor(musc[None,...],torch.Tensor([musc.shape[0]]))\n",
    "    music_encodings.append(ec)\n",
    "    music_encodings_name.append(os.path.basename(i).split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "d2cf98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_encodings_np = torch.cat(music_encodings, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "8902f2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 128)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_encodings_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "ade610b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/1910 [00:08<06:32,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "em_ = []\n",
    "ec_ = []\n",
    "names = []\n",
    "\n",
    "for batch in tqdm(aist_loader):\n",
    "#     if  int(batch[\"motion_lengths\"][0]) < 600:\n",
    "#         continue\n",
    "    if len(em_) > 40:\n",
    "        break\n",
    "\n",
    "    music_name = batch[\"names\"][0].split(\"_\")[-2]\n",
    "    \n",
    "\n",
    "#     if music_name not in names:\n",
    "    names.append(music_name)\n",
    "\n",
    "    em = motion_extractor(batch[\"motion\"], batch[\"motion_lengths\"])\n",
    "    ec = music_extractor(batch[\"condition\"],batch[\"motion_lengths\"])\n",
    "    em_.append(em)\n",
    "    ec_.append(ec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583586b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "3923081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_np = torch.cat(em_, dim=0).detach().cpu().numpy()\n",
    "ec_np = torch.cat(ec_, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "0d897ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1910"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "880230af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0663], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(torch.Tensor(em_[0]),torch.Tensor(ec_[0]) , dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "7ea760eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0663, -0.1124, -0.0715,  ...,  0.0680, -0.0378, -0.0770])"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "F.cosine_similarity(torch.Tensor(em_np),torch.Tensor(ec_np) , dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227f0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "983dbe1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1910, 128)"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "b57fb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "0003e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06629127  0.10820688  0.10866637  0.10676001]\n",
      " [-0.07823959 -0.1123687  -0.07883547 -0.10849159]\n",
      " [-0.11301767 -0.08041526 -0.07153425 -0.08809774]\n",
      " [ 0.0334188   0.0267519   0.03598911  0.01656152]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(em_np , ec_np)[:4,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1e9a137d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mJB0 [25]\n",
      "music:  mWA0 motion : mJB0 -0.08787343\n",
      "music:  mHO0 motion : mJB0 0.032452673\n",
      "music:  mJB0 motion : mJB0 0.36589286\n",
      "music:  mMH0 motion : mJB0 0.27313977\n",
      "music:  mPO0 motion : mJB0 -0.00822898\n",
      "music:  mJS0 motion : mJB0 0.15264952\n",
      "music:  mBR3 motion : mJB0 0.057543725\n",
      "music:  mBR4 motion : mJB0 -0.14038682\n",
      "music:  mLH0 motion : mJB0 0.12863988\n",
      "music:  mLO2 motion : mJB0 -0.10100515\n",
      "music:  mKR1 motion : mJB0 0.0504803\n",
      "music:  mLO5 motion : mJB0 0.06902988\n",
      "music:  mMH1 motion : mJB0 0.15503535\n",
      "music:  mPO1 motion : mJB0 0.08475751\n",
      "music:  mJB1 motion : mJB0 0.07236068\n",
      "music:  mHO1 motion : mJB0 0.20598385\n",
      "music:  mWA1 motion : mJB0 0.0021195784\n",
      "music:  mKR0 motion : mJB0 0.11459133\n",
      "music:  mLO4 motion : mJB0 0.048606966\n",
      "music:  mLO3 motion : mJB0 -0.05313343\n",
      "music:  mLH1 motion : mJB0 -0.0067003807\n",
      "music:  mBR5 motion : mJB0 0.023721173\n",
      "music:  mJS1 motion : mJB0 0.15082487\n",
      "music:  mBR2 motion : mJB0 0.04827468\n",
      "music:  mKR4 motion : mJB0 0.064786606\n",
      "music:  mLO0 motion : mJB0 0.0743303\n",
      "music:  mKR3 motion : mJB0 0.11400167\n",
      "music:  mLH5 motion : mJB0 -0.041239526\n",
      "music:  mLH2 motion : mJB0 0.18125895\n",
      "music:  mBR1 motion : mJB0 0.12262667\n",
      "music:  mJS2 motion : mJB0 0.14020596\n",
      "music:  mJS5 motion : mJB0 0.08308457\n",
      "music:  mPO5 motion : mJB0 0.013729736\n",
      "music:  mMH5 motion : mJB0 0.0763317\n",
      "music:  mJB2 motion : mJB0 0.09319343\n",
      "music:  mJB5 motion : mJB0 0.08094703\n",
      "music:  mMH2 motion : mJB0 0.04074858\n",
      "music:  mPO2 motion : mJB0 0.16204931\n",
      "music:  mHO2 motion : mJB0 0.15332726\n",
      "music:  mHO5 motion : mJB0 0.08037922\n",
      "music:  mWA5 motion : mJB0 -0.06384511\n",
      "music:  mWA2 motion : mJB0 -6.400421e-05\n",
      "music:  mJS4 motion : mJB0 0.11413962\n",
      "music:  mBR0 motion : mJB0 0.09918222\n",
      "music:  mJS3 motion : mJB0 -0.03702741\n",
      "music:  mLH3 motion : mJB0 -0.024880381\n",
      "music:  mLH4 motion : mJB0 0.012370635\n",
      "music:  mKR2 motion : mJB0 0.10680305\n",
      "music:  mKR5 motion : mJB0 0.048827533\n",
      "music:  mLO1 motion : mJB0 0.06580298\n",
      "music:  mWA3 motion : mJB0 0.041320518\n",
      "music:  mWA4 motion : mJB0 0.11716397\n",
      "music:  mHO4 motion : mJB0 0.17151248\n",
      "music:  mHO3 motion : mJB0 0.095007926\n",
      "music:  mMH3 motion : mJB0 0.10041989\n",
      "music:  mJB4 motion : mJB0 -0.22994903\n",
      "music:  mPO3 motion : mJB0 0.10835729\n",
      "music:  mPO4 motion : mJB0 -0.19878995\n",
      "music:  mJB3 motion : mJB0 -0.09737246\n",
      "music:  mMH4 motion : mJB0 0.057597905\n"
     ]
    }
   ],
   "source": [
    "indx = 2\n",
    "print(music_encodings_name[indx], [names.index(music_encodings_name[indx])])\n",
    "for i in range(60):\n",
    "    print(\"music: \", music_encodings_name[i] , \"motion :\", names[names.index(music_encodings_name[indx])] , (F.cosine_similarity(music_encodings[i],em_[names.index(music_encodings_name[indx])] , dim=1)).detach().cpu().numpy()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c808b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1fc38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "6a3b5ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1067], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(music_encodings[4],em_[27] , dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c9983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315af9f9",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a2cfcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist/var_len_768_768_aist.yaml\"\n",
    "encodec_sine = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_sine_aist/var_len_768_768_sine_aist.yaml\"\n",
    "librosa = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_35/var_len_768_768_aist_35.yaml\"\n",
    "encodec_prob50 = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_mask_prob50/trans_768_768_albi_aist_mask_prob50.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ac21878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([210000.])\n"
     ]
    }
   ],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "trans_option = \"encodec\"\n",
    "\n",
    "cfg_trans = get_cfg_defaults()\n",
    "cfg_trans.merge_from_file(encodec)\n",
    "\n",
    "\n",
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()\n",
    "pkg_trans = torch.load(f\"{os.path.dirname(encodec)}/trans_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg_trans[\"steps\"])\n",
    "trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "13ebc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "770d2fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([295000.])\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1d001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e6db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b82bb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.evaluator_wrapper import AISTEvaluatorModelWrapper\n",
    "from utils.eval_music import evaluate_music_motion_generative_extractors\n",
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "72224e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "\n",
    "cfg_eval = get_cfg_defaults()\n",
    "cfg_eval.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/extractors/big/aist_extractor.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "73a753b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 784.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.evaluator_dataset import EvaluatorMotionCollator, EvaluatorVarLenMotionDataset, EvaluatorMotionDataset\n",
    "aist_ds = EvaluatorMotionDataset(split = \"test\" ,data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" ,window_size = -1 )\n",
    "collate_fn = EvaluatorMotionCollator()\n",
    "\n",
    "aist_loader = DATALoader(aist_ds,20,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "dfc1eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from:  /srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/extractors/big/extractors.pt steps:  tensor([95000.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "aist_evaluator = AISTEvaluatorModelWrapper(cfg_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "ba4918af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = []\n",
    "div_R = []\n",
    "div_p = []\n",
    "\n",
    "for i in range(20):\n",
    "    fid, diversity_real, diversity = evaluate_music_motion_generative_extractors(aist_loader , vqvae_model= vqvae_model ,net = trans_model, eval_wrapper = aist_evaluator, seq_len = 600)\n",
    "    fids.append(fid)\n",
    "    div_R.append(diversity_real)\n",
    "    div_p.apppend(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585af910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "0782d59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:04<00:00, 53.21it/s]\n",
      "100%|██████████| 141/141 [00:02<00:00, 54.95it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.76it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 54.11it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.46it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 53.68it/s]\n",
      "100%|██████████| 141/141 [00:02<00:00, 53.69it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.30it/s]\n",
      "100%|██████████| 147/147 [00:02<00:00, 53.53it/s]\n",
      "100%|██████████| 159/159 [00:02<00:00, 53.24it/s]\n",
      "100%|██████████| 141/141 [00:02<00:00, 53.22it/s]\n",
      "100%|██████████| 147/147 [00:02<00:00, 53.63it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.06it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.26it/s]\n",
      "100%|██████████| 159/159 [00:02<00:00, 53.34it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 51.33it/s]\n",
      "100%|██████████| 141/141 [00:03<00:00, 35.45it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.76it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.87it/s]\n",
      "100%|██████████| 147/147 [00:02<00:00, 54.31it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 52.91it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 53.02it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 52.91it/s]\n",
      "100%|██████████| 159/159 [00:02<00:00, 53.76it/s]\n",
      "100%|██████████| 159/159 [00:02<00:00, 53.13it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 54.15it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 53.37it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 54.32it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 53.33it/s]\n",
      "100%|██████████| 213/213 [00:04<00:00, 53.14it/s]\n",
      "100%|██████████| 147/147 [00:02<00:00, 53.08it/s]\n",
      "100%|██████████| 213/213 [00:03<00:00, 53.37it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.80it/s]\n",
      "100%|██████████| 239/239 [00:04<00:00, 53.57it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.28it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 53.23it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.58it/s]\n",
      "100%|██████████| 191/191 [00:03<00:00, 52.97it/s]\n",
      "100%|██████████| 213/213 [00:04<00:00, 53.06it/s]\n",
      "100%|██████████| 174/174 [00:03<00:00, 53.55it/s]\n",
      "100%|██████████| 2/2 [02:22<00:00, 71.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid  0.6284469415386564\n",
      "1.1949672 1.1772099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "seq_len = 600\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "music_annotation_list = []\n",
    "music_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "\n",
    "audio_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/music\"\n",
    "\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "motion_extractor = motion_extractor.cuda()\n",
    "music_extractor = music_extractor.cuda()\n",
    "\n",
    "for j,aist_batch in enumerate(tqdm(aist_loader)):\n",
    "    \n",
    "#     if j>3:\n",
    "#         break\n",
    "    \n",
    "    bs, seq = aist_batch[\"motion\"].shape[0], aist_batch[\"motion\"].shape[1]\n",
    "    \n",
    "    em = motion_extractor(aist_batch[\"motion\"].cuda(), aist_batch[\"motion_lengths\"].cuda())\n",
    "    et = music_extractor(aist_batch[\"condition\"].cuda(),aist_batch[\"motion_lengths\"].cuda())\n",
    "    \n",
    "    generated_motion = torch.zeros(aist_batch[\"motion\"].shape).cuda()\n",
    "    \n",
    "    for i in range(bs):\n",
    "        \n",
    "        mot_len = int(aist_batch[\"motion_lengths\"][i])\n",
    "        motion_name = aist_batch[\"names\"][i]\n",
    "\n",
    "        music_name = motion_name.split('_')[-2]\n",
    "        music_encoding=  np.load(os.path.join(audio_dir , music_name + \".npy\"))\n",
    "\n",
    "        music_name = motion_name.split('_')[-2]\n",
    "        gen_motion_indices = torch.randint(0 , 1024 , (1,1))\n",
    "\n",
    "        gen_motion_indices = trans_model.generate(start_tokens =gen_motion_indices.cuda(),\\\n",
    "                                                    seq_len=mot_len , \\\n",
    "                                                    context = torch.Tensor(music_encoding)[None,...].cuda(), \\\n",
    "                                                    context_mask=torch.ones((1 ,music_encoding.shape[0]) , dtype = torch.bool).cuda()\n",
    "                                                    )\n",
    "\n",
    "\n",
    "        out_motion = torch.zeros((1, gen_motion_indices.shape[-1] , aist_batch[\"motion\"].shape[-1]))\n",
    "        for j in range(0 , mot_len, 200):\n",
    "            quant , out_motion_= vqvae_model.decode(gen_motion_indices[:,j:j+200])\n",
    "            out_motion[:,j:j+200] = out_motion_\n",
    "            \n",
    "        generated_motion[i:i+1,:mot_len,:] = out_motion[:,1:,:]\n",
    "        \n",
    "#         print(out_motion.shape)\n",
    "        \n",
    "\n",
    "    em_pred = motion_extractor(generated_motion.cuda(), aist_batch[\"motion_lengths\"].cuda())\n",
    "    et_pred = music_extractor(aist_batch[\"condition\"].cuda(),aist_batch[\"motion_lengths\"].cuda())\n",
    "    \n",
    "\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    music_pred_list.append(et_pred)\n",
    "    music_annotation_list.append(et)\n",
    "    \n",
    "    \n",
    "    temp_R, temp_match = calculate_R_precision(et.detach().cpu().numpy(), em.detach().cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "\n",
    "    temp_R_pred, temp_match_pred = calculate_R_precision(et_pred.detach().cpu().numpy(), em_pred.detach().cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision += temp_R_pred\n",
    "    matching_score_pred += temp_match_pred\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    nb_sample += bs\n",
    "    \n",
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).detach().cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "music_annotation_np = torch.cat(music_annotation_list, dim=0).detach().cpu().numpy()\n",
    "music_pred_np = torch.cat(music_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n",
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)\n",
    "print(\"fid \", fid)\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 100 if nb_sample > 100 else 10)\n",
    "diversity = calculate_diversity(motion_pred_np, 100 if nb_sample > 100 else 10)\n",
    "print(diversity_real , diversity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "6a7a9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid  0.6284469415386564\n",
      "1.1452008 1.1187509\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).detach().cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "music_annotation_np = torch.cat(music_annotation_list, dim=0).detach().cpu().numpy()\n",
    "music_pred_np = torch.cat(music_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n",
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)\n",
    "print(\"fid \", fid)\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 100 if nb_sample > 100 else 30)\n",
    "diversity = calculate_diversity(motion_pred_np, 100 if nb_sample > 100 else 30)\n",
    "print(diversity_real , diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "0d820cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 64)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_annotation_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "06afa421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.075 0.1   0.1  ]\n",
      "[0.05  0.125 0.175]\n",
      "2.016997146606445\n",
      "2.1148876190185546\n"
     ]
    }
   ],
   "source": [
    "print(R_precision_real/motion_annotation_np.shape[0])\n",
    "print(R_precision/motion_annotation_np.shape[0])\n",
    "\n",
    "print(matching_score_real/motion_annotation_np.shape[0])\n",
    "print(matching_score_pred/motion_annotation_np.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "2ca8d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025 0.05  0.1  ] 2.016996955871582\n",
      "[0.    0.05  0.075] 2.1148876190185546\n"
     ]
    }
   ],
   "source": [
    "ln = music_annotation_np.shape[0]\n",
    "temp_R, temp_match = calculate_R_precision(music_annotation_np, motion_annotation_np, top_k=3, sum_all=True)\n",
    "print(temp_R/ln, temp_match/ln)\n",
    "temp_R_pred, temp_match_pred = calculate_R_precision(music_annotation_np,motion_pred_np, top_k=3, sum_all=True)\n",
    "print(temp_R_pred/ln, temp_match_pred/ln )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ac533",
   "metadata": {},
   "source": [
    "## Eval on EDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "d1bb23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.spatial.transform import Rotation as R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02dacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_motions = glob(\"/srv/scratch/sanisetty3/music_motion/EDGE/eval/hml/new_joint_vecs/*.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "66d77b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_motions = glob(\"/srv/scratch/sanisetty3/music_motion/EDGE/eval/motions/*.pkl\")\n",
    "aist_motions = glob(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/g*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff246d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "d33b5f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mKR2',\n",
       " 'mWA0',\n",
       " 'mPO1',\n",
       " 'mHO5',\n",
       " 'mLO2',\n",
       " 'mJB5',\n",
       " 'mLH4',\n",
       " 'mBR0',\n",
       " 'mMH3',\n",
       " 'mJS3']"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "d3f52ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mBR0', 'mHO5', 'mJS3', 'mKR2', 'mLH4', 'mLO2', 'mMH3', 'mPO1', 'mWA0'}"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(names) & set([i.split(\".\")[0][-4:] for i in edge_motions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "1e7c0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hml_msc = ['mBR0', 'mHO5', 'mJS3', 'mKR2', 'mLH4', 'mLO2', 'mMH3', 'mPO1', 'mWA0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "a2ea23e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([899, 263])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.load(f\"/srv/scratch/sanisetty3/music_motion/EDGE/eval/hml/new_joint_vecs/{music_name}.npy\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "fb40eb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 809.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from core.datasets.evaluator_dataset import EvaluatorMotionCollator, EvaluatorVarLenMotionDataset, EvaluatorMotionDataset\n",
    "aist_ds = EvaluatorMotionDataset(split = \"test\" ,data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" ,window_size = -1 )\n",
    "collate_fn = EvaluatorMotionCollator()\n",
    "\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "86da46de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:00<00:00, 59.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mKR2\n",
      "mPO1\n",
      "mLH4\n",
      "mWA0\n",
      "mJS3\n",
      "mMH3\n",
      "mKR2\n",
      "mHO5\n",
      "mBR0\n",
      "mMH3\n",
      "mHO5\n",
      "mMH3\n",
      "mJS3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [00:00<00:00, 65.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mLH4\n",
      "mJB5\n",
      "mKR2\n",
      "mWA0\n",
      "mLH4\n",
      "mHO5\n",
      "mJB5\n",
      "mJB5\n",
      "mJB5\n",
      "mPO1\n",
      "mLH4\n",
      "mLO2\n",
      "mMH3\n",
      "mLO2\n",
      "mJS3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 34/40 [00:00<00:00, 58.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mPO1\n",
      "mBR0\n",
      "mLO2\n",
      "mPO1\n",
      "mWA0\n",
      "mKR2\n",
      "mHO5\n",
      "mBR0\n",
      "mJS3\n",
      "mLO2\n",
      "mBR0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 60.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mWA0\n",
      "fid  1.6043953599870409\n",
      "1.1438 0.49290603\n",
      "[1 2 4] 71.93391\n",
      "[1 2 3] 82.030975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "seq_len = 600\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "music_annotation_list = []\n",
    "music_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "\n",
    "audio_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/music\"\n",
    "\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "motion_extractor = motion_extractor.cuda()\n",
    "music_extractor = music_extractor.cuda()\n",
    "\n",
    "for j,aist_batch in enumerate(tqdm(aist_loader)):\n",
    "    \n",
    "#     if j>3:\n",
    "#         break\n",
    "    \n",
    "    bs, seq = aist_batch[\"motion\"].shape[0], aist_batch[\"motion\"].shape[1]\n",
    "    \n",
    "    \n",
    "    mot_len = int(aist_batch[\"motion_lengths\"][0])\n",
    "\n",
    "    em = motion_extractor(aist_batch[\"motion\"].cuda(), aist_batch[\"motion_lengths\"].cuda())\n",
    "    et = music_extractor(aist_batch[\"condition\"].cuda(),aist_batch[\"motion_lengths\"].cuda())\n",
    "    \n",
    "    motion_name = aist_batch[\"names\"][0]\n",
    "    \n",
    "    music_name = motion_name.split('_')[-2]\n",
    "    print(music_name)\n",
    "    music_encoding=  np.load(os.path.join(audio_dir , music_name + \".npy\"))\n",
    "    \n",
    "\n",
    "    try:\n",
    "        generated_motion = torch.Tensor(np.load(f\"/srv/scratch/sanisetty3/music_motion/EDGE/eval/hml/new_joint_vecs/{music_name}.npy\"))[:mot_len,:][None,...]\n",
    "    except:\n",
    "        continue\n",
    "                                   \n",
    "\n",
    "    em_pred = motion_extractor(generated_motion.cuda(), aist_batch[\"motion_lengths\"].cuda())\n",
    "    et_pred = music_extractor(aist_batch[\"condition\"].cuda(),aist_batch[\"motion_lengths\"].cuda())\n",
    "    \n",
    "\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    music_pred_list.append(et_pred)\n",
    "    music_annotation_list.append(et)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nb_sample += bs\n",
    "    \n",
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).detach().cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "music_annotation_np = torch.cat(music_annotation_list, dim=0).detach().cpu().numpy()\n",
    "music_pred_np = torch.cat(music_pred_list, dim=0).detach().cpu().numpy()\n",
    "\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n",
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)\n",
    "print(\"fid \", fid)\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 100 if nb_sample > 100 else 10)\n",
    "diversity = calculate_diversity(motion_pred_np, 100 if nb_sample > 100 else 10)\n",
    "print(diversity_real , diversity)\n",
    "\n",
    "\n",
    "temp_R, temp_match = calculate_R_precision(music_annotation_np, motion_annotation_np, top_k=3, sum_all=True)\n",
    "print(temp_R, temp_match)\n",
    "temp_R_pred, temp_match_pred = calculate_R_precision(music_pred_np,motion_pred_np, top_k=3, sum_all=True)\n",
    "print(temp_R_pred, temp_match_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "526d8fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 64)"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_annotation_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a484f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18ee69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dc63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a864fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "45d14d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "9503441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smplx import SMPL\n",
    "\n",
    "# set smpl\n",
    "smpl = SMPL(model_path=\"/srv/share/datasets/AIST/smpl_models\", gender='MALE', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "7403ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aist_plusplus.loader import AISTDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "a4ba1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_motions = glob(\"/srv/scratch/sanisetty3/music_motion/EDGE/eval/motions/*.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "610ea2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir = \"/srv/share/datasets/AIST/aist_plusplus_final/\"\n",
    "audio_dir = \"/srv/share/datasets/AIST/aist_plusplus_media/audio/wav/\"\n",
    "audio_cache_dir = \"/srv/scratch/sanisetty3/clean/mint/data/audio_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f820db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "f2b5feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_names = []\n",
    "seq_names += np.loadtxt(\n",
    "            os.path.join(\"/srv/scratch/sanisetty3/mint_pyt/mint/third_party/aist_plusplus_final/splits/crossmodal_test.txt\"), dtype=str\n",
    "        ).tolist()\n",
    "\n",
    "ignore_list = np.loadtxt(\n",
    "    os.path.join(\"/srv/scratch/sanisetty3/mint_pyt/mint/third_party/aist_plusplus_final/\", \"ignore_list.txt\"), dtype=str\n",
    ").tolist()\n",
    "seq_names = [name for name in seq_names if name not in ignore_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "ec855398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "871dd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cache_audio_features(seq_names):\n",
    "    FPS = 60\n",
    "    HOP_LENGTH = 512\n",
    "    SR = FPS * HOP_LENGTH\n",
    "    EPS = 1e-6\n",
    "\n",
    "    def _get_tempo(audio_name):\n",
    "        \"\"\"Get tempo (BPM) for a music by parsing music name.\"\"\"\n",
    "        assert len(audio_name) == 4\n",
    "        if audio_name[0:3] in ['mBR', 'mPO', 'mLO', 'mMH', 'mLH', 'mWA', 'mKR', 'mJS', 'mJB']:\n",
    "            return int(audio_name[3]) * 10 + 80\n",
    "        elif audio_name[0:3] == 'mHO':\n",
    "            return int(audio_name[3]) * 5 + 110\n",
    "        else: assert False, audio_name\n",
    "\n",
    "    audio_names = list(set([seq_name.split(\"_\")[-2] for seq_name in seq_names]))\n",
    "    print(audio_names)\n",
    "\n",
    "    for audio_name in audio_names:\n",
    "        save_path = os.path.join(\"/srv/scratch/sanisetty3/clean/mint/data/audio_cache\", f\"{audio_name}.npy\")\n",
    "        if os.path.exists(save_path):\n",
    "            continue\n",
    "        data, _ = librosa.load(os.path.join(audio_dir, f\"{audio_name}.wav\"), sr=SR)\n",
    "        envelope = librosa.onset.onset_strength(data, sr=SR)  # (seq_len,)\n",
    "        mfcc = librosa.feature.mfcc(data, sr=SR, n_mfcc=20).T  # (seq_len, 20)\n",
    "        chroma = librosa.feature.chroma_cens(\n",
    "            data, sr=SR, hop_length=HOP_LENGTH, n_chroma=12).T  # (seq_len, 12)\n",
    "\n",
    "        peak_idxs = librosa.onset.onset_detect(\n",
    "            onset_envelope=envelope.flatten(), sr=SR, hop_length=HOP_LENGTH)\n",
    "        peak_onehot = np.zeros_like(envelope, dtype=np.float32)\n",
    "        peak_onehot[peak_idxs] = 1.0  # (seq_len,)\n",
    "\n",
    "        tempo, beat_idxs = librosa.beat.beat_track(\n",
    "            onset_envelope=envelope, sr=SR, hop_length=HOP_LENGTH,\n",
    "            start_bpm=_get_tempo(audio_name), tightness=100)\n",
    "        beat_onehot = np.zeros_like(envelope, dtype=np.float32)\n",
    "        beat_onehot[beat_idxs] = 1.0  # (seq_len,)\n",
    "\n",
    "        audio_feature = np.concatenate([\n",
    "            envelope[:, None], mfcc, chroma, peak_onehot[:, None], beat_onehot[:, None]\n",
    "        ], axis=-1)\n",
    "        np.save(save_path, audio_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54395dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mKR2', 'mWA0', 'mPO1', 'mHO5', 'mLO2', 'mJB5', 'mLH4', 'mBR0', 'mMH3', 'mJS3']\n"
     ]
    }
   ],
   "source": [
    "cache_audio_features(seq_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf3c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42e227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "c4e64624",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(source_file, \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "    \n",
    "smpl_poses = data[\"smpl_poses\"]\n",
    "smpl_trans =  data[\"smpl_trans\"]\n",
    "\n",
    "smpl_poses_ = R.from_rotvec(smpl_poses.reshape(-1, 3)).as_matrix().reshape(smpl_poses.shape[0], -1)\n",
    "motion_sequence = np.concatenate([smpl_trans, smpl_poses_], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "037c9e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "319dc83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "3f003b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "f56a2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_dir = \"/srv/share/datasets/AIST/aist_plusplus_final/\"\n",
    "audio_dir = \"/srv/share/datasets/AIST/aist_plusplus_media/audio/wav/\"\n",
    "audio_cache_dir = \"/srv/scratch/sanisetty3/clean/mint/data/audio_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "90ac1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye(n, batch_shape):\n",
    "    iden = np.zeros(np.concatenate([batch_shape, [n, n]]))\n",
    "    iden[..., 0, 0] = 1.0\n",
    "    iden[..., 1, 1] = 1.0\n",
    "    iden[..., 2, 2] = 1.0\n",
    "    return iden\n",
    "\n",
    "\n",
    "def get_closest_rotmat(rotmats):\n",
    "    \"\"\"\n",
    "    Finds the rotation matrix that is closest to the inputs in terms of the Frobenius norm. For each input matrix\n",
    "    it computes the SVD as R = USV' and sets R_closest = UV'. Additionally, it is made sure that det(R_closest) == 1.\n",
    "    Args:\n",
    "        rotmats: np array of shape (..., 3, 3).\n",
    "    Returns:\n",
    "        A numpy array of the same shape as the inputs.\n",
    "    \"\"\"\n",
    "    u, s, vh = np.linalg.svd(rotmats)\n",
    "    r_closest = np.matmul(u, vh)\n",
    "\n",
    "    # if the determinant of UV' is -1, we must flip the sign of the last column of u\n",
    "    det = np.linalg.det(r_closest)  # (..., )\n",
    "    iden = eye(3, det.shape)\n",
    "    iden[..., 2, 2] = np.sign(det)\n",
    "    r_closest = np.matmul(np.matmul(u, iden), vh)\n",
    "    return r_closest\n",
    "\n",
    "\n",
    "def recover_to_axis_angles(motion):\n",
    "    batch_size, seq_len, dim = motion.shape\n",
    "    assert dim == 225\n",
    "    transl = motion[:, :, 6:9]\n",
    "    rotmats = get_closest_rotmat(\n",
    "        np.reshape(motion[:, :, 9:], (batch_size, seq_len, 24, 3, 3))\n",
    "    )\n",
    "    axis_angles = R.from_matrix(\n",
    "        rotmats.reshape(-1, 3, 3)\n",
    "    ).as_rotvec().reshape(batch_size, seq_len, 24, 3)\n",
    "    return axis_angles, transl\n",
    "\n",
    "\n",
    "def recover_motion_to_keypoints(motion, smpl_model):\n",
    "    smpl_poses, smpl_trans = recover_to_axis_angles(motion)\n",
    "    smpl_poses = np.squeeze(smpl_poses, axis=0)  # (seq_len, 24, 3)\n",
    "    smpl_trans = np.squeeze(smpl_trans, axis=0)  # (seq_len, 3)\n",
    "    keypoints3d = smpl_model.forward(\n",
    "        global_orient=torch.from_numpy(smpl_poses[:, 0:1]).float(),\n",
    "        body_pose=torch.from_numpy(smpl_poses[:, 1:]).float(),\n",
    "        transl=torch.from_numpy(smpl_trans).float(),\n",
    "    ).joints.detach().numpy()[:, :24, :]   # (seq_len, 24, 3)\n",
    "    return keypoints3d\n",
    "\n",
    "\n",
    "def motion_peak_onehot(joints):\n",
    "    \"\"\"Calculate motion beats.\n",
    "    Kwargs:\n",
    "        joints: [nframes, njoints, 3]\n",
    "    Returns:\n",
    "        - peak_onhot: motion beats.\n",
    "    \"\"\"\n",
    "    # Calculate velocity.\n",
    "    velocity = np.zeros_like(joints, dtype=np.float32)\n",
    "    velocity[1:] = joints[1:] - joints[:-1]\n",
    "    velocity_norms = np.linalg.norm(velocity, axis=2)\n",
    "    envelope = np.sum(velocity_norms, axis=1)  # (seq_len,)\n",
    "\n",
    "    # Find local minima in velocity -- beats\n",
    "    peak_idxs = scisignal.argrelextrema(envelope, np.less, axis=0, order=10) # 10 for 60FPS\n",
    "    peak_onehot = np.zeros_like(envelope, dtype=bool)\n",
    "    peak_onehot[peak_idxs] = 1\n",
    "\n",
    "    # # Second-derivative of the velocity shows the energy of the beats\n",
    "    # peak_energy = np.gradient(np.gradient(envelope)) # (seq_len,)\n",
    "    # # optimize peaks\n",
    "    # peak_onehot[peak_energy<0.001] = 0\n",
    "    return peak_onehot\n",
    "\n",
    "\n",
    "def alignment_score(music_beats, motion_beats, sigma=3):\n",
    "    \"\"\"Calculate alignment score between music and motion.\"\"\"\n",
    "    if motion_beats.sum() == 0:\n",
    "        return 0.0\n",
    "    music_beat_idxs = np.where(music_beats)[0]\n",
    "    motion_beat_idxs = np.where(motion_beats)[0]\n",
    "    score_all = []\n",
    "    for motion_beat_idx in motion_beat_idxs:\n",
    "        dists = np.abs(music_beat_idxs - motion_beat_idx).astype(np.float32)\n",
    "        ind = np.argmin(dists)\n",
    "        score = np.exp(- dists[ind]**2 / 2 / sigma**2)\n",
    "        score_all.append(score)\n",
    "    return sum(score_all) / len(score_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "b9463aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1 / 952\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'motion_peak_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-717-815bbecca7e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtransl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmpl_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     ).joints.detach().numpy()[:, :24, :]   # (seq_len, 24, 3)\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmotion_beats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion_peak_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# get real data music beats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0maudio_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'motion_peak_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = AISTDataset(anno_dir)\n",
    "n_samples = len(seq_names)\n",
    "beat_scores = []\n",
    "for i, seq_name in enumerate(seq_names):\n",
    "    print(\"processing %d / %d\" % (i + 1, n_samples))\n",
    "    # get real data motion beats\n",
    "    smpl_poses, smpl_scaling, smpl_trans = AISTDataset.load_motion(\n",
    "        dataset.motion_dir, seq_name)\n",
    "    smpl_trans /= smpl_scaling\n",
    "    keypoints3d = smpl.forward(\n",
    "        global_orient=torch.from_numpy(smpl_poses[:, 0:1]).float(),\n",
    "        body_pose=torch.from_numpy(smpl_poses[:, 1:]).float(),\n",
    "        transl=torch.from_numpy(smpl_trans).float(),\n",
    "    ).joints.detach().numpy()[:, :24, :]   # (seq_len, 24, 3)\n",
    "    motion_beats = motion_peak_onehot(keypoints3d)\n",
    "    # get real data music beats\n",
    "    audio_name = seq_name.split(\"_\")[4]\n",
    "    audio_feature = np.load(os.path.join(audio_cache_dir, f\"{audio_name}.npy\"))\n",
    "    audio_beats = audio_feature[:keypoints3d.shape[0], -1] # last dim is the music beats\n",
    "    # get beat alignment scores\n",
    "    beat_score = alignment_score(audio_beats, motion_beats, sigma=3)\n",
    "    beat_scores.append(beat_score)\n",
    "print (\"\\nBeat score on real data: %.3f\\n\" % (sum(beat_scores) / n_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93342c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101bf024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "60fcd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aist(aist_vec):\n",
    "    aist_ex = torch.Tensor(aist_vec)\n",
    "    trans = aist_ex[:,:3]\n",
    "    aist_ex_9d = aist_ex[:,3:].reshape(aist_vec.shape[0],24,3,3)\n",
    "    aist_ex_9d[:,-2:] = torch.zeros((aist_vec.shape[0], 2, 3, 3))\n",
    "    global_orient = aist_ex_9d[:, 0:1]\n",
    "    rotations = aist_ex_9d[:, 1:]\n",
    "    out_aist_og = smpl_model(body_pose=rotations, global_orient=global_orient, transl = trans)\n",
    "    \n",
    "            \n",
    "    return  out_aist_og\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "1dc9bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = process_aist(smpl_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "09c4f226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vertices': tensor([[[ 0.6551, -0.0735,  2.6354],\n",
       "          [ 0.6521, -0.0824,  2.6211],\n",
       "          [ 0.6559, -0.0690,  2.6174],\n",
       "          ...,\n",
       "          [ 0.4940, -0.0758,  2.5989],\n",
       "          [ 0.4952, -0.0773,  2.6004],\n",
       "          [ 0.4945, -0.0811,  2.5978]],\n",
       " \n",
       "         [[ 0.6392, -0.0803,  2.6505],\n",
       "          [ 0.6365, -0.0893,  2.6363],\n",
       "          [ 0.6404, -0.0759,  2.6326],\n",
       "          ...,\n",
       "          [ 0.4787, -0.0826,  2.6107],\n",
       "          [ 0.4798, -0.0841,  2.6121],\n",
       "          [ 0.4792, -0.0879,  2.6096]],\n",
       " \n",
       "         [[ 0.6277, -0.0911,  2.6582],\n",
       "          [ 0.6250, -0.1000,  2.6440],\n",
       "          [ 0.6294, -0.0868,  2.6404],\n",
       "          ...,\n",
       "          [ 0.4678, -0.0867,  2.6159],\n",
       "          [ 0.4688, -0.0883,  2.6173],\n",
       "          [ 0.4681, -0.0920,  2.6147]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0608, -0.0188,  2.6868],\n",
       "          [-0.0655, -0.0282,  2.6726],\n",
       "          [-0.0531, -0.0208,  2.6695],\n",
       "          ...,\n",
       "          [-0.1725,  0.0707,  2.6069],\n",
       "          [-0.1730,  0.0690,  2.6086],\n",
       "          [-0.1753,  0.0660,  2.6063]],\n",
       " \n",
       "         [[-0.0522, -0.0297,  2.6782],\n",
       "          [-0.0570, -0.0381,  2.6631],\n",
       "          [-0.0445, -0.0308,  2.6605],\n",
       "          ...,\n",
       "          [-0.1627,  0.0651,  2.6029],\n",
       "          [-0.1631,  0.0631,  2.6044],\n",
       "          [-0.1654,  0.0603,  2.6020]],\n",
       " \n",
       "         [[-0.0456, -0.0338,  2.6737],\n",
       "          [-0.0504, -0.0421,  2.6584],\n",
       "          [-0.0381, -0.0345,  2.6558],\n",
       "          ...,\n",
       "          [-0.1580,  0.0597,  2.6006],\n",
       "          [-0.1585,  0.0576,  2.6020],\n",
       "          [-0.1606,  0.0549,  2.5996]]]),\n",
       " 'vibe': tensor([[[ 6.3225e-01, -1.2746e-01,  2.5481e+00],\n",
       "          [ 5.2309e-01,  1.7635e-02,  2.4659e+00],\n",
       "          [ 3.3601e-01,  4.0470e-02,  2.4177e+00],\n",
       "          ...,\n",
       "          [ 5.9090e-01, -1.2385e-01,  2.5936e+00],\n",
       "          [ 6.1263e-01,  1.3768e-02,  2.5801e+00],\n",
       "          [ 4.9535e-01, -7.3878e-02,  2.5889e+00]],\n",
       " \n",
       "         [[ 6.1815e-01, -1.3474e-01,  2.5635e+00],\n",
       "          [ 5.1162e-01,  1.1414e-02,  2.4795e+00],\n",
       "          [ 3.2628e-01,  3.6222e-02,  2.4357e+00],\n",
       "          ...,\n",
       "          [ 5.7574e-01, -1.3066e-01,  2.6081e+00],\n",
       "          [ 5.9798e-01,  6.6718e-03,  2.5939e+00],\n",
       "          [ 4.8032e-01, -8.0732e-02,  2.6007e+00]],\n",
       " \n",
       "         [[ 6.0597e-01, -1.4461e-01,  2.5713e+00],\n",
       "          [ 5.0704e-01,  7.4164e-03,  2.4880e+00],\n",
       "          [ 3.2313e-01,  3.4945e-02,  2.4489e+00],\n",
       "          ...,\n",
       "          [ 5.6291e-01, -1.3871e-01,  2.6150e+00],\n",
       "          [ 5.9125e-01, -2.4380e-03,  2.6012e+00],\n",
       "          [ 4.6972e-01, -8.5030e-02,  2.6060e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-9.6507e-02, -6.5938e-02,  2.5990e+00],\n",
       "          [-7.9981e-02,  9.4543e-02,  2.4813e+00],\n",
       "          [-2.4910e-01,  1.1836e-01,  2.4303e+00],\n",
       "          ...,\n",
       "          [-1.3346e-01, -2.6088e-02,  2.6304e+00],\n",
       "          [-2.1599e-02,  6.0719e-02,  2.6172e+00],\n",
       "          [-1.6836e-01,  6.9385e-02,  2.5976e+00]],\n",
       " \n",
       "         [[-8.7130e-02, -7.1419e-02,  2.5861e+00],\n",
       "          [-7.1112e-02,  9.3418e-02,  2.4775e+00],\n",
       "          [-2.4173e-01,  1.1751e-01,  2.4237e+00],\n",
       "          ...,\n",
       "          [-1.2404e-01, -3.3904e-02,  2.6203e+00],\n",
       "          [-1.2276e-02,  5.3238e-02,  2.6130e+00],\n",
       "          [-1.5826e-01,  6.4527e-02,  2.5934e+00]],\n",
       " \n",
       "         [[-8.0026e-02, -7.4790e-02,  2.5806e+00],\n",
       "          [-6.9029e-02,  9.0974e-02,  2.4738e+00],\n",
       "          [-2.4229e-01,  1.1356e-01,  2.4135e+00],\n",
       "          ...,\n",
       "          [-1.1741e-01, -3.8853e-02,  2.6158e+00],\n",
       "          [-7.7044e-03,  5.0688e-02,  2.6094e+00],\n",
       "          [-1.5355e-01,  5.9402e-02,  2.5910e+00]]]),\n",
       " 'a2m': tensor([[[ 0.5119,  0.0211,  1.9618],\n",
       "          [ 0.5231,  0.0176,  2.4659],\n",
       "          [ 0.3360,  0.0405,  2.4177],\n",
       "          ...,\n",
       "          [ 0.7644,  0.2414,  1.1997],\n",
       "          [ 0.4691,  0.1681,  1.1074],\n",
       "          [ 0.5829, -0.0451,  2.7159]],\n",
       " \n",
       "         [[ 0.5048,  0.0292,  1.9772],\n",
       "          [ 0.5116,  0.0114,  2.4795],\n",
       "          [ 0.3263,  0.0362,  2.4357],\n",
       "          ...,\n",
       "          [ 0.7571,  0.2446,  1.2535],\n",
       "          [ 0.4913,  0.1439,  1.1076],\n",
       "          [ 0.5652, -0.0510,  2.7294]],\n",
       " \n",
       "         [[ 0.5019,  0.0469,  1.9894],\n",
       "          [ 0.5070,  0.0074,  2.4880],\n",
       "          [ 0.3231,  0.0349,  2.4489],\n",
       "          ...,\n",
       "          [ 0.7422,  0.2549,  1.3156],\n",
       "          [ 0.5181,  0.1191,  1.1086],\n",
       "          [ 0.5537, -0.0587,  2.7360]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0777,  0.1109,  1.9681],\n",
       "          [-0.0800,  0.0945,  2.4813],\n",
       "          [-0.2491,  0.1184,  2.4303],\n",
       "          ...,\n",
       "          [ 0.1708,  0.2649,  1.1014],\n",
       "          [-0.3458,  0.2393,  1.0820],\n",
       "          [-0.1100,  0.0638,  2.7419]],\n",
       " \n",
       "         [[-0.0689,  0.1180,  1.9634],\n",
       "          [-0.0711,  0.0934,  2.4775],\n",
       "          [-0.2417,  0.1175,  2.4237],\n",
       "          ...,\n",
       "          [ 0.1814,  0.2768,  1.0963],\n",
       "          [-0.3353,  0.2505,  1.0795],\n",
       "          [-0.1006,  0.0488,  2.7369]],\n",
       " \n",
       "         [[-0.0646,  0.1209,  1.9598],\n",
       "          [-0.0690,  0.0910,  2.4738],\n",
       "          [-0.2423,  0.1136,  2.4135],\n",
       "          ...,\n",
       "          [ 0.1879,  0.2818,  1.0931],\n",
       "          [-0.3347,  0.2523,  1.0784],\n",
       "          [-0.0948,  0.0424,  2.7334]]]),\n",
       " 'smpl': tensor([[[ 5.1190e-01,  2.1078e-02,  1.9618e+00],\n",
       "          [ 5.8742e-01,  2.0037e-02,  1.8750e+00],\n",
       "          [ 4.5058e-01,  1.3774e-02,  1.8671e+00],\n",
       "          ...,\n",
       "          [ 3.4739e-01, -1.1398e-01,  2.3004e+00],\n",
       "          [ 6.3436e-01, -1.6097e-01,  2.3482e+00],\n",
       "          [ 4.1425e-01, -1.6587e-01,  2.3124e+00]],\n",
       " \n",
       "         [[ 5.0476e-01,  2.9204e-02,  1.9772e+00],\n",
       "          [ 5.8280e-01,  2.6000e-02,  1.8928e+00],\n",
       "          [ 4.4614e-01,  2.3470e-02,  1.8807e+00],\n",
       "          ...,\n",
       "          [ 3.2488e-01, -1.0665e-01,  2.3186e+00],\n",
       "          [ 6.2666e-01, -1.5678e-01,  2.3724e+00],\n",
       "          [ 3.9353e-01, -1.5741e-01,  2.3223e+00]],\n",
       " \n",
       "         [[ 5.0190e-01,  4.6898e-02,  1.9894e+00],\n",
       "          [ 5.8187e-01,  4.1315e-02,  1.9068e+00],\n",
       "          [ 4.4554e-01,  4.3220e-02,  1.8914e+00],\n",
       "          ...,\n",
       "          [ 3.0287e-01, -1.0705e-01,  2.3207e+00],\n",
       "          [ 6.1451e-01, -1.6694e-01,  2.3866e+00],\n",
       "          [ 3.7097e-01, -1.5734e-01,  2.3089e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-7.7746e-02,  1.1088e-01,  1.9681e+00],\n",
       "          [-9.7145e-03,  1.1888e-01,  1.8757e+00],\n",
       "          [-1.4684e-01,  1.2244e-01,  1.8793e+00],\n",
       "          ...,\n",
       "          [-3.1148e-01, -1.6636e-01,  2.3036e+00],\n",
       "          [ 8.5953e-02, -1.6364e-01,  2.3249e+00],\n",
       "          [-2.6310e-01, -2.3134e-01,  2.2764e+00]],\n",
       " \n",
       "         [[-6.8903e-02,  1.1797e-01,  1.9634e+00],\n",
       "          [-9.4606e-04,  1.2987e-01,  1.8713e+00],\n",
       "          [-1.3808e-01,  1.3275e-01,  1.8751e+00],\n",
       "          ...,\n",
       "          [-3.6318e-01, -8.2883e-02,  2.1161e+00],\n",
       "          [ 1.0851e-01, -5.0149e-02,  2.1427e+00],\n",
       "          [-3.3217e-01, -1.2607e-01,  2.0492e+00]],\n",
       " \n",
       "         [[-6.4608e-02,  1.2094e-01,  1.9598e+00],\n",
       "          [ 3.4257e-03,  1.3499e-01,  1.8681e+00],\n",
       "          [-1.3374e-01,  1.3619e-01,  1.8716e+00],\n",
       "          ...,\n",
       "          [-3.7997e-01, -1.5767e-02,  2.0368e+00],\n",
       "          [ 1.0279e-01,  2.7096e-03,  2.0713e+00],\n",
       "          [-3.5595e-01, -4.3137e-02,  1.9594e+00]]]),\n",
       " 'a2mpl': tensor([[[ 5.1190e-01,  2.1078e-02,  1.9618e+00],\n",
       "          [ 5.8742e-01,  2.0037e-02,  1.8750e+00],\n",
       "          [ 4.5058e-01,  1.3774e-02,  1.8671e+00],\n",
       "          ...,\n",
       "          [ 7.6437e-01,  2.4145e-01,  1.1997e+00],\n",
       "          [ 4.6915e-01,  1.6813e-01,  1.1074e+00],\n",
       "          [ 5.8294e-01, -4.5128e-02,  2.7159e+00]],\n",
       " \n",
       "         [[ 5.0476e-01,  2.9204e-02,  1.9772e+00],\n",
       "          [ 5.8280e-01,  2.6000e-02,  1.8928e+00],\n",
       "          [ 4.4614e-01,  2.3470e-02,  1.8807e+00],\n",
       "          ...,\n",
       "          [ 7.5714e-01,  2.4462e-01,  1.2535e+00],\n",
       "          [ 4.9129e-01,  1.4391e-01,  1.1076e+00],\n",
       "          [ 5.6518e-01, -5.1037e-02,  2.7294e+00]],\n",
       " \n",
       "         [[ 5.0190e-01,  4.6898e-02,  1.9894e+00],\n",
       "          [ 5.8187e-01,  4.1315e-02,  1.9068e+00],\n",
       "          [ 4.4554e-01,  4.3220e-02,  1.8914e+00],\n",
       "          ...,\n",
       "          [ 7.4219e-01,  2.5491e-01,  1.3156e+00],\n",
       "          [ 5.1812e-01,  1.1909e-01,  1.1086e+00],\n",
       "          [ 5.5365e-01, -5.8701e-02,  2.7360e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-7.7746e-02,  1.1088e-01,  1.9681e+00],\n",
       "          [-9.7145e-03,  1.1888e-01,  1.8757e+00],\n",
       "          [-1.4684e-01,  1.2244e-01,  1.8793e+00],\n",
       "          ...,\n",
       "          [ 1.7081e-01,  2.6486e-01,  1.1014e+00],\n",
       "          [-3.4580e-01,  2.3930e-01,  1.0820e+00],\n",
       "          [-1.0999e-01,  6.3767e-02,  2.7419e+00]],\n",
       " \n",
       "         [[-6.8903e-02,  1.1797e-01,  1.9634e+00],\n",
       "          [-9.4606e-04,  1.2987e-01,  1.8713e+00],\n",
       "          [-1.3808e-01,  1.3275e-01,  1.8751e+00],\n",
       "          ...,\n",
       "          [ 1.8139e-01,  2.7681e-01,  1.0963e+00],\n",
       "          [-3.3532e-01,  2.5051e-01,  1.0795e+00],\n",
       "          [-1.0060e-01,  4.8787e-02,  2.7369e+00]],\n",
       " \n",
       "         [[-6.4608e-02,  1.2094e-01,  1.9598e+00],\n",
       "          [ 3.4257e-03,  1.3499e-01,  1.8681e+00],\n",
       "          [-1.3374e-01,  1.3619e-01,  1.8716e+00],\n",
       "          ...,\n",
       "          [ 1.8794e-01,  2.8178e-01,  1.0931e+00],\n",
       "          [-3.3465e-01,  2.5228e-01,  1.0784e+00],\n",
       "          [-9.4829e-02,  4.2443e-02,  2.7334e+00]]])}"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40445f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
