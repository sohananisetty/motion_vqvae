{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ac1e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15e3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8a9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81d4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, mean ,std , j = 22):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*std+mean, j)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, j, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baa151d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_trans = get_cfg_defaults()\n",
    "cfg_trans.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_style/var_len_768_768_aist_style.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()\n",
    "# pkg_trans = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist/trans_motion_best_fid.pt\", map_location = 'cpu')\n",
    "# print(pkg_trans[\"steps\"])\n",
    "# trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "# trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970089a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efefb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional,VQVarLenMotionDatasetConditional, TransMotionDatasetConditional,MotionCollatorConditionalStyle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd41d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hml_train_ds = TransMotionDatasetConditional(dataset_name = \"t2m\", split = \"val\",datafolder=\"joint_indices_max_400\",data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\" , window_size = 400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b2b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:01<00:00, 1335.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_train_ds = TransMotionDatasetConditional(\"aist\", split = \"train\",data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , datafolder=\"joint_indices_max_400\",  musicfolder = \"music\",window_size = 400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14256730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d30193",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn2 = MotionCollatorConditionalStyle(clip_model=clip_model, bos = 1024, pad = 1025, eos = 1026)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c9f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DATALoader(aist_train_ds , batch_size = 1,collate_fn=collate_fn2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7736989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([191.])\n",
      "['gBR_sBM_cAll_d04_mBR2_ch05']\n",
      "motion torch.Size([1, 192])\n",
      "motion_lengths torch.Size([1])\n",
      "motion_mask torch.Size([1, 192])\n",
      "names (1,)\n",
      "condition torch.Size([1, 191, 128])\n",
      "condition_mask torch.Size([1, 191])\n",
      "style torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "for reg_batch in dl:\n",
    "    break\n",
    "print(reg_batch[\"motion_lengths\"])\n",
    "print(reg_batch[\"names\"])\n",
    "for k,v in reg_batch.items():\n",
    "    print(k , v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d437b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f6ed63",
   "metadata": {},
   "source": [
    "## Style only aist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533d3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = reg_batch[\"motion\"][:, :-1], reg_batch[\"motion\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da843ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a91d2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = trans_model(motion = inp , mask = reg_batch[\"motion_mask\"][:,:-1]  , \\\n",
    "    context = reg_batch[\"condition\"], context_mask = reg_batch[\"condition_mask\"] , \\\n",
    "                     style_context = reg_batch[\"style\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58a37212",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.randint(0,1024 , (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10bfe02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 191, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_batch[\"condition\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9025d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[878, 254,   3, 180, 562, 666,  18,  24, 278, 146, 850,  97, 153, 910,\n",
       "         420, 126, 159, 572, 345,  82, 666,  86, 732, 365, 850, 963,  69, 542,\n",
       "          97, 906, 808, 512, 115, 768, 126, 834, 554, 554, 420, 834,  21, 454,\n",
       "           3, 824,  78, 115, 448, 844, 344, 115, 881, 144, 146, 207, 554, 685,\n",
       "         146,  31, 682, 180, 666, 613,  24,  41, 604, 512, 763, 977,  18, 910,\n",
       "         732, 666,  18, 844, 463, 116, 115, 560,  24, 348, 990, 590,  69,   3,\n",
       "          86, 850, 946,  78,  18, 159,  18, 254, 278, 345, 159, 473, 834, 706,\n",
       "         963, 159, 303]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_model.generate(start,100,context = reg_batch[\"condition\"], context_mask = reg_batch[\"condition_mask\"],style_context = reg_batch[\"style\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a55664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe873a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce13aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0415768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e558fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clip.tokenize([\"dance\"], truncate=True).cuda()\n",
    "encodings = clip_model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96ced140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFiLM(nn.Module):\n",
    "    \"\"\"Feature-wise linear modulation (FiLM) generator.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_channels):\n",
    "        super().__init__()\n",
    "        self.embed_channels = embed_channels\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Mish(), nn.Linear(embed_channels, embed_channels * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, position):\n",
    "        pos_encoding = self.block(position)\n",
    "        pos_encoding = rearrange(pos_encoding, \"b c -> b 1 c\")\n",
    "        scale_shift = pos_encoding.chunk(2, dim=-1)\n",
    "        return scale_shift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9324dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b7c5851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_batch[\"style\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d5a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = nn.Linear(512, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94777aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "style  = proj(reg_batch[\"style\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "66db0094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 191, 35])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_batch[\"condition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5d0a1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = Attention(dim = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ae14ae5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = att(x = torch.randn((2,191,768)) , context = torch.randn((2,191,768)) , mask = reg_batch[\"motion_mask\"][:,:-1] , context_mask = reg_batch[\"condition_mask\"] )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ff93421f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([191, 768])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bedf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da152f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = reg_batch[\"motion\"][:, :-1], reg_batch[\"motion\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e844dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c404d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = trans_model(motion = inp , mask = reg_batch[\"motion_mask\"][:,:-1]  , \\\n",
    "    context = reg_batch[\"condition\"], context_mask = reg_batch[\"condition_mask\"] , style_context = reg_batch[\"style\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b70e9dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 191, 1027])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34f91647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reg_batch[\"names\"][0].split(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c9a96f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"t2m\" in [\"t2m\" , \"aist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eba192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dance2'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([\"Dance1\" , \"Dance2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba0530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
