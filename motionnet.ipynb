{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f08df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "A40\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcc57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# from os.path import join as pjoin\n",
    "# import random\n",
    "# import codecs as cs\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# class VQMotionDataset(data.Dataset):\n",
    "#     def __init__(self, dataset_name, window_size = 64, unit_length = 4):\n",
    "#         self.window_size = window_size\n",
    "#         self.unit_length = unit_length\n",
    "#         self.dataset_name = dataset_name\n",
    "\n",
    "#         if dataset_name == 't2m':\n",
    "#             self.data_root = '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 22\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = ''\n",
    "\n",
    "#         elif dataset_name == 'kit':\n",
    "#             self.data_root = './dataset/KIT-ML'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = 'checkpoints/kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta'\n",
    "        \n",
    "#         joints_num = self.joints_num\n",
    "\n",
    "#         mean = np.load(pjoin(self.data_root, 'Mean.npy'))\n",
    "#         std = np.load(pjoin(self.data_root, 'Std.npy'))\n",
    "\n",
    "#         split_file = pjoin(self.data_root, 'val.txt')\n",
    "\n",
    "#         self.data = []\n",
    "#         self.lengths = []\n",
    "#         id_list = []\n",
    "#         with cs.open(split_file, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 id_list.append(line.strip())\n",
    "\n",
    "#         for name in tqdm(id_list):\n",
    "#             try:\n",
    "#                 motion = np.load(pjoin(self.motion_dir, name + '.npy'))\n",
    "#                 if motion.shape[0] < self.window_size:\n",
    "#                     continue\n",
    "#                 self.lengths.append(motion.shape[0] - self.window_size)\n",
    "#                 self.data.append(motion)\n",
    "#             except:\n",
    "#                 # Some motion may not exist in KIT dataset\n",
    "#                 pass\n",
    "\n",
    "            \n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "#     def inv_transform(self, data):\n",
    "#         return data * self.std + self.mean\n",
    "    \n",
    "#     def compute_sampling_prob(self) : \n",
    "        \n",
    "#         prob = np.array(self.lengths, dtype=np.float32)\n",
    "#         prob /= np.sum(prob)\n",
    "#         return prob\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         motion = self.data[item]\n",
    "        \n",
    "#         idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "#         motion = motion[idx:idx+self.window_size]\n",
    "#         \"Z Normalization\"\n",
    "#         motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         return motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "191dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 952/952 [00:54<00:00, 17.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet = VQMotionDataset(\"aist\", data_root=\"/srv/scratch/sanisetty3/music_motion/AIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c955e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATALoader(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            num_workers = 0,\n",
    "            shuffle = True,\n",
    "           ):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                #sampler=sampler,\n",
    "                                                num_workers=num_workers,\n",
    "                                                #collate_fn=collate_fn,\n",
    "                                                drop_last = True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ffc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(trainSet , 2)\n",
    "train_loader_iter = cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f6baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c04284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 263])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed7f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l2\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70a8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQMotionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9c31aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantize(\n",
       "  (project_in): Identity()\n",
       "  (project_out): Identity()\n",
       "  (_codebook): EuclideanCodebook()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f567b04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_motion , indices, commit_loss = model(gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad74b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(pred_motion, gt_motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "fbc4802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "04a27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*trainSet.std+trainSet.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 263])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6576efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(pred_motion[0:1].detach().cpu()), \"aist0_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4cd12",
   "metadata": {},
   "source": [
    "## Variable motion lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import random\n",
    "import codecs as cs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63beb225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "8685900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionCollator():\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.bos = torch.LongTensor(([0]))\n",
    "        self.eos = torch.LongTensor(([2]))\n",
    "        self.pad = torch.LongTensor(([1]))\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        \n",
    "\n",
    "        pad_batch_inputs = []\n",
    "        pad_batch_mask = []\n",
    "        motion_lengths = []\n",
    "        names = []\n",
    "        max_len = max([sample.shape[0] for sample, name in samples])\n",
    "\n",
    "\n",
    "        for inp,name in samples:\n",
    "            n,d = inp.shape\n",
    "            diff = max_len - n\n",
    "            mask = torch.BoolTensor([1]*n + [0]*diff)\n",
    "            padded = torch.concatenate((torch.tensor(inp) , torch.ones((diff,d))*self.pad))\n",
    "            pad_batch_inputs.append(padded)\n",
    "            pad_batch_mask.append(mask)\n",
    "            motion_lengths.append(n)\n",
    "            names.append(name)\n",
    "\n",
    "    \n",
    "        batch = {\n",
    "            \"motion\": torch.stack(pad_batch_inputs , 0),\n",
    "            \"motion_length\": torch.Tensor(motion_lengths),\n",
    "            \"motion_mask\" : torch.stack(pad_batch_mask , 0),\n",
    "            \"names\" : np.array(names)\n",
    "\n",
    "        }\n",
    "\n",
    "   \n",
    "        return batch    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd528348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from core.datasets import vqa_motion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab75ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 864.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_ds = vqa_motion_dataset.VQMotionDataset(\"t2m\",split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0a383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "033ffc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1092.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b1aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea48283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(train_ds,\n",
    "                                                4,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5b493aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88125b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94080757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d2a244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 74, 263])\n",
      "tensor([71., 74., 61., 64.])\n"
     ]
    }
   ],
   "source": [
    "gt_motion = batch[\"motion\"]\n",
    "print(gt_motion.shape)\n",
    "print(batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4486523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 80\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804784ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129845b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cfab19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_len = int(batch.get(\"motion_lengths\" , [gt_motion.shape[1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fec5f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 263])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[:,:motion_len,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "685cc630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a9be4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import MotionTransformer, MotionDecoder, LinearEmbedding, VQMotionModel\n",
    "from core.quantization.core_vq import VectorQuantization\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd7d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5902d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b312d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "pred_motion , indices, commit_loss = model(batch[\"motion\"].cuda() , mask = batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e001a301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d8dd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l1\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49169cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1719, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "317aeed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2821, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc.forward_vel(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "eecfcf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionTransformer(\n",
    "        inp_dim =263,\n",
    "        max_seq_len = 200,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        attn_layers = Encoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "83397829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_motion = motionEncoder(batch[\"motion\"] , mask = batch[\"motion_mask\"])\n",
    "print(encoded_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "af436068",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization(\n",
    "        dim = 768,\n",
    "        codebook_dim = 128,\n",
    "        codebook_size = 1024,\n",
    "        decay = 0.95,\n",
    "        commitment_weight = 1,\n",
    "        kmeans_init = True,\n",
    "        threshold_ema_dead_code = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "220a3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:350: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion)\n",
    "print(quantized_enc_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "89f78d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = vq.codebook.t()\n",
    "embed.pow(2).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "c43e4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_motion[:,:,:128]\n",
    "shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "58bbb7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801,  0.7024,  1.5966,  ...,  0.7535, -1.5004, -0.4283],\n",
       "        [-0.2908,  0.5800,  1.6096,  ...,  0.4755, -1.5223, -0.3614],\n",
       "        [-0.2561,  0.4115,  1.5109,  ...,  0.1626, -1.4867, -0.2438],\n",
       "        ...,\n",
       "        [ 0.5579,  0.1785,  0.0483,  ..., -0.8444, -0.6883,  1.2005],\n",
       "        [ 0.5358,  0.1656,  0.1023,  ..., -0.8245, -0.6591,  1.2180],\n",
       "        [ 0.5378,  0.0897,  0.1818,  ..., -0.8273, -0.6676,  1.2187]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(x, \"... d -> (...) d\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "eea02234",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = -(\n",
    "    x.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * x @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b5a17f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "5288c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = dist.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "bdfa5861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "57283217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = rearrange(batch[\"motion_mask\"], \"... -> (...)\")\n",
    "inp_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a4c9cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.finfo(dist.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b089e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d3d19a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6291e+02, -1.6781e+02, -1.2629e+02,  ..., -3.8359e+11,\n",
       "        -3.0852e+11, -4.7003e+11], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "4e3090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = (dist + dist.masked_fill(~inp_mask[...,None] , mask_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7941437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "769477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind2 = dist2.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "2795abd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind2[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "8b51456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot = F.one_hot(embed_ind2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "e959e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = vq._codebook.postprocess_emb(embed_ind, shape)\n",
    "quantize = vq._codebook.dequantize(embed_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "9697ea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "3548ff15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "393e948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 128])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "cf8ca363",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot2 = (embed_onehot * inp_mask[...,None] ).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "45701775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "0e2d99bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T@embed_onehot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "c2493e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26c6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "0b727116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 768])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "61541d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "1e267fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion , batch[\"motion_mask\"])\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "5d3bf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionDecoder(\n",
    "    dim = 768,\n",
    "    logit_dim = 263,\n",
    "    attn_layers = Decoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "8bf67db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion = motionDecoder(quantized_enc_motion, inp_mask = batch[\"motion_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "214e5c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "baa2beaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 263])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion[batch[\"motion_mask\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "0420cd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "1035a8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88422.6406, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(batch[\"motion\"] * batch[\"motion_mask\"][...,None] , pred_motion*batch[\"motion\"] * batch[\"motion_mask\"][...,None], reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8055cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de1c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "94dfb090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1263)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"] .numel()/(batch[\"motion_mask\"].sum()*263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "9cf35953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112564"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "22c48451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "417b4988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99940)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].sum()*263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80c776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9a6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35bee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17985817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea65ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1c4dcef",
   "metadata": {},
   "source": [
    "## Decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470e10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "      \n",
    "    def __init__(self, my_dict):\n",
    "          \n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9718dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', default=\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", help=\"folder with train and test data\")\n",
    "parser.add_argument('--pretrained', default='')\n",
    "parser.add_argument('--resume', default=True, type = bool)\n",
    "parser.add_argument('--output_dir', default=\"./checkpoints/vq_768_128\")\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--fp16', default=True, type=bool)\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='aist', help=\"t2m or kit or aist\")\n",
    "parser.add_argument('--var_len', default=False, type=bool)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_bs', default=64, type=int,)\n",
    "parser.add_argument('--eval_bs', default=64, type=int,)\n",
    "parser.add_argument('--gradient_accumulation_steps', default=4, type=int,)\n",
    "\n",
    "parser.add_argument('--motion_dim', type=int, default=263, help='Input motion dimension dimension')\n",
    "parser.add_argument('--enc_dec_dim', type=int, default=768, help='Encoder and Decoder dimension')\n",
    "parser.add_argument('--depth', type=int, default=12, help='Encoder Decoder depth')\n",
    "parser.add_argument('--heads', type=int, default=10, help='Encoder Decoder number of heads')\n",
    "parser.add_argument('--codebook_dim', type=int, default=128, help='codeboook dimension')\n",
    "parser.add_argument('--codebook_size', type=int, default=1024, help='number of codebook embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--commit\", type=float, default=1, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss_vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons_loss', type=str, default='l1_smooth', help='reconstruction loss')\n",
    "parser.add_argument('--max_seq_length', type=int, default=200, help='max sequence length')\n",
    "\n",
    "parser.add_argument(\"--num_train_iters\",  default=500000,type=int)\n",
    "parser.add_argument(\"--save_steps\",  default=5000,type=int)\n",
    "parser.add_argument(\"--logging_steps\",  default=10,type=int)\n",
    "parser.add_argument(\"--wandb_every\",  default=100,type=int)\n",
    "parser.add_argument(\"--evaluate_every\",  default=10000,type=int)\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument('--warmup_steps', default=4000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--learning_rate', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "parser.add_argument('--lr_scheduler_type', default=\"cosine\", help=\"learning rate schedule type\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cf867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4c70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "759f76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "argss = Dict2Class(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3668569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25839a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "\n",
    "model = VQMotionModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "801e22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\", map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc3af2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "192876a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c18d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python VQ_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "7dc089de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "f27ecafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = torch.randint(0,1024,(1,40))\n",
    "quantized, out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "db57f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[1:2].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "beb573e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "d9b12da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0577)\n",
      "tensor(1.9838)\n",
      "\n",
      "\n",
      "tensor(0.1757)\n",
      "tensor(2.9448)\n",
      "\n",
      "\n",
      "tensor(0.2407)\n",
      "tensor(1.6494)\n",
      "\n",
      "\n",
      "tensor(0.1197)\n",
      "tensor(0.4883)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(F.mse_loss(batch[\"motion\"][i,:40,:].cpu() , out_motion[i,:40,:].cpu()))\n",
    "    print(F.mse_loss(batch[\"motion\"][i,40:int(batch[\"motion_length\"][i]),:].cpu() , out_motion[i,40:int(batch[\"motion_length\"][i]),:].cpu()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d413cc",
   "metadata": {},
   "source": [
    "## Music codes to motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4106161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f2f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b005d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9beecc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc2s = msc[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccb84889",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(torch.Tensor(msc2s.T)[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ea3b7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 263])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af595741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(decoded_motion_features.detach().cpu()), \"mus_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9ad5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b60058fb",
   "metadata": {},
   "source": [
    "## VQ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d74f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"t2m\"\n",
    "exp_name = \"vq_768_128\"\n",
    "out_dir = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\"\n",
    "out_dir = os.path.join(out_dir, f'{exp_name}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b1ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils_model.get_logger(out_dir)\n",
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9489be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:11<00:00, 386.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "\n",
    "checkpoint_dir = \"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints\"\n",
    "# dataset_opt_path = f'{checkpoint_dir}/kit/Comp_v6_KLD005/opt.txt' if dataname == 'kit' else f'{checkpoint_dir}/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "nb_joints = 21 if dataname == 'kit' else 22\n",
    "\n",
    "val_loader = dataset_TM_eval.DATALoader(dataname, True, 4, w_vectorizer, unit_length=2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d383135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "repeat_time = 20\n",
    "for i in tqdm(range(repeat_time)):\n",
    "    best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(out_dir, val_loader, model, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n",
    "    fid.append(best_fid)\n",
    "    div.append(best_div)\n",
    "    top1.append(best_top1)\n",
    "    top2.append(best_top2)\n",
    "    top3.append(best_top3)\n",
    "    matching.append(best_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3173dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('final result:')\n",
    "print('fid: ', sum(fid)/repeat_time)\n",
    "print('div: ', sum(div)/repeat_time)\n",
    "print('top1: ', sum(top1)/repeat_time)\n",
    "print('top2: ', sum(top2)/repeat_time)\n",
    "print('top3: ', sum(top3)/repeat_time)\n",
    "print('matching: ', sum(matching)/repeat_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfbd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140bd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b584aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601df37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af1ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8160b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2a397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56513117",
   "metadata": {},
   "source": [
    "## T2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "617d876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "24e1ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afde12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdc88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "108c1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "3d9c7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from utils.eval_trans import *\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e8dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "bf8740ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = option_vq.get_args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "6e5e60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "5e3f3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "dataset_opt_path = 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "15a4a9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_opt.dim_movement_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "bc55b242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'checkpoints_dir': './checkpoints',\n",
       " 'dataset_name': 't2m',\n",
       " 'decomp_name': 'Decomp_SP001_SM001_H512',\n",
       " 'dim_att_vec': 512,\n",
       " 'dim_dec_hidden': 1024,\n",
       " 'dim_movement2_dec_hidden': 512,\n",
       " 'dim_movement_dec_hidden': 512,\n",
       " 'dim_movement_enc_hidden': 512,\n",
       " 'dim_movement_latent': 512,\n",
       " 'dim_msd_hidden': 512,\n",
       " 'dim_pos_hidden': 1024,\n",
       " 'dim_pri_hidden': 1024,\n",
       " 'dim_seq_de_hidden': 512,\n",
       " 'dim_seq_en_hidden': 512,\n",
       " 'dim_text_hidden': 512,\n",
       " 'dim_z': 128,\n",
       " 'early_stop_count': 3,\n",
       " 'estimator_mod': 'bigru',\n",
       " 'eval_every_e': 5,\n",
       " 'feat_bias': 5,\n",
       " 'fixed_steps': 5,\n",
       " 'gpu_id': 1,\n",
       " 'input_z': False,\n",
       " 'is_continue': False,\n",
       " 'is_train': False,\n",
       " 'lambda_fake': 10,\n",
       " 'lambda_gan_l': 0.1,\n",
       " 'lambda_gan_mt': 0.1,\n",
       " 'lambda_gan_mv': 0.1,\n",
       " 'lambda_kld': 0.005,\n",
       " 'lambda_rec': 1,\n",
       " 'lambda_rec_init': 1,\n",
       " 'lambda_rec_mot': 1,\n",
       " 'lambda_rec_mov': 1,\n",
       " 'log_every': 50,\n",
       " 'lr': 0.0002,\n",
       " 'max_sub_epoch': 50,\n",
       " 'max_text_len': 20,\n",
       " 'n_layers_dec': 1,\n",
       " 'n_layers_msd': 2,\n",
       " 'n_layers_pos': 1,\n",
       " 'n_layers_pri': 1,\n",
       " 'n_layers_seq_de': 2,\n",
       " 'n_layers_seq_en': 1,\n",
       " 'name': 'Comp_v6_KLD005',\n",
       " 'num_experts': 4,\n",
       " 'save_every_e': 10,\n",
       " 'save_latest': 500,\n",
       " 'text_enc_mod': 'bigru',\n",
       " 'tf_ratio': 0.4,\n",
       " 'unit_length': 4,\n",
       " 'which_epoch': 'finest',\n",
       " 'save_root': './checkpoints/t2m/Comp_v6_KLD005',\n",
       " 'model_dir': './checkpoints/t2m/Comp_v6_KLD005/model',\n",
       " 'meta_dir': './checkpoints/t2m/Comp_v6_KLD005/meta',\n",
       " 'data_root': './dataset/HumanML3D/',\n",
       " 'motion_dir': './dataset/HumanML3D/new_joint_vecs',\n",
       " 'text_dir': './dataset/HumanML3D/texts',\n",
       " 'joints_num': 22,\n",
       " 'dim_pose': 263,\n",
       " 'max_motion_length': 196,\n",
       " 'max_motion_frame': 196,\n",
       " 'max_motion_token': 55,\n",
       " 'dim_word': 300,\n",
       " 'num_classes': 50,\n",
       " 'device': device(type='cuda'),\n",
       " 'dim_pos_ohot': 15,\n",
       " 'dim_motion_hidden': 1024,\n",
       " 'dim_coemb_hidden': 512}"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(wrapper_opt)[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991a127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "79c66ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1332.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(\"t2m\", True, 4, w_vectorizer, unit_length=2**2, num_workers = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "1101017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "14a75cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "619a3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f319386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "a1038320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([192, 192, 180, 136])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "2250154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion , ind , com_loss = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(args.out_dir, val_loader, net, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "9c98b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "\n",
    "\n",
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)\n",
    "motion = motion.cuda()\n",
    "et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, motion, m_length)\n",
    "bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "\n",
    "pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "\n",
    "for i in range(bs):\n",
    "    pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "    pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "    pred_pose, ind,loss_commit = model(motion[i:i+1, :m_length[i]])\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "    pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#     if savenpy:\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_gt.npy'), pose_xyz[:, :m_length[i]].cpu().numpy())\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_pred.npy'), pred_xyz.detach().cpu().numpy())\n",
    "\n",
    "    pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "    if i < min(4, bs):\n",
    "        draw_org.append(pose_xyz)\n",
    "        draw_pred.append(pred_xyz)\n",
    "        draw_text.append(caption[i])\n",
    "\n",
    "et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length)\n",
    "\n",
    "motion_pred_list.append(em_pred)\n",
    "motion_annotation_list.append(em)\n",
    "\n",
    "temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision_real += temp_R\n",
    "matching_score_real += temp_match\n",
    "temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision += temp_R\n",
    "matching_score_pred += temp_match\n",
    "\n",
    "nb_sample += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "c81020b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "a46dc656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7bf54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a336089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
