{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f08df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "TITAN Xp\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader\n",
    "from einops import rearrange, reduce, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcc57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# from os.path import join as pjoin\n",
    "# import random\n",
    "# import codecs as cs\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# class VQMotionDataset(data.Dataset):\n",
    "#     def __init__(self, dataset_name, window_size = 64, unit_length = 4):\n",
    "#         self.window_size = window_size\n",
    "#         self.unit_length = unit_length\n",
    "#         self.dataset_name = dataset_name\n",
    "\n",
    "#         if dataset_name == 't2m':\n",
    "#             self.data_root = '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 22\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = ''\n",
    "\n",
    "#         elif dataset_name == 'kit':\n",
    "#             self.data_root = './dataset/KIT-ML'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = 'checkpoints/kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta'\n",
    "        \n",
    "#         joints_num = self.joints_num\n",
    "\n",
    "#         mean = np.load(pjoin(self.data_root, 'Mean.npy'))\n",
    "#         std = np.load(pjoin(self.data_root, 'Std.npy'))\n",
    "\n",
    "#         split_file = pjoin(self.data_root, 'val.txt')\n",
    "\n",
    "#         self.data = []\n",
    "#         self.lengths = []\n",
    "#         id_list = []\n",
    "#         with cs.open(split_file, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 id_list.append(line.strip())\n",
    "\n",
    "#         for name in tqdm(id_list):\n",
    "#             try:\n",
    "#                 motion = np.load(pjoin(self.motion_dir, name + '.npy'))\n",
    "#                 if motion.shape[0] < self.window_size:\n",
    "#                     continue\n",
    "#                 self.lengths.append(motion.shape[0] - self.window_size)\n",
    "#                 self.data.append(motion)\n",
    "#             except:\n",
    "#                 # Some motion may not exist in KIT dataset\n",
    "#                 pass\n",
    "\n",
    "            \n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "#     def inv_transform(self, data):\n",
    "#         return data * self.std + self.mean\n",
    "    \n",
    "#     def compute_sampling_prob(self) : \n",
    "        \n",
    "#         prob = np.array(self.lengths, dtype=np.float32)\n",
    "#         prob /= np.sum(prob)\n",
    "#         return prob\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         motion = self.data[item]\n",
    "        \n",
    "#         idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "#         motion = motion[idx:idx+self.window_size]\n",
    "#         \"Z Normalization\"\n",
    "#         motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         return motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "191dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 952/952 [00:54<00:00, 17.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet = VQMotionDataset(\"aist\", data_root=\"/srv/scratch/sanisetty3/music_motion/AIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c955e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATALoader(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            num_workers = 0,\n",
    "            shuffle = True,\n",
    "           ):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                #sampler=sampler,\n",
    "                                                num_workers=num_workers,\n",
    "                                                #collate_fn=collate_fn,\n",
    "                                                drop_last = True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ffc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(trainSet , 2)\n",
    "train_loader_iter = cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f6baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c04284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 263])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed7f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l2\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70a8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQMotionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54471dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantize(\n",
       "  (project_in): Identity()\n",
       "  (project_out): Identity()\n",
       "  (_codebook): EuclideanCodebook()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f567b04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_motion , indices, commit_loss = model(gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad74b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(pred_motion, gt_motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "fbc4802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "04a27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*trainSet.std+trainSet.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 263])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6576efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(pred_motion[0:1].detach().cpu()), \"aist0_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4b785",
   "metadata": {},
   "source": [
    "## Variable motion lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae65dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import random\n",
    "import codecs as cs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e599ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQMotionDataset(data.Dataset):\n",
    "    def __init__(self, dataset_name, data_root, max_length_seconds = 10, min_length_seconds = 1.5, fps = 20, split = \"train\"):\n",
    "        self.fps = fps\n",
    "        self.min_motion_length = self.fps*min_length_seconds\n",
    "        self.max_motion_length = self.fps*max_length_seconds\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.window_size = 40\n",
    "\n",
    "        if dataset_name == 't2m':\n",
    "            self.data_root = data_root\n",
    "            self.motion_dir = os.path.join(self.data_root, 'new_joint_vecs')\n",
    "            self.text_dir = os.path.join(self.data_root, 'texts')\n",
    "            self.joints_num = 22\n",
    "#             self.max_motion_length = max_motion_length\n",
    "            self.meta_dir = ''\n",
    "\n",
    "        if dataset_name == 'aist':\n",
    "            self.data_root = data_root\n",
    "            self.motion_dir = os.path.join(self.data_root, 'new_joint_vecs')\n",
    "            self.text_dir = os.path.join(self.data_root, 'texts')\n",
    "            self.joints_num = 22\n",
    "#             self.max_motion_length = max_motion_length\n",
    "            self.meta_dir = ''\n",
    "\n",
    "        elif dataset_name == 'kit':\n",
    "            self.data_root = data_root\n",
    "            #'./dataset/KIT-ML'\n",
    "            self.motion_dir = os.path.join(self.data_root, 'new_joint_vecs')\n",
    "            self.text_dir = os.path.join(self.data_root, 'texts')\n",
    "            self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = max_motion_length\n",
    "            self.meta_dir = ''\n",
    "        \n",
    "        joints_num = self.joints_num\n",
    "\n",
    "        mean = np.load(os.path.join(self.data_root, 'Mean.npy'))\n",
    "        std = np.load(os.path.join(self.data_root, 'Std.npy'))\n",
    "\n",
    "        split_file = os.path.join(self.data_root, f'{split}.txt')\n",
    "\n",
    "        self.data = []\n",
    "        self.lengths = []\n",
    "        self.id_list = []\n",
    "        with cs.open(split_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                self.id_list.append(line.strip())\n",
    "\n",
    "        for name in tqdm(self.id_list):\n",
    "            try:\n",
    "                motion = np.load(os.path.join(self.motion_dir, name + '.npy'))\n",
    "                if motion.shape[0] < self.window_size:\n",
    "                    continue\n",
    "                self.lengths.append(motion.shape[0] - self.window_size)\n",
    "                self.data.append(motion)\n",
    "            except:\n",
    "                # Some motion may not exist in KIT dataset\n",
    "                pass\n",
    "\n",
    "            \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "    def inv_transform(self, data):\n",
    "        return data * self.std + self.mean\n",
    "    \n",
    "    def compute_sampling_prob(self) : \n",
    "        \n",
    "        prob = np.array(self.lengths, dtype=np.float32)\n",
    "        prob /= np.sum(prob)\n",
    "        return prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        motion = self.data[item]\n",
    "        \n",
    "        motion_len = len(motion)\n",
    "        \n",
    "        self.window_size = np.random.randint(self.min_motion_length , min(motion_len , self.max_motion_length))\n",
    "        \n",
    "        \n",
    "        idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "        motion = motion[idx:idx+self.window_size]\n",
    "#         print( motion_len , self.window_size , idx , motion.shape)\n",
    "\n",
    "        \"Z Normalization\"\n",
    "        motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         if self.split in [\"val\", \"test\" , \"render\"]:\n",
    "        return motion , self.id_list[item]\n",
    "\n",
    "\n",
    "#         return motion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "50165356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionCollator():\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.bos = torch.LongTensor(([0]))\n",
    "        self.eos = torch.LongTensor(([2]))\n",
    "        self.pad = torch.LongTensor(([1]))\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        \n",
    "\n",
    "        pad_batch_inputs = []\n",
    "        pad_batch_mask = []\n",
    "        motion_lengths = []\n",
    "        names = []\n",
    "        max_len = max([sample.shape[0] for sample, name in samples])\n",
    "\n",
    "\n",
    "        for inp,name in samples:\n",
    "            n,d = inp.shape\n",
    "            diff = max_len - n\n",
    "            mask = torch.BoolTensor([1]*n + [0]*diff)\n",
    "            padded = torch.concatenate((torch.tensor(inp) , torch.ones((diff,d))*self.pad))\n",
    "            pad_batch_inputs.append(padded)\n",
    "            pad_batch_mask.append(mask)\n",
    "            motion_lengths.append(n)\n",
    "            names.append(name)\n",
    "\n",
    "    \n",
    "        batch = {\n",
    "            \"motion\": torch.stack(pad_batch_inputs , 0),\n",
    "            \"motion_length\": torch.Tensor(motion_lengths),\n",
    "            \"motion_mask\" : torch.stack(pad_batch_mask , 0),\n",
    "            \"names\" : np.array(names)\n",
    "\n",
    "        }\n",
    "\n",
    "   \n",
    "        return batch    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b957d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1423.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQMotionDataset(\"t2m\",split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "ada12479",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f7e22169",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds,\n",
    "                                                4,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "7b679d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "9d04a435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "8d2c4b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "d29a6525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177, 263])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "87aff6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import MotionTransformer, MotionDecoder, LinearEmbedding\n",
    "from core.quantization.core_vq import VectorQuantization\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d263aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionTransformer(\n",
    "        inp_dim =263,\n",
    "        max_seq_len = 200,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        attn_layers = Encoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "7a89681d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_motion = motionEncoder(batch[\"motion\"] , mask = batch[\"motion_mask\"])\n",
    "print(encoded_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "cf3fcb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization(\n",
    "        dim = 768,\n",
    "        codebook_dim = 128,\n",
    "        codebook_size = 1024,\n",
    "        decay = 0.95,\n",
    "        commitment_weight = 1,\n",
    "        kmeans_init = True,\n",
    "        threshold_ema_dead_code = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7ec75e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:350: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion)\n",
    "print(quantized_enc_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "bcb2398e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = vq.codebook.t()\n",
    "embed.pow(2).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "6170d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_motion[:,:,:128]\n",
    "shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "c5bb434c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801,  0.7024,  1.5966,  ...,  0.7535, -1.5004, -0.4283],\n",
       "        [-0.2908,  0.5800,  1.6096,  ...,  0.4755, -1.5223, -0.3614],\n",
       "        [-0.2561,  0.4115,  1.5109,  ...,  0.1626, -1.4867, -0.2438],\n",
       "        ...,\n",
       "        [ 0.5579,  0.1785,  0.0483,  ..., -0.8444, -0.6883,  1.2005],\n",
       "        [ 0.5358,  0.1656,  0.1023,  ..., -0.8245, -0.6591,  1.2180],\n",
       "        [ 0.5378,  0.0897,  0.1818,  ..., -0.8273, -0.6676,  1.2187]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(x, \"... d -> (...) d\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "7a9080c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = -(\n",
    "    x.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * x @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d8c41254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "f7e1ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = dist.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "31242ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bc2a649b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = rearrange(batch[\"motion_mask\"], \"... -> (...)\")\n",
    "inp_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b26260dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.finfo(dist.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20002e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "81f398ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6291e+02, -1.6781e+02, -1.2629e+02,  ..., -3.8359e+11,\n",
       "        -3.0852e+11, -4.7003e+11], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "39685737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = (dist + dist.masked_fill(~inp_mask[...,None] , mask_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bcbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b4e81e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind2 = dist2.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "7abcf2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind2[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5f26670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot = F.one_hot(embed_ind2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "9856c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = vq._codebook.postprocess_emb(embed_ind, shape)\n",
    "quantize = vq._codebook.dequantize(embed_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "85de559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "7e4207ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "e1576426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 128])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "0a789e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot2 = (embed_onehot * inp_mask[...,None] ).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "7c25f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "75c211f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T@embed_onehot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "20e19dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ffc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "baf4c0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 768])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "1e4cc3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "0bf656d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion , batch[\"motion_mask\"])\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "ae174a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionDecoder(\n",
    "    dim = 768,\n",
    "    logit_dim = 263,\n",
    "    attn_layers = Decoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "3638ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion = motionDecoder(quantized_enc_motion, inp_mask = batch[\"motion_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "8c267b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "0a5480c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 263])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion[batch[\"motion_mask\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "2d1c328f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "941d49a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88422.6406, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(batch[\"motion\"] * batch[\"motion_mask\"][...,None] , pred_motion*batch[\"motion\"] * batch[\"motion_mask\"][...,None], reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87973d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbd1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a0a27d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1263)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"] .numel()/(batch[\"motion_mask\"].sum()*263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "80a70339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112564"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "9a973a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "d8011dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99940)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].sum()*263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c6bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaff569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537185a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc9e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6ad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279c8db5",
   "metadata": {},
   "source": [
    "## Decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca388f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "      \n",
    "    def __init__(self, my_dict):\n",
    "          \n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370726cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', default=\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", help=\"folder with train and test data\")\n",
    "parser.add_argument('--pretrained', default='')\n",
    "parser.add_argument('--resume', default=True, type = bool)\n",
    "parser.add_argument('--output_dir', default=\"./checkpoints/vq_768_128\")\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--fp16', default=True, type=bool)\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='aist', help=\"t2m or kit or aist\")\n",
    "parser.add_argument('--var_len', default=False, type=bool)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_bs', default=64, type=int,)\n",
    "parser.add_argument('--eval_bs', default=64, type=int,)\n",
    "parser.add_argument('--gradient_accumulation_steps', default=4, type=int,)\n",
    "\n",
    "parser.add_argument('--motion_dim', type=int, default=263, help='Input motion dimension dimension')\n",
    "parser.add_argument('--enc_dec_dim', type=int, default=768, help='Encoder and Decoder dimension')\n",
    "parser.add_argument('--depth', type=int, default=12, help='Encoder Decoder depth')\n",
    "parser.add_argument('--heads', type=int, default=10, help='Encoder Decoder number of heads')\n",
    "parser.add_argument('--codebook_dim', type=int, default=128, help='codeboook dimension')\n",
    "parser.add_argument('--codebook_size', type=int, default=1024, help='number of codebook embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--commit\", type=float, default=1, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss_vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons_loss', type=str, default='l1_smooth', help='reconstruction loss')\n",
    "parser.add_argument('--max_seq_length', type=int, default=200, help='max sequence length')\n",
    "\n",
    "parser.add_argument(\"--num_train_iters\",  default=500000,type=int)\n",
    "parser.add_argument(\"--save_steps\",  default=5000,type=int)\n",
    "parser.add_argument(\"--logging_steps\",  default=10,type=int)\n",
    "parser.add_argument(\"--wandb_every\",  default=100,type=int)\n",
    "parser.add_argument(\"--evaluate_every\",  default=10000,type=int)\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument('--warmup_steps', default=4000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--learning_rate', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "parser.add_argument('--lr_scheduler_type', default=\"cosine\", help=\"learning rate schedule type\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ea8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1bb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0b08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "argss = Dict2Class(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea5aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1045bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "\n",
    "model = VQMotionModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b40aac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\", map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32711115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd106a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ba8fb779f1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \"\"\"\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a487f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python VQ_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "6896fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "46999ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = torch.randint(0,1024,(1,40))\n",
    "quantized, out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "bec0751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[1:2].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "7d0f38a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "7896f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0577)\n",
      "tensor(1.9838)\n",
      "\n",
      "\n",
      "tensor(0.1757)\n",
      "tensor(2.9448)\n",
      "\n",
      "\n",
      "tensor(0.2407)\n",
      "tensor(1.6494)\n",
      "\n",
      "\n",
      "tensor(0.1197)\n",
      "tensor(0.4883)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(F.mse_loss(batch[\"motion\"][i,:40,:].cpu() , out_motion[i,:40,:].cpu()))\n",
    "    print(F.mse_loss(batch[\"motion\"][i,40:int(batch[\"motion_length\"][i]),:].cpu() , out_motion[i,40:int(batch[\"motion_length\"][i]),:].cpu()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75096b30",
   "metadata": {},
   "source": [
    "## Music codes to motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb7040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3bcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60d5243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7f4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc2s = msc[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2de5f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(torch.Tensor(msc2s.T)[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00a9a5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 263])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9ce0122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(decoded_motion_features.detach().cpu()), \"mus_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f3ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4449cd",
   "metadata": {},
   "source": [
    "## VQ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "972746dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.models.vqvae as vqvae\n",
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ddd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"t2m\"\n",
    "exp_name = \"vq_768_128\"\n",
    "out_dir = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\"\n",
    "out_dir = os.path.join(out_dir, f'{exp_name}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07572023",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils_model.get_logger(out_dir)\n",
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4de7a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:11<00:00, 386.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "\n",
    "checkpoint_dir = \"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints\"\n",
    "# dataset_opt_path = f'{checkpoint_dir}/kit/Comp_v6_KLD005/opt.txt' if dataname == 'kit' else f'{checkpoint_dir}/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "nb_joints = 21 if dataname == 'kit' else 22\n",
    "\n",
    "val_loader = dataset_TM_eval.DATALoader(dataname, True, 4, w_vectorizer, unit_length=2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "repeat_time = 20\n",
    "for i in range(repeat_time):\n",
    "    best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(out_dir, val_loader, model, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n",
    "    fid.append(best_fid)\n",
    "    div.append(best_div)\n",
    "    top1.append(best_top1)\n",
    "    top2.append(best_top2)\n",
    "    top3.append(best_top3)\n",
    "    matching.append(best_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-04 12:33:06.006401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 12:33:06.207210: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-04 12:33:07.043432: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /coc/pskynet2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2023-04-04 12:33:07.043569: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /coc/pskynet2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2023-04-04 12:33:07.043583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n",
      "100%|█████████████████████████████████████| 4384/4384 [00:02<00:00, 1912.08it/s]\n",
      "4648 4648\n",
      "Pointer Pointing at 0\n",
      "2023-04-04 12:33:15,184 INFO loading checkpoint from /coc/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\n",
      "2023-04-04 12:47:16,920 INFO --> \t Eva. Iter 0 :, FID. 17.9142, Diversity Real. 9.2507, Diversity. 6.2813, R_precision_real. [0.86682444 0.97332186 0.99333046], R_precision. [0.59638554 0.83433735 0.9419105 ], matching_score_real. 2.9669558100158704, matching_score_pred. 5.959107627967138\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t FID Improved from 1000.00000 to 17.91424 !!!\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t Diversity Improved from 100.00000 to 6.28133 !!!\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t Top1 Improved from 0.0000 to 0.5964 !!!\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t Top2 Improved from 0.0000 to 0.8343 !!!\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t Top3 Improved from 0.0000 to 0.9419 !!!\n",
      "2023-04-04 12:47:17,353 INFO --> --> \t matching_score Improved from 100.00000 to 5.95911 !!!\n",
      "2023-04-04 12:49:38,327 INFO --> \t Eva. Iter 0 :, FID. 17.8229, Diversity Real. 9.3512, Diversity. 6.0950, R_precision_real. [0.866179   0.9707401  0.99246988], R_precision. [0.60864888 0.83820998 0.94384682], matching_score_real. 2.988310998155, matching_score_pred. 5.9501948748521265\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t FID Improved from 1000.00000 to 17.82293 !!!\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t Diversity Improved from 100.00000 to 6.09498 !!!\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t Top1 Improved from 0.0000 to 0.6086 !!!\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t Top2 Improved from 0.0000 to 0.8382 !!!\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t Top3 Improved from 0.0000 to 0.9438 !!!\n",
      "2023-04-04 12:49:38,328 INFO --> --> \t matching_score Improved from 100.00000 to 5.95019 !!!\n",
      "2023-04-04 12:52:00,206 INFO --> \t Eva. Iter 0 :, FID. 17.7838, Diversity Real. 9.5355, Diversity. 5.9275, R_precision_real. [0.86660929 0.96987952 0.9939759 ], R_precision. [0.61531842 0.84466437 0.94728916], matching_score_real. 2.9645442925714174, matching_score_pred. 5.930225705731141\n",
      "2023-04-04 12:52:00,206 INFO --> --> \t FID Improved from 1000.00000 to 17.78376 !!!\n",
      "2023-04-04 12:52:00,206 INFO --> --> \t Diversity Improved from 100.00000 to 5.92752 !!!\n",
      "2023-04-04 12:52:00,207 INFO --> --> \t Top1 Improved from 0.0000 to 0.6153 !!!\n",
      "2023-04-04 12:52:00,207 INFO --> --> \t Top2 Improved from 0.0000 to 0.8447 !!!\n",
      "2023-04-04 12:52:00,207 INFO --> --> \t Top3 Improved from 0.0000 to 0.9473 !!!\n",
      "2023-04-04 12:52:00,207 INFO --> --> \t matching_score Improved from 100.00000 to 5.93023 !!!\n",
      "2023-04-04 12:54:21,502 INFO --> \t Eva. Iter 0 :, FID. 17.7544, Diversity Real. 9.4548, Diversity. 6.2342, R_precision_real. [0.87435456 0.97246127 0.99225473], R_precision. [0.60327022 0.83024957 0.94535284], matching_score_real. 2.9910756190549486, matching_score_pred. 5.951792567609305\n",
      "2023-04-04 12:54:21,502 INFO --> --> \t FID Improved from 1000.00000 to 17.75445 !!!\n",
      "2023-04-04 12:54:21,503 INFO --> --> \t Diversity Improved from 100.00000 to 6.23420 !!!\n",
      "2023-04-04 12:54:21,503 INFO --> --> \t Top1 Improved from 0.0000 to 0.6033 !!!\n",
      "2023-04-04 12:54:21,503 INFO --> --> \t Top2 Improved from 0.0000 to 0.8302 !!!\n",
      "2023-04-04 12:54:21,503 INFO --> --> \t Top3 Improved from 0.0000 to 0.9454 !!!\n",
      "2023-04-04 12:54:21,503 INFO --> --> \t matching_score Improved from 100.00000 to 5.95179 !!!\n",
      "2023-04-04 12:56:47,517 INFO --> \t Eva. Iter 0 :, FID. 18.0066, Diversity Real. 9.6570, Diversity. 6.2594, R_precision_real. [0.86488812 0.96944923 0.9939759 ], R_precision. [0.60563683 0.84272806 0.94922547], matching_score_real. 2.9678111822182465, matching_score_pred. 5.96306260789323\n",
      "2023-04-04 12:56:47,517 INFO --> --> \t FID Improved from 1000.00000 to 18.00661 !!!\n",
      "2023-04-04 12:56:47,517 INFO --> --> \t Diversity Improved from 100.00000 to 6.25941 !!!\n",
      "2023-04-04 12:56:47,517 INFO --> --> \t Top1 Improved from 0.0000 to 0.6056 !!!\n",
      "2023-04-04 12:56:47,518 INFO --> --> \t Top2 Improved from 0.0000 to 0.8427 !!!\n",
      "2023-04-04 12:56:47,518 INFO --> --> \t Top3 Improved from 0.0000 to 0.9492 !!!\n",
      "2023-04-04 12:56:47,518 INFO --> --> \t matching_score Improved from 100.00000 to 5.96306 !!!\n",
      "2023-04-04 12:59:09,267 INFO --> \t Eva. Iter 0 :, FID. 17.7530, Diversity Real. 9.4053, Diversity. 6.0664, R_precision_real. [0.8689759  0.97052496 0.991179  ], R_precision. [0.60864888 0.84143718 0.94492255], matching_score_real. 2.9606571994632125, matching_score_pred. 5.9654906386967985\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t FID Improved from 1000.00000 to 17.75298 !!!\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t Diversity Improved from 100.00000 to 6.06639 !!!\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t Top1 Improved from 0.0000 to 0.6086 !!!\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t Top2 Improved from 0.0000 to 0.8414 !!!\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t Top3 Improved from 0.0000 to 0.9449 !!!\n",
      "2023-04-04 12:59:09,268 INFO --> --> \t matching_score Improved from 100.00000 to 5.96549 !!!\n",
      "2023-04-04 13:01:31,538 INFO --> \t Eva. Iter 0 :, FID. 17.8747, Diversity Real. 9.5008, Diversity. 6.1545, R_precision_real. [0.86488812 0.97375215 0.99354561], R_precision. [0.61381239 0.83304647 0.94642857], matching_score_real. 2.972187684242984, matching_score_pred. 5.949640504259251\n",
      "2023-04-04 13:01:31,538 INFO --> --> \t FID Improved from 1000.00000 to 17.87469 !!!\n",
      "2023-04-04 13:01:31,539 INFO --> --> \t Diversity Improved from 100.00000 to 6.15453 !!!\n",
      "2023-04-04 13:01:31,539 INFO --> --> \t Top1 Improved from 0.0000 to 0.6138 !!!\n",
      "2023-04-04 13:01:31,539 INFO --> --> \t Top2 Improved from 0.0000 to 0.8330 !!!\n",
      "2023-04-04 13:01:31,539 INFO --> --> \t Top3 Improved from 0.0000 to 0.9464 !!!\n",
      "2023-04-04 13:01:31,539 INFO --> --> \t matching_score Improved from 100.00000 to 5.94964 !!!\n",
      "2023-04-04 13:03:53,573 INFO --> \t Eva. Iter 0 :, FID. 17.9557, Diversity Real. 9.3399, Diversity. 5.8761, R_precision_real. [0.86574871 0.97138554 0.99225473], R_precision. [0.61123064 0.83304647 0.9475043 ], matching_score_real. 2.9960829058530614, matching_score_pred. 5.969451132821955\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t FID Improved from 1000.00000 to 17.95566 !!!\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t Diversity Improved from 100.00000 to 5.87607 !!!\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t Top1 Improved from 0.0000 to 0.6112 !!!\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t Top2 Improved from 0.0000 to 0.8330 !!!\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t Top3 Improved from 0.0000 to 0.9475 !!!\n",
      "2023-04-04 13:03:53,574 INFO --> --> \t matching_score Improved from 100.00000 to 5.96945 !!!\n",
      "2023-04-04 13:06:15,619 INFO --> \t Eva. Iter 0 :, FID. 17.9119, Diversity Real. 9.6490, Diversity. 6.1839, R_precision_real. [0.86725473 0.9651463  0.98945783], R_precision. [0.61359725 0.83175559 0.94212565], matching_score_real. 3.0143823092242323, matching_score_pred. 5.999458714934922\n",
      "2023-04-04 13:06:15,619 INFO --> --> \t FID Improved from 1000.00000 to 17.91190 !!!\n",
      "2023-04-04 13:06:15,619 INFO --> --> \t Diversity Improved from 100.00000 to 6.18389 !!!\n",
      "2023-04-04 13:06:15,619 INFO --> --> \t Top1 Improved from 0.0000 to 0.6136 !!!\n",
      "2023-04-04 13:06:15,620 INFO --> --> \t Top2 Improved from 0.0000 to 0.8318 !!!\n",
      "2023-04-04 13:06:15,620 INFO --> --> \t Top3 Improved from 0.0000 to 0.9421 !!!\n",
      "2023-04-04 13:06:15,620 INFO --> --> \t matching_score Improved from 100.00000 to 5.99946 !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-04 13:08:37,849 INFO --> \t Eva. Iter 0 :, FID. 17.8940, Diversity Real. 9.6206, Diversity. 5.8583, R_precision_real. [0.87069707 0.97310671 0.99419105], R_precision. [0.60348537 0.83648881 0.94449225], matching_score_real. 2.969936873847154, matching_score_pred. 5.952303482620318\r\n",
      "2023-04-04 13:08:37,849 INFO --> --> \t FID Improved from 1000.00000 to 17.89405 !!!\r\n",
      "2023-04-04 13:08:37,850 INFO --> --> \t Diversity Improved from 100.00000 to 5.85825 !!!\r\n",
      "2023-04-04 13:08:37,850 INFO --> --> \t Top1 Improved from 0.0000 to 0.6035 !!!\r\n",
      "2023-04-04 13:08:37,850 INFO --> --> \t Top2 Improved from 0.0000 to 0.8365 !!!\r\n",
      "2023-04-04 13:08:37,850 INFO --> --> \t Top3 Improved from 0.0000 to 0.9445 !!!\r\n",
      "2023-04-04 13:08:37,850 INFO --> --> \t matching_score Improved from 100.00000 to 5.95230 !!!\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b8d32e",
   "metadata": {},
   "source": [
    "## T2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "11a07ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "ddaf21ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e309ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc1ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "119929a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "55fee9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from utils.eval_trans import *\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78874d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "b43ce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = option_vq.get_args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "4336b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "7d2e5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "dataset_opt_path = 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "8af63cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_opt.dim_movement_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "084c4ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'checkpoints_dir': './checkpoints',\n",
       " 'dataset_name': 't2m',\n",
       " 'decomp_name': 'Decomp_SP001_SM001_H512',\n",
       " 'dim_att_vec': 512,\n",
       " 'dim_dec_hidden': 1024,\n",
       " 'dim_movement2_dec_hidden': 512,\n",
       " 'dim_movement_dec_hidden': 512,\n",
       " 'dim_movement_enc_hidden': 512,\n",
       " 'dim_movement_latent': 512,\n",
       " 'dim_msd_hidden': 512,\n",
       " 'dim_pos_hidden': 1024,\n",
       " 'dim_pri_hidden': 1024,\n",
       " 'dim_seq_de_hidden': 512,\n",
       " 'dim_seq_en_hidden': 512,\n",
       " 'dim_text_hidden': 512,\n",
       " 'dim_z': 128,\n",
       " 'early_stop_count': 3,\n",
       " 'estimator_mod': 'bigru',\n",
       " 'eval_every_e': 5,\n",
       " 'feat_bias': 5,\n",
       " 'fixed_steps': 5,\n",
       " 'gpu_id': 1,\n",
       " 'input_z': False,\n",
       " 'is_continue': False,\n",
       " 'is_train': False,\n",
       " 'lambda_fake': 10,\n",
       " 'lambda_gan_l': 0.1,\n",
       " 'lambda_gan_mt': 0.1,\n",
       " 'lambda_gan_mv': 0.1,\n",
       " 'lambda_kld': 0.005,\n",
       " 'lambda_rec': 1,\n",
       " 'lambda_rec_init': 1,\n",
       " 'lambda_rec_mot': 1,\n",
       " 'lambda_rec_mov': 1,\n",
       " 'log_every': 50,\n",
       " 'lr': 0.0002,\n",
       " 'max_sub_epoch': 50,\n",
       " 'max_text_len': 20,\n",
       " 'n_layers_dec': 1,\n",
       " 'n_layers_msd': 2,\n",
       " 'n_layers_pos': 1,\n",
       " 'n_layers_pri': 1,\n",
       " 'n_layers_seq_de': 2,\n",
       " 'n_layers_seq_en': 1,\n",
       " 'name': 'Comp_v6_KLD005',\n",
       " 'num_experts': 4,\n",
       " 'save_every_e': 10,\n",
       " 'save_latest': 500,\n",
       " 'text_enc_mod': 'bigru',\n",
       " 'tf_ratio': 0.4,\n",
       " 'unit_length': 4,\n",
       " 'which_epoch': 'finest',\n",
       " 'save_root': './checkpoints/t2m/Comp_v6_KLD005',\n",
       " 'model_dir': './checkpoints/t2m/Comp_v6_KLD005/model',\n",
       " 'meta_dir': './checkpoints/t2m/Comp_v6_KLD005/meta',\n",
       " 'data_root': './dataset/HumanML3D/',\n",
       " 'motion_dir': './dataset/HumanML3D/new_joint_vecs',\n",
       " 'text_dir': './dataset/HumanML3D/texts',\n",
       " 'joints_num': 22,\n",
       " 'dim_pose': 263,\n",
       " 'max_motion_length': 196,\n",
       " 'max_motion_frame': 196,\n",
       " 'max_motion_token': 55,\n",
       " 'dim_word': 300,\n",
       " 'num_classes': 50,\n",
       " 'device': device(type='cuda'),\n",
       " 'dim_pos_ohot': 15,\n",
       " 'dim_motion_hidden': 1024,\n",
       " 'dim_coemb_hidden': 512}"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(wrapper_opt)[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c9fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aeeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "a10416b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1332.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(\"t2m\", True, 4, w_vectorizer, unit_length=2**2, num_workers = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "884c3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "06062643",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "39700b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b0951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "235416c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([192, 192, 180, 136])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "420548bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion , ind , com_loss = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(args.out_dir, val_loader, net, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "70cb4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "\n",
    "\n",
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)\n",
    "motion = motion.cuda()\n",
    "et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, motion, m_length)\n",
    "bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "\n",
    "pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "\n",
    "for i in range(bs):\n",
    "    pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "    pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "    pred_pose, ind,loss_commit = model(motion[i:i+1, :m_length[i]])\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "    pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#     if savenpy:\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_gt.npy'), pose_xyz[:, :m_length[i]].cpu().numpy())\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_pred.npy'), pred_xyz.detach().cpu().numpy())\n",
    "\n",
    "    pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "    if i < min(4, bs):\n",
    "        draw_org.append(pose_xyz)\n",
    "        draw_pred.append(pred_xyz)\n",
    "        draw_text.append(caption[i])\n",
    "\n",
    "et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length)\n",
    "\n",
    "motion_pred_list.append(em_pred)\n",
    "motion_annotation_list.append(em)\n",
    "\n",
    "temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision_real += temp_R\n",
    "matching_score_real += temp_match\n",
    "temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision += temp_R\n",
    "matching_score_pred += temp_match\n",
    "\n",
    "nb_sample += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "4412f361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "512ba29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636710ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
