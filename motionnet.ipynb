{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f08df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcc57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# from os.path import join as pjoin\n",
    "# import random\n",
    "# import codecs as cs\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# class VQMotionDataset(data.Dataset):\n",
    "#     def __init__(self, dataset_name, window_size = 64, unit_length = 4):\n",
    "#         self.window_size = window_size\n",
    "#         self.unit_length = unit_length\n",
    "#         self.dataset_name = dataset_name\n",
    "\n",
    "#         if dataset_name == 't2m':\n",
    "#             self.data_root = '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 22\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = ''\n",
    "\n",
    "#         elif dataset_name == 'kit':\n",
    "#             self.data_root = './dataset/KIT-ML'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = 'checkpoints/kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta'\n",
    "        \n",
    "#         joints_num = self.joints_num\n",
    "\n",
    "#         mean = np.load(pjoin(self.data_root, 'Mean.npy'))\n",
    "#         std = np.load(pjoin(self.data_root, 'Std.npy'))\n",
    "\n",
    "#         split_file = pjoin(self.data_root, 'val.txt')\n",
    "\n",
    "#         self.data = []\n",
    "#         self.lengths = []\n",
    "#         id_list = []\n",
    "#         with cs.open(split_file, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 id_list.append(line.strip())\n",
    "\n",
    "#         for name in tqdm(id_list):\n",
    "#             try:\n",
    "#                 motion = np.load(pjoin(self.motion_dir, name + '.npy'))\n",
    "#                 if motion.shape[0] < self.window_size:\n",
    "#                     continue\n",
    "#                 self.lengths.append(motion.shape[0] - self.window_size)\n",
    "#                 self.data.append(motion)\n",
    "#             except:\n",
    "#                 # Some motion may not exist in KIT dataset\n",
    "#                 pass\n",
    "\n",
    "            \n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "#     def inv_transform(self, data):\n",
    "#         return data * self.std + self.mean\n",
    "    \n",
    "#     def compute_sampling_prob(self) : \n",
    "        \n",
    "#         prob = np.array(self.lengths, dtype=np.float32)\n",
    "#         prob /= np.sum(prob)\n",
    "#         return prob\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         motion = self.data[item]\n",
    "        \n",
    "#         idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "#         motion = motion[idx:idx+self.window_size]\n",
    "#         \"Z Normalization\"\n",
    "#         motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         return motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:02<00:00, 896.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet = VQMotionDataset(\"aist\", data_root=\"/srv/scratch/sanisetty3/music_motion/AIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c955e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATALoader(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            num_workers = 0,\n",
    "            shuffle = True,\n",
    "           ):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                #sampler=sampler,\n",
    "                                                num_workers=num_workers,\n",
    "                                                #collate_fn=collate_fn,\n",
    "                                                drop_last = True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ffc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(trainSet , 2)\n",
    "train_loader_iter = cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f6baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c04284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 263])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed7f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l2\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70a8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQMotionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "955434e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantize(\n",
       "  (project_in): Identity()\n",
       "  (project_out): Identity()\n",
       "  (_codebook): EuclideanCodebook()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f567b04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_motion , indices, commit_loss = model(gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad74b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(pred_motion, gt_motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbc4802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04a27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 263])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6576efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(pred_motion[0:1].detach().cpu()), \"aist0_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cebe1",
   "metadata": {},
   "source": [
    "## Variable motion lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2d204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import random\n",
    "import codecs as cs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e21233d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f803a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionCollator():\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.bos = torch.LongTensor(([0]))\n",
    "        self.eos = torch.LongTensor(([2]))\n",
    "        self.pad = torch.LongTensor(([1]))\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        \n",
    "\n",
    "        pad_batch_inputs = []\n",
    "        pad_batch_mask = []\n",
    "        motion_lengths = []\n",
    "        names = []\n",
    "        max_len = max([sample.shape[0] for sample, name in samples])\n",
    "\n",
    "\n",
    "        for inp,name in samples:\n",
    "            n,d = inp.shape\n",
    "            diff = max_len - n\n",
    "            mask = torch.BoolTensor([1]*n + [0]*diff)\n",
    "            padded = torch.concatenate((torch.tensor(inp) , torch.ones((diff,d))*self.pad))\n",
    "            pad_batch_inputs.append(padded)\n",
    "            pad_batch_mask.append(mask)\n",
    "            motion_lengths.append(n)\n",
    "            names.append(name)\n",
    "\n",
    "    \n",
    "        batch = {\n",
    "            \"motion\": torch.stack(pad_batch_inputs , 0),\n",
    "            \"motion_length\": torch.Tensor(motion_lengths),\n",
    "            \"motion_mask\" : torch.stack(pad_batch_mask , 0),\n",
    "            \"names\" : np.array(names)\n",
    "\n",
    "        }\n",
    "\n",
    "   \n",
    "        return batch    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2f75b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from core.datasets import vqa_motion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa75fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 864.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_ds = vqa_motion_dataset.VQMotionDataset(\"t2m\",split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f1f1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 40.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95b501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "761b8543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1952.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator()\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                4,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f9ae5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123220e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0fd13502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c02e19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 199, 263])\n",
      "tensor([ 69., 184., 199., 193.])\n"
     ]
    }
   ],
   "source": [
    "gt_motion = batch[\"motion\"]\n",
    "print(gt_motion.shape)\n",
    "print(batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf528414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 140\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aa2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c03c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38ba246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_len = int(batch.get(\"motion_lengths\" , [gt_motion.shape[1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a78e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 263])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[:,:motion_len,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea5f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4e7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import MotionTransformer, MotionDecoder, LinearEmbedding, VQMotionModel\n",
    "from core.quantization.core_vq import VectorQuantization\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b56531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8bbc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b73c8266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "pred_motion , indices, commit_loss = model(batch[\"motion\"].cuda() , mask = batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead5b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93fd453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l1\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79098dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1719, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abecb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2821, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc.forward_vel(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2ff08a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionTransformer(\n",
    "        inp_dim =263,\n",
    "        max_seq_len = 200,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        attn_layers = Encoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "dd6b943b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_motion = motionEncoder(batch[\"motion\"] , mask = batch[\"motion_mask\"])\n",
    "print(encoded_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c45277c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization(\n",
    "        dim = 768,\n",
    "        codebook_dim = 128,\n",
    "        codebook_size = 1024,\n",
    "        decay = 0.95,\n",
    "        commitment_weight = 1,\n",
    "        kmeans_init = True,\n",
    "        threshold_ema_dead_code = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c560ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:350: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion)\n",
    "print(quantized_enc_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "46a0be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = vq.codebook.t()\n",
    "embed.pow(2).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "686c6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_motion[:,:,:128]\n",
    "shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "098f7dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801,  0.7024,  1.5966,  ...,  0.7535, -1.5004, -0.4283],\n",
       "        [-0.2908,  0.5800,  1.6096,  ...,  0.4755, -1.5223, -0.3614],\n",
       "        [-0.2561,  0.4115,  1.5109,  ...,  0.1626, -1.4867, -0.2438],\n",
       "        ...,\n",
       "        [ 0.5579,  0.1785,  0.0483,  ..., -0.8444, -0.6883,  1.2005],\n",
       "        [ 0.5358,  0.1656,  0.1023,  ..., -0.8245, -0.6591,  1.2180],\n",
       "        [ 0.5378,  0.0897,  0.1818,  ..., -0.8273, -0.6676,  1.2187]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(x, \"... d -> (...) d\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a94a1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = -(\n",
    "    x.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * x @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "01f8da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "8049f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = dist.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5628c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bb2b807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = rearrange(batch[\"motion_mask\"], \"... -> (...)\")\n",
    "inp_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "116e3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.finfo(dist.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d192c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "eb18c634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6291e+02, -1.6781e+02, -1.2629e+02,  ..., -3.8359e+11,\n",
       "        -3.0852e+11, -4.7003e+11], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a2ee54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = (dist + dist.masked_fill(~inp_mask[...,None] , mask_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fde61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "8a80a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind2 = dist2.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "bc4b4929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind2[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7937985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot = F.one_hot(embed_ind2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c80bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = vq._codebook.postprocess_emb(embed_ind, shape)\n",
    "quantize = vq._codebook.dequantize(embed_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "eeb5793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "031c3f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "8191be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 128])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "34a9eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot2 = (embed_onehot * inp_mask[...,None] ).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "4dd40b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "4e8de320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T@embed_onehot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "cd151efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e67863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "55cbce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 768])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c4eb9e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12fad71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion , batch[\"motion_mask\"])\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f341fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionDecoder(\n",
    "    dim = 768,\n",
    "    logit_dim = 263,\n",
    "    attn_layers = Decoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "253fae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion = motionDecoder(quantized_enc_motion, inp_mask = batch[\"motion_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e479fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "cf9f2239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 263])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion[batch[\"motion_mask\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "bb1de763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "36335a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88422.6406, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(batch[\"motion\"] * batch[\"motion_mask\"][...,None] , pred_motion*batch[\"motion\"] * batch[\"motion_mask\"][...,None], reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1396e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "bc200fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1263)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"] .numel()/(batch[\"motion_mask\"].sum()*263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "39280ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112564"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "cf080128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9d3480dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99940)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].sum()*263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dde29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55a6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fba5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022932fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379864c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2f0f42",
   "metadata": {},
   "source": [
    "## Decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "522459ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "      \n",
    "    def __init__(self, my_dict):\n",
    "          \n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1854299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', default=\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", help=\"folder with train and test data\")\n",
    "parser.add_argument('--pretrained', default='')\n",
    "parser.add_argument('--resume', default=True, type = bool)\n",
    "parser.add_argument('--output_dir', default=\"./checkpoints/vq_768_128\")\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--fp16', default=True, type=bool)\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='aist', help=\"t2m or kit or aist\")\n",
    "parser.add_argument('--var_len', default=False, type=bool)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_bs', default=64, type=int,)\n",
    "parser.add_argument('--eval_bs', default=64, type=int,)\n",
    "parser.add_argument('--gradient_accumulation_steps', default=4, type=int,)\n",
    "\n",
    "parser.add_argument('--motion_dim', type=int, default=263, help='Input motion dimension dimension')\n",
    "parser.add_argument('--enc_dec_dim', type=int, default=768, help='Encoder and Decoder dimension')\n",
    "parser.add_argument('--depth', type=int, default=12, help='Encoder Decoder depth')\n",
    "parser.add_argument('--heads', type=int, default=10, help='Encoder Decoder number of heads')\n",
    "parser.add_argument('--codebook_dim', type=int, default=128, help='codeboook dimension')\n",
    "parser.add_argument('--codebook_size', type=int, default=1024, help='number of codebook embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--commit\", type=float, default=1, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss_vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons_loss', type=str, default='l1_smooth', help='reconstruction loss')\n",
    "parser.add_argument('--max_seq_length', type=int, default=200, help='max sequence length')\n",
    "\n",
    "parser.add_argument(\"--num_train_iters\",  default=500000,type=int)\n",
    "parser.add_argument(\"--save_steps\",  default=5000,type=int)\n",
    "parser.add_argument(\"--logging_steps\",  default=10,type=int)\n",
    "parser.add_argument(\"--wandb_every\",  default=100,type=int)\n",
    "parser.add_argument(\"--evaluate_every\",  default=10000,type=int)\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument('--warmup_steps', default=4000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--learning_rate', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "parser.add_argument('--lr_scheduler_type', default=\"cosine\", help=\"learning rate schedule type\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2ac6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aad762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d27dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "argss = Dict2Class(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9474025e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b4439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from configs.config import cfg\n",
    "\n",
    "model = VQMotionModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "012bc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\", map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd994df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77090528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d83144",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python VQ_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "3062d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "6f266128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = torch.randint(0,1024,(1,40))\n",
    "quantized, out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "37add0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[1:2].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "d0f07dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "85e9654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0577)\n",
      "tensor(1.9838)\n",
      "\n",
      "\n",
      "tensor(0.1757)\n",
      "tensor(2.9448)\n",
      "\n",
      "\n",
      "tensor(0.2407)\n",
      "tensor(1.6494)\n",
      "\n",
      "\n",
      "tensor(0.1197)\n",
      "tensor(0.4883)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(F.mse_loss(batch[\"motion\"][i,:40,:].cpu() , out_motion[i,:40,:].cpu()))\n",
    "    print(F.mse_loss(batch[\"motion\"][i,40:int(batch[\"motion_length\"][i]),:].cpu() , out_motion[i,40:int(batch[\"motion_length\"][i]),:].cpu()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8afe2",
   "metadata": {},
   "source": [
    "## Music codes to motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46c343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "155dadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9329ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4de37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c7ebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc2s = msc[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2824c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(torch.Tensor(msc2s.T)[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f6f9d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 263])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7224126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(decoded_motion_features.detach().cpu()), \"mus_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "911fcd08",
   "metadata": {},
   "source": [
    "## Eval regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99fe4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, mean ,std , j = 22):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*std+mean, j)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, j, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3cd8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3024505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQVarLenMotionDatasetConditional, TransMotionDatasetConditional, MotionCollatorConditional\n",
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d6d6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist.yaml\")\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6f9ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de8724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7836fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:02<00:00, 824.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = TransMotionDatasetConditional(\"aist\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\",split = \"train\", max_length_seconds = 30 , window_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9de1ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollatorConditional(dataset_name =\"aist\" , bos = 1024, pad = 1025, eos = 1026)\n",
    "\n",
    "dl = DATALoader(aist_ds , batch_size = 1,collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b5e436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "786a8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = MotionRegressorModel(args = cfg.motion_trans ,pad_value=cfg.train.pad_index )\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5403bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 320000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c58f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 20000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/const_len/trans_768_768_aist/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "trans_model.load_state_dict(pkg[\"model\"])\n",
    "trans_model = trans_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11509b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits2indx(logits , flag = False):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    if flag:\n",
    "        _, cls_pred_index = torch.max(probs, dim=-1)\n",
    "    else:\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        cls_pred_index = dist.sample()\n",
    "    return cls_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b101529",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = batch[\"motion\"][:, :-1], batch[\"motion\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4b5d6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1024,  349,  565,  565,  685,  103,  103,  329,  461,  240,  178,  823,\n",
       "           49,  363,  577,  349,  675,    9,    8,  136,  905,  212,  131,  216,\n",
       "          140,  813,  877,  997, 1022,  474,  103,  103,  103,  103,  103,  877,\n",
       "          593,  886,  240,  675,  269,  675,  240,  421,  212,  553,   33,   33,\n",
       "           86,  997,  167,  153,  719,  391,  127,   12,  823,  329,  103,  103,\n",
       "          103,  135,  444,  429,  260,  194,  374,  349,  675,  349,  685,   27,\n",
       "          186,  262,  186,  351,  349,  296,  654,  485,  757,  997,  325,  474,\n",
       "          103,  103,  103,  103,  103,  135,  266,   26,  240,  349,  349,  349,\n",
       "          472,  699,  823,  303,   33]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1cc3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = trans_model(motion = inp , mask = batch[\"motion_mask\"][:,:-1]  , context = batch[\"condition\"], context_mask = batch[\"condition_mask\"])\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5458598",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inp = logits2indx(logits , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c72f41fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  93,  565,  506,  877,  103,  103,  329,  515,  240,  349,  349,  303,\n",
       "          363,  577,   84,  675,  800,  187,  136,  905,  212,  131,  216,  140,\n",
       "          813,  877,  997, 1022, 1022,  474,  103,  103,  103,  103,  757,  421,\n",
       "          886,  240,  675,  349,  675,  240,  593,  593,  553,   33,   33,   86,\n",
       "          997,  399,  719,  167,  391,  127,   12,  540,   33,  103,  103,  103,\n",
       "          329,  444,  205,  282,  407,  374,  349,  675,  349,  685,   27,  186,\n",
       "          262,  186,  351,  349,  296,  654,  430,   58,  997,  325,  474,  103,\n",
       "          103,  103,  103,  103,  135,  266,   26,  240,  349,  349,  349,  324,\n",
       "           10,  823,  303,   33, 1026]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd628d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pred_inp[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b0fb5f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "quant , out_motion = model.decode(indices.cuda())\n",
    "sample_render(to_xyz(out_motion.detach().cpu(), aist_ds), \"pred_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/test_regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a029ff41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gBR_sBM_cAll_d05_mBR1_ch05'], dtype='<U26')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eaa0deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  (np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/music/mBR1.npy\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a9d8d766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201, 128])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.concatenate((music_paths[:200] , np.ones((1,128)) *1026)))[None,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8d3a2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.BoolTensor(np.ones((1,41)))\n",
    "m[:,-1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8715aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b4cb7ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:04<00:00,  9.77it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_motion_indices = trans_model.generate(start_tokens = inp[:,:1], seq_len=40, context = torch.Tensor(np.concatenate((music_paths[:40] , np.ones((1,128)) *1026)))[None,...], context_mask = m)\n",
    "gen_motion_indices_ = gen_motion_indices[gen_motion_indices<1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b06cb63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "_ , pred_motion = model.decode(gen_motion_indices_.reshape(inp.shape[0],-1).cuda())\n",
    "sample_render(to_xyz(pred_motion.detach().cpu(), aist_ds), \"pred_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/test_regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb681c7",
   "metadata": {},
   "source": [
    "## KIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e9410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.rotation2xyz import Rotation2xyz\n",
    "import numpy as np\n",
    "from trimesh import Trimesh\n",
    "import os\n",
    "os.environ['PYOPENGL_PLATFORM'] = \"osmesa\"\n",
    "\n",
    "import torch\n",
    "from visualize.simplify_loc2rot import joints2smpl\n",
    "import pyrender\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import imageio\n",
    "from shapely import geometry\n",
    "import trimesh\n",
    "from pyrender.constants import RenderFlags\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0ef7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, mean ,std , j = 22):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*std+mean, j)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, j, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee83663",
   "metadata": {},
   "outputs": [],
   "source": [
    "kit_root = \"/srv/scratch/sanisetty3/music_motion/KIT-ML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20c0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_k = np.load(\"/srv/scratch/sanisetty3/music_motion/KIT-ML/Mean.npy\")\n",
    "std_k = np.load(\"/srv/scratch/sanisetty3/music_motion/KIT-ML/Std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9f8cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kit0 = torch.Tensor(np.load(\"/srv/scratch/sanisetty3/music_motion/KIT-ML/new_joint_vecs/00010.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62c179e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kit0 = (kit0 - mean_k) / std_k\n",
    "kit0_xyz = to_xyz( kit0[None,...], mean_k, std_k , 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5136172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render((kit0_xyz), \"pred_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/test_regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b99db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073dceac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74e64061",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions = kit0_xyz[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e59374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from render_final import render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b8fd3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# render(kit0_xyz[0].numpy(), outdir=\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/kit\", step=0, name=\"0\", pred=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d090f099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "frames, njoints, nfeats = motions.shape\n",
    "MINS = motions.min(axis=0).min(axis=0)\n",
    "MAXS = motions.max(axis=0).max(axis=0)\n",
    "\n",
    "height_offset = MINS[1]\n",
    "motions[:, :, 1] -= height_offset\n",
    "trajec = motions[:, 0, [0, 2]]\n",
    "\n",
    "j2s = joints2smpl(num_frames=frames, device_id=0, cuda=False)\n",
    "# rot2xyz = Rotation2xyz(device=torch.device(\"cuda:0\"))\n",
    "rot2xyz = Rotation2xyz(device=torch.device(\"cpu\"))\n",
    "\n",
    "faces = rot2xyz.smpl_model.faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9faeeb37",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for dimension 0 with size 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-811393214a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmotion_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint2smpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [nframes, njoints, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/visualize/simplify_loc2rot.py\u001b[0m in \u001b[0;36mjoint2smpl\u001b[0;34m(self, input_joints, init_params)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpred_cam_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mkeypoints_3d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mconf_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfidence_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;31m# seq_ind=idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         )\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/visualize/joints2smpl/src/smplify.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, init_pose, init_betas, init_cam_t, j3d, conf_3d, seq_ind)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0mbody_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mbody_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_opt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/visualize/joints2smpl/src/smplify.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                     loss = body_fitting_loss_3d(body_pose, preserve_pose, betas, model_joints[:, self.smpl_index], camera_translation,\n\u001b[0;32m--> 227\u001b[0;31m                                                 \u001b[0mj3d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                                                 \u001b[0mjoints3d_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf_3d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                                                 \u001b[0mjoint_loss_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for dimension 0 with size 21"
     ]
    }
   ],
   "source": [
    "motion_tensor, opt_dict = j2s.joint2smpl(motions)  # [nframes, njoints, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0, 11, 12, 13, 14, 15], [0, 16, 17, 18, 19, 20], [0, 1, 2, 3, 4], [3, 5, 6, 7], [3, 8, 9, 10]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d60a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7fc869",
   "metadata": {},
   "source": [
    "## VQ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69d0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb06dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"t2m\"\n",
    "exp_name = \"vq_768_768_aist\"\n",
    "out_dir = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\"\n",
    "out_dir = os.path.join(out_dir, f'{exp_name}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711a7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils_model.get_logger(out_dir)\n",
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6510837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84939b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42824b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/vqvae_motion.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c92c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200000.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc53c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbbc5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:05<00:00, 749.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "\n",
    "# checkpoint_dir = \"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints\"\n",
    "# dataset_opt_path = f'{checkpoint_dir}/kit/Comp_v6_KLD005/opt.txt' if dataname == 'kit' else f'{checkpoint_dir}/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "nb_joints = 21 if dataname == 'kit' else 22\n",
    "\n",
    "val_loader = dataset_TM_eval.DATALoader(\"t2m\", True, 4, w_vectorizer, unit_length=2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c0c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0e6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e37606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 325000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31963e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1165 [00:30<41:09,  2.15s/it] \n",
      "  0%|          | 0/2 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [196, 263] at entry 0 and [252, 263] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a24692995545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrepeat_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbest_fid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_div\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_matching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_vqvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_fid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_matching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_wrapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavenpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/eval_trans.py\u001b[0m in \u001b[0;36mevaluation_vqvae\u001b[0;34m(out_dir, val_loader, net, logger, writer, nb_iter, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, eval_wrapper, draw, save, savegif, savenpy)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmatching_score_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmatching_score_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_one_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mmotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/datasets/dataset_TM_eval.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [196, 263] at entry 0 and [252, 263] at entry 2"
     ]
    }
   ],
   "source": [
    "\n",
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "repeat_time = 2\n",
    "for i in tqdm(range(repeat_time)):\n",
    "    best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(out_dir, val_loader, model, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n",
    "    fid.append(best_fid)\n",
    "    div.append(best_div)\n",
    "    top1.append(best_top1)\n",
    "    top2.append(best_top2)\n",
    "    top3.append(best_top3)\n",
    "    matching.append(best_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fc87f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48a7639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07952023417818133\n",
      "div:  9.660172\n",
      "top1:  0.8640006454388985\n",
      "top2:  0.9694402610441767\n",
      "top3:  0.9935366465863454\n",
      "matching:  3.024575046017313\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', np.mean(fid))\n",
    "print('div: ', np.mean(div))\n",
    "print('top1: ', np.mean(top1))\n",
    "print('top2: ', np.mean(top2))\n",
    "print('top3: ', np.mean(top3))\n",
    "print('matching: ', np.mean(matching))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b78e2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07902298641742789\n",
      "div:  9.737120151519775\n",
      "top1:  0.8569277108433735\n",
      "top2:  0.9676204819277109\n",
      "top3:  0.992039586919105\n",
      "matching:  3.050672186836729\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', sum(fid)/repeat_time)\n",
    "print('div: ', sum(div)/repeat_time)\n",
    "print('top1: ', sum(top1)/repeat_time)\n",
    "print('top2: ', sum(top2)/repeat_time)\n",
    "print('top3: ', sum(top3)/repeat_time)\n",
    "print('matching: ', sum(matching)/repeat_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b81a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df310cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49b4b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 220000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c5e6d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0117, requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd88f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [01:17<00:00, 15.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "mean_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy\")\n",
    "std_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy\")\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "for batch in tqdm(val_loader):\n",
    "    word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "    motion = motion.to(torch.float32)\n",
    "    denorm = val_loader.dataset.inv_transform(motion.detach().cpu())\n",
    "    denorm = (denorm - mean_gpt) / std_gpt\n",
    "    \n",
    "    max_len = motion.shape[1]\n",
    "    mask = []\n",
    "    for n in m_length:\n",
    "        diff = max_len - n\n",
    "        mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "    mask = torch.stack(mask , 0)\n",
    "#     print(mask.shape)\n",
    "\n",
    "\n",
    "    motion = motion.cuda()\n",
    "    et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, denorm, m_length)\n",
    "    bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "    num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "#     for n in m_length:\n",
    "#         pred_pose_eval[:,:n] = 1\n",
    "\n",
    "#     pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "    pred_pose, ind, loss_commit = model(motion)\n",
    "    pred_pose = pred_pose.cpu()*mask[...,None]\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu())\n",
    "    pred_denorm = (pred_denorm - mean_gpt) / std_gpt\n",
    "\n",
    "#     for i in range(bs):\n",
    "#         pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "#         pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "#         pred_pose, ind, loss_commit = net(motion[i:i+1, :m_length[i]])\n",
    "#         pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "#         pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#         pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "#         if i < min(4, bs):\n",
    "#             draw_org.append(pose_xyz)\n",
    "#             draw_pred.append(pred_xyz)\n",
    "#             draw_text.append(caption[i])\n",
    "\n",
    "    et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, (pred_denorm).detach().cpu(), m_length)\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "    temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision += temp_R\n",
    "    matching_score_pred += temp_match\n",
    "    nb_sample += bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c00269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "39879e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c16137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ebbe0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 22, 18, 15])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35823a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([136, 172, 108, 132])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5c6591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 196\n",
    "mask = []\n",
    "for n in m_length:\n",
    "    diff = max_len - n\n",
    "    mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "mask = torch.stack(mask , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffe8e2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80630389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose, ind, loss_commit = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5d88a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose = pred_pose.cpu()*mask[...,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59710b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pose[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3275c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).cpu().numpy()\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cb5c840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.667021077708341"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6b984e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5662530162371695"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dacdbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23019442"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_mu-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9980a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.109602005790272"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(gt_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2476b235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.78282411570007"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa0e5eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6446423427524115"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_cov-cov)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91659bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 300 if nb_sample > 300 else 100)\n",
    "diversity = calculate_diversity(motion_pred_np, 300 if nb_sample > 300 else 100)\n",
    "\n",
    "R_precision_real = R_precision_real / nb_sample\n",
    "R_precision = R_precision / nb_sample\n",
    "\n",
    "matching_score_real = matching_score_real / nb_sample\n",
    "matching_score_pred = matching_score_pred / nb_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74e15670",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "beac57a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  9.563007 7.2707257 fid:  3.4606680342994736\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbdb04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  8.298933 8.4250345 fid:  0.11668709473214278\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc3f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fdffe3",
   "metadata": {},
   "source": [
    "## Mean comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd290f",
   "metadata": {},
   "source": [
    "### HumanML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7167f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.50831097963555 48.37199901603162\n"
     ]
    }
   ],
   "source": [
    "mean_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Mean.npy\")\n",
    "std_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Std.npy\")\n",
    "print(sum(mean_hml) , sum(std_hml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93931eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e334da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.31540985721672 49.82533999346197\n"
     ]
    }
   ],
   "source": [
    "mean_aist = np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/Mean.npy\")\n",
    "std_aist = np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/Std.npy\")\n",
    "print(sum(mean_aist) , sum(std_aist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b93da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006871098"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean_hml-mean_aist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e665223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060514163"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(mean_hml-mean_aist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd1984",
   "metadata": {},
   "source": [
    "### T2M Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f2f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/mean.npy\")\n",
    "std_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/std.npy\")\n",
    "print(mean_t2m.shape , std_t2m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bbe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "707fda7f",
   "metadata": {},
   "source": [
    "### T2MGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43cd0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy\")\n",
    "std_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy\")\n",
    "print(mean_gpt.shape , std_gpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b8270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83d748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b936b260",
   "metadata": {},
   "source": [
    "### MDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29ae96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f61808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e38297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80f22be",
   "metadata": {},
   "source": [
    "## Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a217836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11760/23384 [10:39<05:12, 37.25it/s]   "
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D//\")\n",
    "collate_fn = MotionCollator(200)\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                1,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0942e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 200\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "30f99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0b0daa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 109, 263])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d23e270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49b8c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,1024,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8ba72877",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb567a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(batch[\"motion\"][0:1].detach().cpu()), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a822e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f90fc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc323539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c54d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf462024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e08f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from options.get_eval_option import get_opt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01d14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Optimal Transport AutoEncoder training for AIST',\n",
    "                                 add_help=True,\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "## dataloader  \n",
    "parser.add_argument('--dataname', type=str, default='t2m', help='dataset directory')\n",
    "parser.add_argument('--batch-size', default=128, type=int, help='batch size')\n",
    "parser.add_argument('--window-size', type=int, default=64, help='training motion length')\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--total-iter', default=200000, type=int, help='number of total iterations to run')\n",
    "parser.add_argument('--warm-up-iter', default=1000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--lr', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--lr-scheduler', default=[50000, 400000], nargs=\"+\", type=int, help=\"learning rate schedule (iterations)\")\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument(\"--commit\", type=float, default=0.02, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss-vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons-loss', type=str, default='l2', help='reconstruction loss')\n",
    "\n",
    "## vqvae arch\n",
    "parser.add_argument(\"--code-dim\", type=int, default=512, help=\"embedding dimension\")\n",
    "parser.add_argument(\"--nb-code\", type=int, default=512, help=\"nb of embedding\")\n",
    "parser.add_argument(\"--mu\", type=float, default=0.99, help=\"exponential moving average to update the codebook\")\n",
    "parser.add_argument(\"--down-t\", type=int, default=2, help=\"downsampling rate\")\n",
    "parser.add_argument(\"--stride-t\", type=int, default=2, help=\"stride size\")\n",
    "parser.add_argument(\"--width\", type=int, default=512, help=\"width of the network\")\n",
    "parser.add_argument(\"--depth\", type=int, default=3, help=\"depth of the network\")\n",
    "parser.add_argument(\"--dilation-growth-rate\", type=int, default=3, help=\"dilation growth rate\")\n",
    "parser.add_argument(\"--output-emb-width\", type=int, default=512, help=\"output embedding width\")\n",
    "parser.add_argument('--vq-act', type=str, default='relu', choices = ['relu', 'silu', 'gelu'], help='dataset directory')\n",
    "parser.add_argument('--vq-norm', type=str, default=None, help='dataset directory')\n",
    "\n",
    "## quantizer\n",
    "parser.add_argument(\"--quantizer\", type=str, default='ema_reset', choices = ['ema', 'orig', 'ema_reset', 'reset'], help=\"eps for optimal transport\")\n",
    "parser.add_argument('--beta', type=float, default=1.0, help='commitment loss in standard VQ')\n",
    "\n",
    "## resume\n",
    "parser.add_argument(\"--resume-pth\", type=str, default=None, help='resume pth for VQ')\n",
    "parser.add_argument(\"--resume-gpt\", type=str, default=None, help='resume pth for GPT')\n",
    "\n",
    "\n",
    "## output directory \n",
    "parser.add_argument('--out-dir', type=str, default='output_vqfinal/', help='output directory')\n",
    "parser.add_argument('--results-dir', type=str, default='visual_results/', help='output directory')\n",
    "parser.add_argument('--visual-name', type=str, default='baseline', help='output directory')\n",
    "parser.add_argument('--exp-name', type=str, default='exp_debug', help='name of the experiment, will create a file inside out-dir')\n",
    "## other\n",
    "parser.add_argument('--print-iter', default=200, type=int, help='print frequency')\n",
    "parser.add_argument('--eval-iter', default=1000, type=int, help='evaluation frequency')\n",
    "parser.add_argument('--seed', default=123, type=int, help='seed for initializing training.')\n",
    "\n",
    "parser.add_argument('--vis-gt', action='store_true', help='whether visualize GT motions')\n",
    "parser.add_argument('--nb-vis', default=20, type=int, help='nb of visualizations')\n",
    "\n",
    "\n",
    "args=parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37fb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12977d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate,\n",
    "                       args.vq_act,\n",
    "                       args.vq_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3ab72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./pretrained/VQVAE/net_last.pth\", map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da8a5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4429148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,512,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b22e3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.forward_decoder(indices.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28ff9a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 263])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0047f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ea85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "289b3f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gKR_sBM_cAll_d28_mKR2_ch02'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(batch[\"names\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96371f",
   "metadata": {},
   "source": [
    "## Motion trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c007078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad717183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b516c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc0b02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )\n",
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2d8f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 128)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca80d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80, 107, 143, 190, 254, 338, 450, 600]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenss = list(np.array(np.logspace(np.log(80), np.log(600), 8, base=np.exp(1)) + 1 , dtype = np.uint))\n",
    "lenss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79bba209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200000, 217142, 234285, 251428, 268571, 285714, 302857, 320000]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_steps = list(np.linspace(200000,320000,8 , dtype = np.uint))\n",
    "stage_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38d99aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = np.searchsorted(stage_steps , 60000) - 1\n",
    "stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b68fa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenss[stage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,100000,40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38b183a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,1,10), np.logspace(np.log(60), np.log(200), 10, base=np.exp(1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c68e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773257b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 220000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38682e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f5961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.motionEncoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.motionDecoder.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ba386d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 0.00M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a5510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:02<00:00, 1572.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 3980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"test\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator()\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                1,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0221710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cfbfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4fef555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.loss import ReConsLoss\n",
    "\n",
    "loss_fnc = ReConsLoss(\"l1_smooth\", 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2a2b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3980/3980 [01:38<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052648645 0.047441807\n"
     ]
    }
   ],
   "source": [
    "losses =[]\n",
    "losses_vel = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        gt_motion = batch[\"motion\"].cuda()\n",
    "        pred_motion , indices, commit_loss = model(gt_motion)\n",
    "        loss_motion = loss_fnc(pred_motion, gt_motion).detach().cpu()\n",
    "        loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion).detach().cpu()\n",
    "#         loss = loss_motion.detach().cpu() + 0.02 * commit_loss + 0.5 * loss_vel.detach().cpu()\n",
    "        losses.append(loss_motion)\n",
    "        losses_vel.append(loss_vel)\n",
    "    \n",
    "print(np.mean(losses) , np.mean(losses_vel))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00106835",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\", map_location = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bff60ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optim', 'steps', 'total_loss'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3bd34eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([275000.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c3162",
   "metadata": {},
   "source": [
    "## Generating indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4e82a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, train_ds):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a298241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "47057b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\", map_location = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "965fb9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([275000.])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "30c31cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 320000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed69ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/srv/scratch/sanisetty3/music_motion/AIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "555abb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1404.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ind_ds = VQMotionDataset(\"aist\", split = \"test\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\",  window_size = 1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e03bc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DATALoader(ind_ds , batch_size = 1, shuffle = False,collate_fn=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "9821448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 213, 263])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "55aeb6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.29it/s]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "motions = []\n",
    "for batch in tqdm(valid_dl):\n",
    "    seq_len = min(batch[\"motion\"].shape[1] , 400)\n",
    "#     print(seq_len)\n",
    "    ind = model.encode(batch[\"motion\"][:,:seq_len,:].cuda())[0]\n",
    "#     motions.append(batch[\"motion\"].cpu())\n",
    "#     indices.append(ind.detach().cpu().numpy())\n",
    "    np.save(os.path.join(\"/srv/scratch/sanisetty3/music_motion/AIST\", \"joint_indices\" ,batch[\"names\"][0] + \".npy\"), ind.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b78ce040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1910 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch_vq in tqdm(valid_dl):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7e22739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion629 = np.load('/srv/scratch/sanisetty3/music_motion/AIST/new_joint_vecs/gJB_sFM_cAll_d08_mJB4_ch12.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "83517c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion629_norm = (motion629 - ind_ds.mean)/ind_ds.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "14b8387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 263)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion629_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "047c5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = model.encode(torch.Tensor(motion629_norm[None,:400]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bebd50b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "553ad885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "quant , out_motion = model.decode(indices.cuda())\n",
    "sample_render(to_xyz(out_motion.detach().cpu(), ind_ds), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "63180ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(torch.Tensor(motion629_norm[None,...]).detach().cpu() , ind_ds), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "1ffd0c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion629_norm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "67239776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 200, 263)\n",
      "200 (1, 200, 263)\n"
     ]
    }
   ],
   "source": [
    "indx = torch.zeros(motion629_norm.shape[0]).to(torch.long)\n",
    "for i in range(0 ,min(motion629_norm.shape[0] , 400) , 200):\n",
    "    print(i , motion629_norm[None,i:i+200].shape)\n",
    "    indices = model.encode(torch.Tensor(motion629_norm[None,i:i+200]).cuda())\n",
    "    indx[i:i+200] = indices[0].to(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "2cb775f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "quant , out_motion = model.decode(indx[None,...].cuda())\n",
    "sample_render(to_xyz(out_motion.detach().cpu(), ind_ds), \"rnd_motion_iter\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "eebc62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36153e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd0807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "86edbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_list = glob(\"/srv/scratch/sanisetty3/music_motion/AIST/joint_indices/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "8b4fcd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gJB_sFM_cAll_d08_mJB4_ch12'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx_list[240].split(\"/\")[-1].split(\".\")[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "f646be0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.load(indx_list[240])    \n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "f4731ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = model.decode(torch.LongTensor(indices[None,...]).cuda())\n",
    "# sample_render(to_xyz(out_motion[None,...].detach().cpu(), ind_ds), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05479148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "57583d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(motions[-1].detach().cpu() , ind_ds), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46643414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1984 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1984 [00:22<12:07:37, 22.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1984 [00:45<12:41:03, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1984 [01:04<11:36:41, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1984 [01:31<12:51:26, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/1984 [01:53<12:38:55, 23.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 6/1984 [02:20<13:21:07, 24.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 7/1984 [02:39<12:19:41, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 8/1984 [03:03<12:38:09, 23.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 9/1984 [03:27<12:49:35, 23.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 10/1984 [03:52<13:00:23, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 11/1984 [04:16<13:01:31, 23.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 12/1984 [04:38<12:43:58, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 13/1984 [04:56<11:55:47, 21.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 14/1984 [05:15<11:25:19, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 15/1984 [05:35<11:17:57, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 16/1984 [05:54<10:58:49, 20.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 17/1984 [06:21<12:04:59, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 18/1984 [06:41<11:47:46, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 19/1984 [07:05<12:11:00, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 20/1984 [07:31<12:50:25, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 21/1984 [07:53<12:33:38, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 22/1984 [08:20<13:10:31, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 23/1984 [08:51<14:13:48, 26.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 24/1984 [09:09<12:58:52, 23.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 25/1984 [09:28<12:05:25, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 26/1984 [09:58<13:25:34, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 27/1984 [10:25<13:45:54, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 28/1984 [10:47<13:14:03, 24.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "for pth in tqdm(indx_list):\n",
    "    indices = np.load(pth) \n",
    "    quant , out_motion = model.decode(torch.LongTensor(indices[None,...]).cuda())\n",
    "    sample_render(to_xyz(out_motion.detach().cpu(), ind_ds), pth.split(\"/\")[-1].split(\".\")[0] , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/test_joint_indx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb2a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e71653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a7f006f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:47<00:00, 39.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07778838 0.041689113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses =[]\n",
    "losses_vel = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl):\n",
    "        gt_motion = batch[\"motion\"].cuda()\n",
    "        pred_motion , indices, commit_loss = model(gt_motion)\n",
    "        loss_motion = loss_fnc(pred_motion, gt_motion).detach().cpu()\n",
    "        loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion).detach().cpu()\n",
    "#         loss = loss_motion.detach().cpu() + 0.02 * commit_loss + 0.5 * loss_vel.detach().cpu()\n",
    "        losses.append(loss_motion)\n",
    "        losses_vel.append(loss_vel)\n",
    "    \n",
    "print(np.mean(losses) , np.mean(losses_vel))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59336574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733f19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb0f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f28544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bc753f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQVarLenMotionDatasetConditional,MotionCollatorConditional,TransMotionDatasetConditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fe47f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 761.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = TransMotionDatasetConditional(\"aist\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\", split = \"val\", max_length_seconds = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "125708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionCollator = MotionCollatorConditional(dataset_name = \"aist\" , bos = 1024 , pad = 1025, eos = 1026)\n",
    "dl = DATALoader(ds , batch_size = 4, shuffle = False,collate_fn=motionCollator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "490f1949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/joint_indices/gBR_sBM_cAll_d05_mBR0_ch02.npy\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "825bea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.dataset.set_stage(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "51a3e1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7fabae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "motreg = MotionRegressorModel(args = cfg.motion_trans , ignore_index=1025 ,pad_value=1025 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12625694",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = torch.LongTensor([1024]*6).reshape(6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04f2c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7bbd4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_motion = motreg.generate(start_tokens = inp, seq_len=100 , context = batch[\"condition\"][0:1], context_mask = batch[\"condition_mask\"][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e3db3dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([486, 182, 952, 943, 149, 452,  54, 336, 649, 148, 739, 572, 169, 403,\n",
       "        852, 771,  36, 537, 349, 239, 486, 821, 834, 201, 759, 309, 788, 852,\n",
       "        403, 821, 127, 367, 590, 749, 952, 289, 160, 674, 649, 833, 114, 621,\n",
       "        423, 649, 571, 629, 771, 674, 920, 102, 555, 705, 336, 349, 267, 590,\n",
       "        621, 839, 467, 942, 613, 152, 739, 821, 148, 149, 382, 203, 807, 771,\n",
       "        739, 182, 250, 102, 606,  97, 880, 487, 448, 749, 448, 200, 629, 825,\n",
       "        705, 382, 952, 923, 982, 467, 487, 555, 934, 149, 908,  97, 260, 148,\n",
       "        629, 452])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d0061fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 263])\n"
     ]
    }
   ],
   "source": [
    "quant , out_motion = model.decode(gen_motion[1:2].cuda())\n",
    "print(out_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b113f581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[None,...].detach().cpu(), ind_ds), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f4584970",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ac579c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['motion', 'motion_lengths', 'motion_mask', 'names', 'condition', 'condition_mask'])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6b678654",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, targets = batch[\"motion\"][:, :-1], batch[\"motion\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7af4db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = motreg(motion = inp , mask = batch[\"motion_mask\"][:,:-1]  , context = batch[\"condition\"], context_mask = batch[\"condition_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d1004a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61, 1027])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e20a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "2f4d386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "889c97bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61, 1027])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5653c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "758aa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Categorical(probs)\n",
    "cls_pred_index = dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "34779ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "746d04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,cls_pred_index = torch.max(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "30d0dd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c716dbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "0387b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_num = 0\n",
    "right_num += (cls_pred_index.flatten(0) == targets.flatten(0)).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7032ad69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_num/((logits.shape[0]*logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5516dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def top_k(logits, thres = 0.9):\n",
    "    k = math.ceil((1 - thres) * logits.shape[-1])\n",
    "    val, ind = torch.topk(logits, k)\n",
    "    probs = torch.full_like(logits, float('-inf'))\n",
    "    probs.scatter_(1, ind, val)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8f28e651",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 289 is out of bounds for dimension 1 with size 61",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-a50658043f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-294-1a3a5c48fc5b>\u001b[0m in \u001b[0;36mtop_k\u001b[0;34m(logits, thres)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 289 is out of bounds for dimension 1 with size 61"
     ]
    }
   ],
   "source": [
    "filtered_logits = top_k(logits[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2243568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(filtered_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "001e6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = (inp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "de9c6a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6fb38946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fa1b873c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb128da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "edc19c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context torch.Size([1, 61, 128]) motion torch.Size([1, 62])\n",
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/61 [00:00<00:02, 20.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1]) torch.Size([1, 1]) torch.Size([1, 1, 128]) torch.Size([1, 1])\n",
      "1\n",
      "torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2, 128]) torch.Size([1, 2])\n",
      "2\n",
      "torch.Size([1, 3]) torch.Size([1, 3]) torch.Size([1, 3, 128]) torch.Size([1, 3])\n",
      "3\n",
      "torch.Size([1, 4]) torch.Size([1, 4]) torch.Size([1, 4, 128]) torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 6/61 [00:00<00:03, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 5]) torch.Size([1, 5]) torch.Size([1, 5, 128]) torch.Size([1, 5])\n",
      "5\n",
      "torch.Size([1, 6]) torch.Size([1, 6]) torch.Size([1, 6, 128]) torch.Size([1, 6])\n",
      "6\n",
      "torch.Size([1, 7]) torch.Size([1, 7]) torch.Size([1, 7, 128]) torch.Size([1, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 8/61 [00:00<00:04, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([1, 8]) torch.Size([1, 8]) torch.Size([1, 8, 128]) torch.Size([1, 8])\n",
      "8\n",
      "torch.Size([1, 9]) torch.Size([1, 9]) torch.Size([1, 9, 128]) torch.Size([1, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▋        | 10/61 [00:00<00:04, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "torch.Size([1, 10]) torch.Size([1, 10]) torch.Size([1, 10, 128]) torch.Size([1, 10])\n",
      "10\n",
      "torch.Size([1, 11]) torch.Size([1, 11]) torch.Size([1, 11, 128]) torch.Size([1, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 12/61 [00:01<00:05,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "torch.Size([1, 12]) torch.Size([1, 12]) torch.Size([1, 12, 128]) torch.Size([1, 12])\n",
      "12\n",
      "torch.Size([1, 13]) torch.Size([1, 13]) torch.Size([1, 13, 128]) torch.Size([1, 13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 14/61 [00:01<00:05,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([1, 14]) torch.Size([1, 14]) torch.Size([1, 14, 128]) torch.Size([1, 14])\n",
      "14\n",
      "torch.Size([1, 15]) torch.Size([1, 15]) torch.Size([1, 15, 128]) torch.Size([1, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 17/61 [00:01<00:04,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "torch.Size([1, 16]) torch.Size([1, 16]) torch.Size([1, 16, 128]) torch.Size([1, 16])\n",
      "16\n",
      "torch.Size([1, 17]) torch.Size([1, 17]) torch.Size([1, 17, 128]) torch.Size([1, 17])\n",
      "17\n",
      "torch.Size([1, 18]) torch.Size([1, 18]) torch.Size([1, 18, 128]) torch.Size([1, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 19/61 [00:01<00:04,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "torch.Size([1, 19]) torch.Size([1, 19]) torch.Size([1, 19, 128]) torch.Size([1, 19])\n",
      "19\n",
      "torch.Size([1, 20]) torch.Size([1, 20]) torch.Size([1, 20, 128]) torch.Size([1, 20])\n",
      "20\n",
      "torch.Size([1, 21]) torch.Size([1, 21]) torch.Size([1, 21, 128]) torch.Size([1, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 22/61 [00:02<00:04,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "torch.Size([1, 22]) torch.Size([1, 22]) torch.Size([1, 22, 128]) torch.Size([1, 22])\n",
      "22\n",
      "torch.Size([1, 23]) torch.Size([1, 23]) torch.Size([1, 23, 128]) torch.Size([1, 23])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 24/61 [00:02<00:03,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "torch.Size([1, 24]) torch.Size([1, 24]) torch.Size([1, 24, 128]) torch.Size([1, 24])\n",
      "24\n",
      "torch.Size([1, 25]) torch.Size([1, 25]) torch.Size([1, 25, 128]) torch.Size([1, 25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 26/61 [00:02<00:03,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "torch.Size([1, 26]) torch.Size([1, 26]) torch.Size([1, 26, 128]) torch.Size([1, 26])\n",
      "26\n",
      "torch.Size([1, 27]) torch.Size([1, 27]) torch.Size([1, 27, 128]) torch.Size([1, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 28/61 [00:02<00:03,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "torch.Size([1, 28]) torch.Size([1, 28]) torch.Size([1, 28, 128]) torch.Size([1, 28])\n",
      "28\n",
      "torch.Size([1, 29]) torch.Size([1, 29]) torch.Size([1, 29, 128]) torch.Size([1, 29])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 30/61 [00:03<00:03,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "torch.Size([1, 30]) torch.Size([1, 30]) torch.Size([1, 30, 128]) torch.Size([1, 30])\n",
      "30\n",
      "torch.Size([1, 31]) torch.Size([1, 31]) torch.Size([1, 31, 128]) torch.Size([1, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 32/61 [00:03<00:03,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "torch.Size([1, 32]) torch.Size([1, 32]) torch.Size([1, 32, 128]) torch.Size([1, 32])\n",
      "32\n",
      "torch.Size([1, 33]) torch.Size([1, 33]) torch.Size([1, 33, 128]) torch.Size([1, 33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 34/61 [00:03<00:03,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "torch.Size([1, 34]) torch.Size([1, 34]) torch.Size([1, 34, 128]) torch.Size([1, 34])\n",
      "34\n",
      "torch.Size([1, 35]) torch.Size([1, 35]) torch.Size([1, 35, 128]) torch.Size([1, 35])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 36/61 [00:03<00:02,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "torch.Size([1, 36]) torch.Size([1, 36]) torch.Size([1, 36, 128]) torch.Size([1, 36])\n",
      "36\n",
      "torch.Size([1, 37]) torch.Size([1, 37]) torch.Size([1, 37, 128]) torch.Size([1, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 38/61 [00:04<00:02,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "torch.Size([1, 38]) torch.Size([1, 38]) torch.Size([1, 38, 128]) torch.Size([1, 38])\n",
      "38\n",
      "torch.Size([1, 39]) torch.Size([1, 39]) torch.Size([1, 39, 128]) torch.Size([1, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 40/61 [00:04<00:02,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "torch.Size([1, 40]) torch.Size([1, 40]) torch.Size([1, 40, 128]) torch.Size([1, 40])\n",
      "40\n",
      "torch.Size([1, 41]) torch.Size([1, 41]) torch.Size([1, 41, 128]) torch.Size([1, 41])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 42/61 [00:04<00:02,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "torch.Size([1, 42]) torch.Size([1, 42]) torch.Size([1, 42, 128]) torch.Size([1, 42])\n",
      "42\n",
      "torch.Size([1, 43]) torch.Size([1, 43]) torch.Size([1, 43, 128]) torch.Size([1, 43])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 44/61 [00:04<00:02,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "torch.Size([1, 44]) torch.Size([1, 44]) torch.Size([1, 44, 128]) torch.Size([1, 44])\n",
      "44\n",
      "torch.Size([1, 45]) torch.Size([1, 45]) torch.Size([1, 45, 128]) torch.Size([1, 45])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 46/61 [00:05<00:01,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "torch.Size([1, 46]) torch.Size([1, 46]) torch.Size([1, 46, 128]) torch.Size([1, 46])\n",
      "46\n",
      "torch.Size([1, 47]) torch.Size([1, 47]) torch.Size([1, 47, 128]) torch.Size([1, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 48/61 [00:05<00:01,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "torch.Size([1, 48]) torch.Size([1, 48]) torch.Size([1, 48, 128]) torch.Size([1, 48])\n",
      "48\n",
      "torch.Size([1, 49]) torch.Size([1, 49]) torch.Size([1, 49, 128]) torch.Size([1, 49])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 50/61 [00:05<00:01,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "torch.Size([1, 50]) torch.Size([1, 50]) torch.Size([1, 50, 128]) torch.Size([1, 50])\n",
      "50\n",
      "torch.Size([1, 51]) torch.Size([1, 51]) torch.Size([1, 51, 128]) torch.Size([1, 51])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 52/61 [00:05<00:01,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "torch.Size([1, 52]) torch.Size([1, 52]) torch.Size([1, 52, 128]) torch.Size([1, 52])\n",
      "52\n",
      "torch.Size([1, 53]) torch.Size([1, 53]) torch.Size([1, 53, 128]) torch.Size([1, 53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 54/61 [00:06<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "torch.Size([1, 54]) torch.Size([1, 54]) torch.Size([1, 54, 128]) torch.Size([1, 54])\n",
      "54\n",
      "torch.Size([1, 55]) torch.Size([1, 55]) torch.Size([1, 55, 128]) torch.Size([1, 55])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 56/61 [00:06<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "torch.Size([1, 56]) torch.Size([1, 56]) torch.Size([1, 56, 128]) torch.Size([1, 56])\n",
      "56\n",
      "torch.Size([1, 57]) torch.Size([1, 57]) torch.Size([1, 57, 128]) torch.Size([1, 57])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 58/61 [00:06<00:00,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "torch.Size([1, 58]) torch.Size([1, 58]) torch.Size([1, 58, 128]) torch.Size([1, 58])\n",
      "58\n",
      "torch.Size([1, 59]) torch.Size([1, 59]) torch.Size([1, 59, 128]) torch.Size([1, 59])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 60/61 [00:07<00:00,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "torch.Size([1, 60]) torch.Size([1, 60]) torch.Size([1, 60, 128]) torch.Size([1, 60])\n",
      "60\n",
      "torch.Size([1, 61]) torch.Size([1, 61]) torch.Size([1, 61, 128]) torch.Size([1, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:07<00:00,  8.41it/s]\n"
     ]
    }
   ],
   "source": [
    "gt_motion_indices = batch[\"motion\"][:,:-1]\n",
    "mask = batch[\"motion_mask\"]\n",
    "context = batch[\"condition\"]\n",
    "context_mask = batch[\"condition_mask\"]\n",
    "\n",
    "print(\"context\", batch[\"condition\"].shape ,\"motion\" , batch[\"motion\"].shape )\n",
    "seq_len = gt_motion_indices.shape[1]\n",
    "print(seq_len)\n",
    "eos_token = 1026\n",
    "out = start_tokens = gt_motion_indices[:,:1]\n",
    "b, t = out.shape\n",
    "\n",
    "for sl in tqdm(range(seq_len)):\n",
    "    print(sl)\n",
    "\n",
    "    x = out[:, -200:]\n",
    "    \n",
    "    \n",
    "    print(x.shape , mask[:,:(sl+1)].shape , context[:,:(sl+1)].shape ,context_mask[:,:(sl+1)].shape )\n",
    "\n",
    "    logits = motreg.forward(motion = x, mask = mask[:,:(sl+1)] , context = context[:,:(sl+1)] , context_mask = context_mask[:,:(sl+1)])[:, -1]\n",
    "    \n",
    "    \n",
    "    filtered_logits = top_k(logits)\n",
    "    probs = F.softmax(filtered_logits / 1, dim=-1)\n",
    "\n",
    "   \n",
    "\n",
    "    sample = torch.multinomial(probs, 1)\n",
    "\n",
    "    out = torch.cat((out, sample), dim=-1)\n",
    "\n",
    "    if (eos_token) is not None:\n",
    "        is_eos_tokens = (out == eos_token)\n",
    "\n",
    "        if is_eos_tokens.any(dim = -1).all():\n",
    "            # mask out everything after the eos tokens\n",
    "            shifted_is_eos_tokens = F.pad(is_eos_tokens, (1, -1))\n",
    "            mask = shifted_is_eos_tokens.float().cumsum(dim = -1) >= 1\n",
    "            out = out.masked_fill(mask, self.pad_value)\n",
    "            break\n",
    "\n",
    "out = out[:, t:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7de43364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915a9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "36468528",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion_indices_ = gt_motion_indices[gt_motion_indices<1024]\n",
    "gen_motion_indices_ = out[out<1024]\n",
    "\n",
    "_ , pred_motion = model.decode(gen_motion_indices_.reshape(1,-1).cuda())\n",
    "_ , gt_motion = model.decode(gt_motion_indices_.reshape(1,-1).cuda())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6fc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "faca8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits.contiguous().view(-1, logits.shape[-1]), targets.contiguous().view(-1),ignore_index=1025, reduction='mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423adc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fde78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb763ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "4a320d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(torch.randn((4, 224 , 1027)).contiguous().view(-1, 1027), targets.contiguous().view(-1),\n",
    "            ignore_index=1025, reduction='none').reshape(4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "23f74f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = batch[\"motion\"]\n",
    "name = batch[\"names\"]\n",
    "start_tokens = gt_motion[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f4c1a396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 61])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "83eed973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:10<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_motion_indices = motreg.generate(start_tokens = start_tokens, seq_len=60 , context = batch[\"condition\"], context_mask = batch[\"condition_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd17b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b8158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2be08316",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion2 = gt_motion[gt_motion<1024]\n",
    "gen_motion_indices2 = gen_motion_indices[gen_motion_indices<1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b3ccf8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion_indices2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "98c43582",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , gt_motion_ = model.decode(gt_motion2.reshape(1,-1).cuda())\n",
    "quant , pred_motion = model.decode(gen_motion_indices2.reshape(1,-1).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b6990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "cc4e64cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200000, 240000, 280000, 320000, 360000, 400000]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.linspace(200000,400000, 6 , dtype = np.uint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8d5b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion_xyz = recover_from_ric(gt_motion_.cpu().float()*ds.std+ds.mean, 22)\n",
    "gt_motion_xyz = gt_motion_xyz.reshape(gt_motion_.shape[0],-1, 22, 3)\n",
    "\n",
    "pred_motion_xyz = recover_from_ric(pred_motion.cpu().float()*ds.std+ds.mean, 22)\n",
    "pred_motion_xyz = pred_motion_xyz.reshape(pred_motion.shape[0],-1, 22, 3)\n",
    "\n",
    "\n",
    "\n",
    "# gt_pose_vis = plot_3d.draw_to_batch(gt_motion_xyz.numpy(),None, [os.path.join(save_file,name[0] + \"_gt.gif\")])\n",
    "# pred_pose_vis = plot_3d.draw_to_batch(pred_motion_xyz.numpy(),None, [os.path.join(save_file,name[0] + \"_pred.gif\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687fc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ac11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4f21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f84de7e",
   "metadata": {},
   "source": [
    "## Cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "e89d06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn((5,4))\n",
    "k = torch.randn((5,4))\n",
    "v = torch.randn((5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "8bae57f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2023,  0.6955, -2.4969, -0.7475],\n",
       "        [-0.6933,  0.3138,  1.3367,  2.4379],\n",
       "        [-0.2542, -1.2591, -0.0520,  0.8134],\n",
       "        [-1.9622,  0.7228, -0.9052, -0.2472],\n",
       "        [ 0.3010,  0.8481,  1.4993, -0.5711]])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "835cd129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0769,  0.0516,  1.2195, -0.1094],\n",
       "        [-1.7388,  0.1488,  1.7214,  0.8309],\n",
       "        [-0.2944, -0.4466, -0.3187,  0.0227]])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "799e7105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.BoolTensor([1,1,1,1,0])\n",
    "context_mask = torch.BoolTensor([0,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ee550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "53271b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.einsum(f'i d, j d -> i j', q, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "823bdfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "a37cc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask = rearrange(context_mask, 'j -> 1 j')\n",
    "dots = dots.masked_fill(~input_mask, -torch.finfo(dots.dtype).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "00a925c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4028e+38, -1.9609e+00,  1.0935e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  3.3129e+00, -4.9215e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  2.5482e+00, -1.2629e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38, -4.5168e+00,  2.2125e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  5.8091e+00, -3.9529e+00, -3.4028e+38, -3.4028e+38]])"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "5414a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = dots.shape[-2:]\n",
    "causal_mask = torch.ones((i, j), dtype = torch.bool).triu(j - i + 1)\n",
    "dots = dots.masked_fill(causal_mask, -torch.finfo(dots.dtype).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "b07a0ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  3.3129e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  2.5482e+00, -1.2629e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38, -4.5168e+00,  2.2125e+00, -3.4028e+38, -3.4028e+38],\n",
       "        [-3.4028e+38,  5.8091e+00, -3.9529e+00, -3.4028e+38, -3.4028e+38]])"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "f2355f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = F.softmax(dots , dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "53ab42ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e-01, 2.0000e-01, 2.0000e-01, 2.0000e-01, 2.0000e-01],\n",
       "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 9.7836e-01, 2.1644e-02, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 1.1939e-03, 9.9881e-01, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 9.9994e-01, 5.7596e-05, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "4174570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.einsum(f'i j, j d-> i d', attn, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "05dd9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335056a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e0ecc8",
   "metadata": {},
   "source": [
    "## Weighted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92380bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9db504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 882.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n",
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 652.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "aist_ds = VQVarLenMotionDataset(\"aist\", split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , num_stages=6, max_length_seconds=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "131f7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f189c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5ac72b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  2.0 2.0\n"
     ]
    }
   ],
   "source": [
    "ds = torch.utils.data.ConcatDataset([train_ds, aist_ds])\n",
    "\n",
    "weights_train = [\n",
    "[ds.__len__() / (train_ds.__len__())] * train_ds.__len__(),\n",
    "[ds.__len__() / (aist_ds.__len__())] * aist_ds.__len__(),\n",
    "]\n",
    "weights_train = list(itertools.chain.from_iterable(weights_train))\n",
    "sampler_train = torch.utils.data.WeightedRandomSampler(weights=weights_train, num_samples=len(weights_train))\n",
    "\n",
    "print(\"weights: \", ds.__len__() / (train_ds.__len__()), ds.__len__() / (aist_ds.__len__()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.datasets[0].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2d09108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size = 1, collate_fn=collate_fn ,sampler = sampler_train,num_workers = 0, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5cc0d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d83a1c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['013022']\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(batch[\"names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fe9eb042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "6\n",
      "1\n",
      "1\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for batch in dl:\n",
    "    print(len(str(batch[\"names\"][0]).split(\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5679edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/vqvae_motion135k.pt\" , map_location = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cca8b93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([135000.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c90bbd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0138, requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ad8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "itertools.chain.from_iterable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c76a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca8d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4eef9bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 918.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 10\n",
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 829.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hml_render_ds = VQVarLenMotionDataset(\"t2m\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", split = \"render\")\n",
    "aist_render_ds = VQVarLenMotionDataset(\"aist\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , split = \"render\")\n",
    "render_ds = torch.utils.data.ConcatDataset([hml_render_ds, aist_render_ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8d55ada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(render_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5b820d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c40edf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_dl = DATALoader(render_ds , batch_size = 1, shuffle = False,collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "201d53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 1779.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "012698\n",
      "012808\n",
      "008646\n",
      "013022\n",
      "003172\n",
      "008859\n",
      "005095\n",
      "012044\n",
      "002345\n",
      "008039\n",
      "gLH_sBM_cAll_d17_mLH4_ch02\n",
      "gLH_sBM_cAll_d18_mLH4_ch02\n",
      "gKR_sBM_cAll_d30_mKR2_ch02\n",
      "gKR_sBM_cAll_d28_mKR2_ch02\n",
      "gBR_sBM_cAll_d04_mBR0_ch02\n",
      "gBR_sBM_cAll_d05_mBR0_ch02\n",
      "gJS_sBM_cAll_d03_mJS3_ch02\n",
      "gLH_sBM_cAll_d17_mLH0_ch09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(render_dl):\n",
    "    print(batch[\"names\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbb25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
