{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f08df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcc57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# from os.path import join as pjoin\n",
    "# import random\n",
    "# import codecs as cs\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# class VQMotionDataset(data.Dataset):\n",
    "#     def __init__(self, dataset_name, window_size = 64, unit_length = 4):\n",
    "#         self.window_size = window_size\n",
    "#         self.unit_length = unit_length\n",
    "#         self.dataset_name = dataset_name\n",
    "\n",
    "#         if dataset_name == 't2m':\n",
    "#             self.data_root = '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 22\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = ''\n",
    "\n",
    "#         elif dataset_name == 'kit':\n",
    "#             self.data_root = './dataset/KIT-ML'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = 'checkpoints/kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta'\n",
    "        \n",
    "#         joints_num = self.joints_num\n",
    "\n",
    "#         mean = np.load(pjoin(self.data_root, 'Mean.npy'))\n",
    "#         std = np.load(pjoin(self.data_root, 'Std.npy'))\n",
    "\n",
    "#         split_file = pjoin(self.data_root, 'val.txt')\n",
    "\n",
    "#         self.data = []\n",
    "#         self.lengths = []\n",
    "#         id_list = []\n",
    "#         with cs.open(split_file, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 id_list.append(line.strip())\n",
    "\n",
    "#         for name in tqdm(id_list):\n",
    "#             try:\n",
    "#                 motion = np.load(pjoin(self.motion_dir, name + '.npy'))\n",
    "#                 if motion.shape[0] < self.window_size:\n",
    "#                     continue\n",
    "#                 self.lengths.append(motion.shape[0] - self.window_size)\n",
    "#                 self.data.append(motion)\n",
    "#             except:\n",
    "#                 # Some motion may not exist in KIT dataset\n",
    "#                 pass\n",
    "\n",
    "            \n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "#     def inv_transform(self, data):\n",
    "#         return data * self.std + self.mean\n",
    "    \n",
    "#     def compute_sampling_prob(self) : \n",
    "        \n",
    "#         prob = np.array(self.lengths, dtype=np.float32)\n",
    "#         prob /= np.sum(prob)\n",
    "#         return prob\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         motion = self.data[item]\n",
    "        \n",
    "#         idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "#         motion = motion[idx:idx+self.window_size]\n",
    "#         \"Z Normalization\"\n",
    "#         motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         return motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "191dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 952/952 [00:54<00:00, 17.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet = VQMotionDataset(\"aist\", data_root=\"/srv/scratch/sanisetty3/music_motion/AIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c955e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATALoader(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            num_workers = 0,\n",
    "            shuffle = True,\n",
    "           ):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                #sampler=sampler,\n",
    "                                                num_workers=num_workers,\n",
    "                                                #collate_fn=collate_fn,\n",
    "                                                drop_last = True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ffc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(trainSet , 2)\n",
    "train_loader_iter = cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f6baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c04284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 263])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed7f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l2\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70a8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQMotionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "955434e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantize(\n",
       "  (project_in): Identity()\n",
       "  (project_out): Identity()\n",
       "  (_codebook): EuclideanCodebook()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f567b04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_motion , indices, commit_loss = model(gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad74b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(pred_motion, gt_motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbc4802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04a27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 263])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6576efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(pred_motion[0:1].detach().cpu()), \"aist0_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cebe1",
   "metadata": {},
   "source": [
    "## Variable motion lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2d204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import random\n",
    "import codecs as cs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e21233d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f803a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionCollator():\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.bos = torch.LongTensor(([0]))\n",
    "        self.eos = torch.LongTensor(([2]))\n",
    "        self.pad = torch.LongTensor(([1]))\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        \n",
    "\n",
    "        pad_batch_inputs = []\n",
    "        pad_batch_mask = []\n",
    "        motion_lengths = []\n",
    "        names = []\n",
    "        max_len = max([sample.shape[0] for sample, name in samples])\n",
    "\n",
    "\n",
    "        for inp,name in samples:\n",
    "            n,d = inp.shape\n",
    "            diff = max_len - n\n",
    "            mask = torch.BoolTensor([1]*n + [0]*diff)\n",
    "            padded = torch.concatenate((torch.tensor(inp) , torch.ones((diff,d))*self.pad))\n",
    "            pad_batch_inputs.append(padded)\n",
    "            pad_batch_mask.append(mask)\n",
    "            motion_lengths.append(n)\n",
    "            names.append(name)\n",
    "\n",
    "    \n",
    "        batch = {\n",
    "            \"motion\": torch.stack(pad_batch_inputs , 0),\n",
    "            \"motion_length\": torch.Tensor(motion_lengths),\n",
    "            \"motion_mask\" : torch.stack(pad_batch_mask , 0),\n",
    "            \"names\" : np.array(names)\n",
    "\n",
    "        }\n",
    "\n",
    "   \n",
    "        return batch    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f75b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from core.datasets import vqa_motion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa75fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 864.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_ds = vqa_motion_dataset.VQMotionDataset(\"t2m\",split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f1f1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 23.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95b501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "761b8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator(200)\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                4,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f9ae5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123220e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fd13502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c02e19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 92, 263])\n",
      "tensor([81., 82., 68., 92.])\n"
     ]
    }
   ],
   "source": [
    "gt_motion = batch[\"motion\"]\n",
    "print(gt_motion.shape)\n",
    "print(batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf528414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 140\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aa2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c03c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38ba246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_len = int(batch.get(\"motion_lengths\" , [gt_motion.shape[1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a78e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 263])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[:,:motion_len,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea5f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4e7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import MotionTransformer, MotionDecoder, LinearEmbedding, VQMotionModel\n",
    "from core.quantization.core_vq import VectorQuantization\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b56531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8bbc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b73c8266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "pred_motion , indices, commit_loss = model(batch[\"motion\"].cuda() , mask = batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead5b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93fd453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l1\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79098dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1719, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abecb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2821, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc.forward_vel(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2ff08a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionTransformer(\n",
    "        inp_dim =263,\n",
    "        max_seq_len = 200,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        attn_layers = Encoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "dd6b943b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_motion = motionEncoder(batch[\"motion\"] , mask = batch[\"motion_mask\"])\n",
    "print(encoded_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c45277c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization(\n",
    "        dim = 768,\n",
    "        codebook_dim = 128,\n",
    "        codebook_size = 1024,\n",
    "        decay = 0.95,\n",
    "        commitment_weight = 1,\n",
    "        kmeans_init = True,\n",
    "        threshold_ema_dead_code = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c560ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:350: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion)\n",
    "print(quantized_enc_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "46a0be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = vq.codebook.t()\n",
    "embed.pow(2).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "686c6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_motion[:,:,:128]\n",
    "shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "098f7dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801,  0.7024,  1.5966,  ...,  0.7535, -1.5004, -0.4283],\n",
       "        [-0.2908,  0.5800,  1.6096,  ...,  0.4755, -1.5223, -0.3614],\n",
       "        [-0.2561,  0.4115,  1.5109,  ...,  0.1626, -1.4867, -0.2438],\n",
       "        ...,\n",
       "        [ 0.5579,  0.1785,  0.0483,  ..., -0.8444, -0.6883,  1.2005],\n",
       "        [ 0.5358,  0.1656,  0.1023,  ..., -0.8245, -0.6591,  1.2180],\n",
       "        [ 0.5378,  0.0897,  0.1818,  ..., -0.8273, -0.6676,  1.2187]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(x, \"... d -> (...) d\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a94a1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = -(\n",
    "    x.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * x @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "01f8da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "8049f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = dist.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5628c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bb2b807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = rearrange(batch[\"motion_mask\"], \"... -> (...)\")\n",
    "inp_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "116e3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.finfo(dist.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d192c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "eb18c634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6291e+02, -1.6781e+02, -1.2629e+02,  ..., -3.8359e+11,\n",
       "        -3.0852e+11, -4.7003e+11], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a2ee54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = (dist + dist.masked_fill(~inp_mask[...,None] , mask_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fde61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "8a80a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind2 = dist2.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "bc4b4929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind2[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7937985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot = F.one_hot(embed_ind2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c80bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = vq._codebook.postprocess_emb(embed_ind, shape)\n",
    "quantize = vq._codebook.dequantize(embed_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "eeb5793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "031c3f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "8191be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 128])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "34a9eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot2 = (embed_onehot * inp_mask[...,None] ).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "4dd40b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "4e8de320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T@embed_onehot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "cd151efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e67863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "55cbce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 768])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c4eb9e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12fad71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion , batch[\"motion_mask\"])\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f341fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionDecoder(\n",
    "    dim = 768,\n",
    "    logit_dim = 263,\n",
    "    attn_layers = Decoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "253fae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion = motionDecoder(quantized_enc_motion, inp_mask = batch[\"motion_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e479fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "cf9f2239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 263])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion[batch[\"motion_mask\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "bb1de763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "36335a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88422.6406, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(batch[\"motion\"] * batch[\"motion_mask\"][...,None] , pred_motion*batch[\"motion\"] * batch[\"motion_mask\"][...,None], reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1396e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "bc200fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1263)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"] .numel()/(batch[\"motion_mask\"].sum()*263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "39280ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112564"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "cf080128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9d3480dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99940)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].sum()*263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dde29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55a6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fba5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022932fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379864c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2f0f42",
   "metadata": {},
   "source": [
    "## Decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "522459ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "      \n",
    "    def __init__(self, my_dict):\n",
    "          \n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1854299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', default=\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", help=\"folder with train and test data\")\n",
    "parser.add_argument('--pretrained', default='')\n",
    "parser.add_argument('--resume', default=True, type = bool)\n",
    "parser.add_argument('--output_dir', default=\"./checkpoints/vq_768_128\")\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--fp16', default=True, type=bool)\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='aist', help=\"t2m or kit or aist\")\n",
    "parser.add_argument('--var_len', default=False, type=bool)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_bs', default=64, type=int,)\n",
    "parser.add_argument('--eval_bs', default=64, type=int,)\n",
    "parser.add_argument('--gradient_accumulation_steps', default=4, type=int,)\n",
    "\n",
    "parser.add_argument('--motion_dim', type=int, default=263, help='Input motion dimension dimension')\n",
    "parser.add_argument('--enc_dec_dim', type=int, default=768, help='Encoder and Decoder dimension')\n",
    "parser.add_argument('--depth', type=int, default=12, help='Encoder Decoder depth')\n",
    "parser.add_argument('--heads', type=int, default=10, help='Encoder Decoder number of heads')\n",
    "parser.add_argument('--codebook_dim', type=int, default=128, help='codeboook dimension')\n",
    "parser.add_argument('--codebook_size', type=int, default=1024, help='number of codebook embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--commit\", type=float, default=1, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss_vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons_loss', type=str, default='l1_smooth', help='reconstruction loss')\n",
    "parser.add_argument('--max_seq_length', type=int, default=200, help='max sequence length')\n",
    "\n",
    "parser.add_argument(\"--num_train_iters\",  default=500000,type=int)\n",
    "parser.add_argument(\"--save_steps\",  default=5000,type=int)\n",
    "parser.add_argument(\"--logging_steps\",  default=10,type=int)\n",
    "parser.add_argument(\"--wandb_every\",  default=100,type=int)\n",
    "parser.add_argument(\"--evaluate_every\",  default=10000,type=int)\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument('--warmup_steps', default=4000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--learning_rate', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "parser.add_argument('--lr_scheduler_type', default=\"cosine\", help=\"learning rate schedule type\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2ac6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aad762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d27dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "argss = Dict2Class(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9474025e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b4439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from configs.config import cfg\n",
    "\n",
    "model = VQMotionModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "012bc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\", map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd994df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77090528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d83144",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python VQ_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "3062d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "6f266128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = torch.randint(0,1024,(1,40))\n",
    "quantized, out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "37add0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[1:2].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "d0f07dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "85e9654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0577)\n",
      "tensor(1.9838)\n",
      "\n",
      "\n",
      "tensor(0.1757)\n",
      "tensor(2.9448)\n",
      "\n",
      "\n",
      "tensor(0.2407)\n",
      "tensor(1.6494)\n",
      "\n",
      "\n",
      "tensor(0.1197)\n",
      "tensor(0.4883)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(F.mse_loss(batch[\"motion\"][i,:40,:].cpu() , out_motion[i,:40,:].cpu()))\n",
    "    print(F.mse_loss(batch[\"motion\"][i,40:int(batch[\"motion_length\"][i]),:].cpu() , out_motion[i,40:int(batch[\"motion_length\"][i]),:].cpu()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8afe2",
   "metadata": {},
   "source": [
    "## Music codes to motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46c343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "155dadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9329ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c7ebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc2s = msc[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2824c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(torch.Tensor(msc2s.T)[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f6f9d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 263])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7224126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(decoded_motion_features.detach().cpu()), \"mus_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7fc869",
   "metadata": {},
   "source": [
    "## VQ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b17af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb06dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"t2m\"\n",
    "exp_name = \"vq_768_768\"\n",
    "out_dir = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\"\n",
    "out_dir = os.path.join(out_dir, f'{exp_name}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711a7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils_model.get_logger(out_dir)\n",
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4512fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cfd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45c322ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/vqvae_motion.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b09ab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([135000.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab12bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbc5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [02:21<00:00, 30.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "\n",
    "# checkpoint_dir = \"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints\"\n",
    "# dataset_opt_path = f'{checkpoint_dir}/kit/Comp_v6_KLD005/opt.txt' if dataname == 'kit' else f'{checkpoint_dir}/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "nb_joints = 21 if dataname == 'kit' else 22\n",
    "\n",
    "val_loader = dataset_TM_eval.DATALoader(dataname, True, 4, w_vectorizer, unit_length=2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6504e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd2e6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31963e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [09:46<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 14:28:03,761 INFO --> \t Eva. Iter 0 :, FID. 0.1848, Diversity Real. 9.5359, Diversity. 9.7786, R_precision_real. [0.8694062  0.97267642 0.99376076], R_precision. [0.85714286 0.96944923 0.99225473], matching_score_real. 2.9661742395460093, matching_score_pred. 3.12408512933324\n",
      "2023-04-11 14:28:03,762 INFO --> --> \t FID Improved from 1000.00000 to 0.18480 !!!\n",
      "2023-04-11 14:28:03,763 INFO --> --> \t Diversity Improved from 100.00000 to 9.77857 !!!\n",
      "2023-04-11 14:28:03,764 INFO --> --> \t Top1 Improved from 0.0000 to 0.8571 !!!\n",
      "2023-04-11 14:28:03,765 INFO --> --> \t Top2 Improved from 0.0000 to 0.9694 !!!\n",
      "2023-04-11 14:28:03,766 INFO --> --> \t Top3 Improved from 0.0000 to 0.9923 !!!\n",
      "2023-04-11 14:28:03,767 INFO --> --> \t matching_score Improved from 100.00000 to 3.12409 !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 229/1162 [00:28<01:55,  8.10it/s]\n",
      " 50%|█████     | 1/2 [10:14<10:14, 614.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-a24692995545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrepeat_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbest_fid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_div\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_matching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_vqvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_fid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_top3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_matching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_wrapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavenpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/eval_trans.py\u001b[0m in \u001b[0;36mevaluation_vqvae\u001b[0;34m(out_dir, val_loader, net, logger, writer, nb_iter, best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, eval_wrapper, draw, save, savegif, savenpy)\u001b[0m\n\u001b[1;32m     78\u001b[0m                                 \u001b[0mdraw_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0met_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_co_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_one_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_pose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mmotion_pred_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/models/evaluator_wrapper.py\u001b[0m in \u001b[0;36mget_co_embeddings\u001b[0;34m(self, word_embs, pos_ohot, cap_lens, motions, m_lens)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mmovements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovement_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mm_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_lens\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mmotion_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotion_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;34m'''Text Encoding'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/models/eval_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, m_lens)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0minput_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "repeat_time = 2\n",
    "for i in tqdm(range(repeat_time)):\n",
    "    best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(out_dir, val_loader, model, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n",
    "    fid.append(best_fid)\n",
    "    div.append(best_div)\n",
    "    top1.append(best_top1)\n",
    "    top2.append(best_top2)\n",
    "    top3.append(best_top3)\n",
    "    matching.append(best_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39a75aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2051f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07952023417818133\n",
      "div:  9.660172\n",
      "top1:  0.8640006454388985\n",
      "top2:  0.9694402610441767\n",
      "top3:  0.9935366465863454\n",
      "matching:  3.024575046017313\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', np.mean(fid))\n",
    "print('div: ', np.mean(div))\n",
    "print('top1: ', np.mean(top1))\n",
    "print('top2: ', np.mean(top2))\n",
    "print('top3: ', np.mean(top3))\n",
    "print('matching: ', np.mean(matching))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b78e2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07902298641742789\n",
      "div:  9.737120151519775\n",
      "top1:  0.8569277108433735\n",
      "top2:  0.9676204819277109\n",
      "top3:  0.992039586919105\n",
      "matching:  3.050672186836729\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', sum(fid)/repeat_time)\n",
    "print('div: ', sum(div)/repeat_time)\n",
    "print('top1: ', sum(top1)/repeat_time)\n",
    "print('top2: ', sum(top2)/repeat_time)\n",
    "print('top3: ', sum(top3)/repeat_time)\n",
    "print('matching: ', sum(matching)/repeat_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96eadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e0d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9115f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 220000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "50f7b3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0117, requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ff77b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:53<00:00, 21.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "for batch in tqdm(val_loader):\n",
    "    word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "    motion = motion.to(torch.float32)\n",
    "    denorm = val_loader.dataset.inv_transform(motion.detach().cpu())\n",
    "    \n",
    "    max_len = motion.shape[1]\n",
    "    mask = []\n",
    "    for n in m_length:\n",
    "        diff = max_len - n\n",
    "        mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "    mask = torch.stack(mask , 0)\n",
    "#     print(mask.shape)\n",
    "\n",
    "\n",
    "    motion = motion.cuda()\n",
    "    et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, denorm, m_length)\n",
    "    bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "    num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "#     for n in m_length:\n",
    "#         pred_pose_eval[:,:n] = 1\n",
    "\n",
    "#     pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "    pred_pose, ind, loss_commit = model(motion)\n",
    "    pred_pose = pred_pose.cpu()*mask[...,None]\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu())\n",
    "\n",
    "#     for i in range(bs):\n",
    "#         pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "#         pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "#         pred_pose, ind, loss_commit = net(motion[i:i+1, :m_length[i]])\n",
    "#         pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "#         pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#         pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "#         if i < min(4, bs):\n",
    "#             draw_org.append(pose_xyz)\n",
    "#             draw_pred.append(pred_xyz)\n",
    "#             draw_text.append(caption[i])\n",
    "\n",
    "    et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, (pred_denorm).detach().cpu(), m_length)\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "    temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision += temp_R\n",
    "    matching_score_pred += temp_match\n",
    "    nb_sample += bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2af1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75209ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "947ca4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 263])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d0992b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "97ebe968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([136, 172, 108, 132])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cccec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 196\n",
    "mask = []\n",
    "for n in m_length:\n",
    "    diff = max_len - n\n",
    "    mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "mask = torch.stack(mask , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7129a9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a55e0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose, ind, loss_commit = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64a5a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose = pred_pose.cpu()*mask[...,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3176f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pose[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3699717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).cpu().numpy()\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a414d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4429b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.667021077708341"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5226b829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5662530162371695"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3cad3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23019442"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_mu-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4aaf7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.109602005790272"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(gt_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8617e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.78282411570007"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4a48499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6446423427524115"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_cov-cov)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c554f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 300 if nb_sample > 300 else 100)\n",
    "diversity = calculate_diversity(motion_pred_np, 300 if nb_sample > 300 else 100)\n",
    "\n",
    "R_precision_real = R_precision_real / nb_sample\n",
    "R_precision = R_precision / nb_sample\n",
    "\n",
    "matching_score_real = matching_score_real / nb_sample\n",
    "matching_score_pred = matching_score_pred / nb_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "969b6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "624cba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  1.3705201 1.3291297 fid:  0.002796184943571589\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7ec9547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  8.298933 8.4250345 fid:  0.11668709473214278\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3da6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8276927",
   "metadata": {},
   "source": [
    "## Mean comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc6f53",
   "metadata": {},
   "source": [
    "### HumanML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f2d4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Mean.npy\")\n",
    "std_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Std.npy\")\n",
    "print(mean_hml.shape , std_hml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9eaa4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3746f080",
   "metadata": {},
   "source": [
    "### T2M Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49765035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/mean.npy\")\n",
    "std_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/std.npy\")\n",
    "print(mean_t2m.shape , std_t2m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fe2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e45c258c",
   "metadata": {},
   "source": [
    "### T2MGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3487433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy\")\n",
    "std_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy\")\n",
    "print(mean_gpt.shape , std_gpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff65f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e27bfc",
   "metadata": {},
   "source": [
    "### MDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549dfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff512d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40956958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07c74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dbaca0f",
   "metadata": {},
   "source": [
    "## Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db80c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d89e5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator(200)\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                1,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ebbbe42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 200\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30f99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f5c9afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 177, 263])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b710c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49b8c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,1024,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8ba72877",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a822e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90fc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea4c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2438ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f827af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69d4d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from options.get_eval_option import get_opt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2713f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Optimal Transport AutoEncoder training for AIST',\n",
    "                                 add_help=True,\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "## dataloader  \n",
    "parser.add_argument('--dataname', type=str, default='t2m', help='dataset directory')\n",
    "parser.add_argument('--batch-size', default=128, type=int, help='batch size')\n",
    "parser.add_argument('--window-size', type=int, default=64, help='training motion length')\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--total-iter', default=200000, type=int, help='number of total iterations to run')\n",
    "parser.add_argument('--warm-up-iter', default=1000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--lr', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--lr-scheduler', default=[50000, 400000], nargs=\"+\", type=int, help=\"learning rate schedule (iterations)\")\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument(\"--commit\", type=float, default=0.02, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss-vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons-loss', type=str, default='l2', help='reconstruction loss')\n",
    "\n",
    "## vqvae arch\n",
    "parser.add_argument(\"--code-dim\", type=int, default=512, help=\"embedding dimension\")\n",
    "parser.add_argument(\"--nb-code\", type=int, default=512, help=\"nb of embedding\")\n",
    "parser.add_argument(\"--mu\", type=float, default=0.99, help=\"exponential moving average to update the codebook\")\n",
    "parser.add_argument(\"--down-t\", type=int, default=2, help=\"downsampling rate\")\n",
    "parser.add_argument(\"--stride-t\", type=int, default=2, help=\"stride size\")\n",
    "parser.add_argument(\"--width\", type=int, default=512, help=\"width of the network\")\n",
    "parser.add_argument(\"--depth\", type=int, default=3, help=\"depth of the network\")\n",
    "parser.add_argument(\"--dilation-growth-rate\", type=int, default=3, help=\"dilation growth rate\")\n",
    "parser.add_argument(\"--output-emb-width\", type=int, default=512, help=\"output embedding width\")\n",
    "parser.add_argument('--vq-act', type=str, default='relu', choices = ['relu', 'silu', 'gelu'], help='dataset directory')\n",
    "parser.add_argument('--vq-norm', type=str, default=None, help='dataset directory')\n",
    "\n",
    "## quantizer\n",
    "parser.add_argument(\"--quantizer\", type=str, default='ema_reset', choices = ['ema', 'orig', 'ema_reset', 'reset'], help=\"eps for optimal transport\")\n",
    "parser.add_argument('--beta', type=float, default=1.0, help='commitment loss in standard VQ')\n",
    "\n",
    "## resume\n",
    "parser.add_argument(\"--resume-pth\", type=str, default=None, help='resume pth for VQ')\n",
    "parser.add_argument(\"--resume-gpt\", type=str, default=None, help='resume pth for GPT')\n",
    "\n",
    "\n",
    "## output directory \n",
    "parser.add_argument('--out-dir', type=str, default='output_vqfinal/', help='output directory')\n",
    "parser.add_argument('--results-dir', type=str, default='visual_results/', help='output directory')\n",
    "parser.add_argument('--visual-name', type=str, default='baseline', help='output directory')\n",
    "parser.add_argument('--exp-name', type=str, default='exp_debug', help='name of the experiment, will create a file inside out-dir')\n",
    "## other\n",
    "parser.add_argument('--print-iter', default=200, type=int, help='print frequency')\n",
    "parser.add_argument('--eval-iter', default=1000, type=int, help='evaluation frequency')\n",
    "parser.add_argument('--seed', default=123, type=int, help='seed for initializing training.')\n",
    "\n",
    "parser.add_argument('--vis-gt', action='store_true', help='whether visualize GT motions')\n",
    "parser.add_argument('--nb-vis', default=20, type=int, help='nb of visualizations')\n",
    "\n",
    "\n",
    "args=parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3917f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate,\n",
    "                       args.vq_act,\n",
    "                       args.vq_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc007519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./pretrained/VQVAE/net_last.pth\", map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a75c251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef4abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,512,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c65d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.forward_decoder(indices.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28ff9a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 263])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c69ad74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813e69c",
   "metadata": {},
   "source": [
    "## T2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9dcebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "bb0d56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc3e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "fdd0a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "9d358a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from utils.eval_trans import *\n",
    "from options.get_eval_option import get_opt\n",
    "from models.evaluator_wrapper import EvaluatorModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a551e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "b067c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = option_vq.get_args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "afbb9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word_vectorizer import WordVectorizer\n",
    "w_vectorizer = WordVectorizer('./glove', 'our_vab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "672134b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading checkpoints/t2m/Comp_v6_KLD005/opt.txt\n",
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    }
   ],
   "source": [
    "dataset_opt_path = 'checkpoints/t2m/Comp_v6_KLD005/opt.txt'\n",
    "wrapper_opt = get_opt(dataset_opt_path, torch.device('cuda'))\n",
    "eval_wrapper = EvaluatorModelWrapper(wrapper_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "6be57dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_opt.dim_movement_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "0eece36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'checkpoints_dir': './checkpoints',\n",
       " 'dataset_name': 't2m',\n",
       " 'decomp_name': 'Decomp_SP001_SM001_H512',\n",
       " 'dim_att_vec': 512,\n",
       " 'dim_dec_hidden': 1024,\n",
       " 'dim_movement2_dec_hidden': 512,\n",
       " 'dim_movement_dec_hidden': 512,\n",
       " 'dim_movement_enc_hidden': 512,\n",
       " 'dim_movement_latent': 512,\n",
       " 'dim_msd_hidden': 512,\n",
       " 'dim_pos_hidden': 1024,\n",
       " 'dim_pri_hidden': 1024,\n",
       " 'dim_seq_de_hidden': 512,\n",
       " 'dim_seq_en_hidden': 512,\n",
       " 'dim_text_hidden': 512,\n",
       " 'dim_z': 128,\n",
       " 'early_stop_count': 3,\n",
       " 'estimator_mod': 'bigru',\n",
       " 'eval_every_e': 5,\n",
       " 'feat_bias': 5,\n",
       " 'fixed_steps': 5,\n",
       " 'gpu_id': 1,\n",
       " 'input_z': False,\n",
       " 'is_continue': False,\n",
       " 'is_train': False,\n",
       " 'lambda_fake': 10,\n",
       " 'lambda_gan_l': 0.1,\n",
       " 'lambda_gan_mt': 0.1,\n",
       " 'lambda_gan_mv': 0.1,\n",
       " 'lambda_kld': 0.005,\n",
       " 'lambda_rec': 1,\n",
       " 'lambda_rec_init': 1,\n",
       " 'lambda_rec_mot': 1,\n",
       " 'lambda_rec_mov': 1,\n",
       " 'log_every': 50,\n",
       " 'lr': 0.0002,\n",
       " 'max_sub_epoch': 50,\n",
       " 'max_text_len': 20,\n",
       " 'n_layers_dec': 1,\n",
       " 'n_layers_msd': 2,\n",
       " 'n_layers_pos': 1,\n",
       " 'n_layers_pri': 1,\n",
       " 'n_layers_seq_de': 2,\n",
       " 'n_layers_seq_en': 1,\n",
       " 'name': 'Comp_v6_KLD005',\n",
       " 'num_experts': 4,\n",
       " 'save_every_e': 10,\n",
       " 'save_latest': 500,\n",
       " 'text_enc_mod': 'bigru',\n",
       " 'tf_ratio': 0.4,\n",
       " 'unit_length': 4,\n",
       " 'which_epoch': 'finest',\n",
       " 'save_root': './checkpoints/t2m/Comp_v6_KLD005',\n",
       " 'model_dir': './checkpoints/t2m/Comp_v6_KLD005/model',\n",
       " 'meta_dir': './checkpoints/t2m/Comp_v6_KLD005/meta',\n",
       " 'data_root': './dataset/HumanML3D/',\n",
       " 'motion_dir': './dataset/HumanML3D/new_joint_vecs',\n",
       " 'text_dir': './dataset/HumanML3D/texts',\n",
       " 'joints_num': 22,\n",
       " 'dim_pose': 263,\n",
       " 'max_motion_length': 196,\n",
       " 'max_motion_frame': 196,\n",
       " 'max_motion_token': 55,\n",
       " 'dim_word': 300,\n",
       " 'num_classes': 50,\n",
       " 'device': device(type='cuda'),\n",
       " 'dim_pos_ohot': 15,\n",
       " 'dim_motion_hidden': 1024,\n",
       " 'dim_coemb_hidden': 512}"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(wrapper_opt)[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61e55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba2196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "34426d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:03<00:00, 1332.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_loader = dataset_TM_eval.DATALoader(\"t2m\", True, 4, w_vectorizer, unit_length=2**2, num_workers = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "26216547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "9994de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "5f859dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "8493ab05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([192, 192, 180, 136])"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "24318dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion , ind , com_loss = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(args.out_dir, val_loader, net, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "dd989791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "\n",
    "\n",
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)\n",
    "motion = motion.cuda()\n",
    "et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, motion, m_length)\n",
    "bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "\n",
    "pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "\n",
    "for i in range(bs):\n",
    "    pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "    pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "    pred_pose, ind,loss_commit = model(motion[i:i+1, :m_length[i]])\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "    pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#     if savenpy:\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_gt.npy'), pose_xyz[:, :m_length[i]].cpu().numpy())\n",
    "#         np.save(os.path.join(out_dir, name[i]+'_pred.npy'), pred_xyz.detach().cpu().numpy())\n",
    "\n",
    "    pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "    if i < min(4, bs):\n",
    "        draw_org.append(pose_xyz)\n",
    "        draw_pred.append(pred_xyz)\n",
    "        draw_text.append(caption[i])\n",
    "\n",
    "et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, pred_pose_eval, m_length)\n",
    "\n",
    "motion_pred_list.append(em_pred)\n",
    "motion_annotation_list.append(em)\n",
    "\n",
    "temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision_real += temp_R\n",
    "matching_score_real += temp_match\n",
    "temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "R_precision += temp_R\n",
    "matching_score_pred += temp_match\n",
    "\n",
    "nb_sample += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "44ef9095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "ccd1608f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f002049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
