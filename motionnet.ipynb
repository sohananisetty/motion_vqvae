{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f08df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c1b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from einops import rearrange, reduce, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb1208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dcc57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils import data\n",
    "# import numpy as np\n",
    "# from os.path import join as pjoin\n",
    "# import random\n",
    "# import codecs as cs\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# class VQMotionDataset(data.Dataset):\n",
    "#     def __init__(self, dataset_name, window_size = 64, unit_length = 4):\n",
    "#         self.window_size = window_size\n",
    "#         self.unit_length = unit_length\n",
    "#         self.dataset_name = dataset_name\n",
    "\n",
    "#         if dataset_name == 't2m':\n",
    "#             self.data_root = '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 22\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = ''\n",
    "\n",
    "#         elif dataset_name == 'kit':\n",
    "#             self.data_root = './dataset/KIT-ML'\n",
    "#             self.motion_dir = pjoin(self.data_root, 'new_joint_vecs')\n",
    "#             self.text_dir = pjoin(self.data_root, 'texts')\n",
    "#             self.joints_num = 21\n",
    "\n",
    "#             self.max_motion_length = 196\n",
    "#             self.meta_dir = 'checkpoints/kit/VQVAEV3_CB1024_CMT_H1024_NRES3/meta'\n",
    "        \n",
    "#         joints_num = self.joints_num\n",
    "\n",
    "#         mean = np.load(pjoin(self.data_root, 'Mean.npy'))\n",
    "#         std = np.load(pjoin(self.data_root, 'Std.npy'))\n",
    "\n",
    "#         split_file = pjoin(self.data_root, 'val.txt')\n",
    "\n",
    "#         self.data = []\n",
    "#         self.lengths = []\n",
    "#         id_list = []\n",
    "#         with cs.open(split_file, 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 id_list.append(line.strip())\n",
    "\n",
    "#         for name in tqdm(id_list):\n",
    "#             try:\n",
    "#                 motion = np.load(pjoin(self.motion_dir, name + '.npy'))\n",
    "#                 if motion.shape[0] < self.window_size:\n",
    "#                     continue\n",
    "#                 self.lengths.append(motion.shape[0] - self.window_size)\n",
    "#                 self.data.append(motion)\n",
    "#             except:\n",
    "#                 # Some motion may not exist in KIT dataset\n",
    "#                 pass\n",
    "\n",
    "            \n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "#         print(\"Total number of motions {}\".format(len(self.data)))\n",
    "\n",
    "#     def inv_transform(self, data):\n",
    "#         return data * self.std + self.mean\n",
    "    \n",
    "#     def compute_sampling_prob(self) : \n",
    "        \n",
    "#         prob = np.array(self.lengths, dtype=np.float32)\n",
    "#         prob /= np.sum(prob)\n",
    "#         return prob\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         motion = self.data[item]\n",
    "        \n",
    "#         idx = random.randint(0, len(motion) - self.window_size)\n",
    "\n",
    "#         motion = motion[idx:idx+self.window_size]\n",
    "#         \"Z Normalization\"\n",
    "#         motion = (motion - self.mean) / self.std\n",
    "\n",
    "#         return motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "191dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 952/952 [00:54<00:00, 17.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainSet = VQMotionDataset(\"aist\", data_root=\"/srv/scratch/sanisetty3/music_motion/AIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c955e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATALoader(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            num_workers = 0,\n",
    "            shuffle = True,\n",
    "           ):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                #sampler=sampler,\n",
    "                                                num_workers=num_workers,\n",
    "                                                #collate_fn=collate_fn,\n",
    "                                                drop_last = True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe1f980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ffc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DATALoader(trainSet , 2)\n",
    "train_loader_iter = cycle(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f6baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion = next(train_loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c04284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 40, 263])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f81db040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed7f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l2\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70a8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQMotionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "955434e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorQuantize(\n",
       "  (project_in): Identity()\n",
       "  (project_out): Identity()\n",
       "  (_codebook): EuclideanCodebook()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f567b04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_motion , indices, commit_loss = model(gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad74b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_motion = loss_fnc(pred_motion, gt_motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cc9a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbc4802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04a27107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f38d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 60, 263])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6576efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(pred_motion[0:1].detach().cpu()), \"aist0_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cebe1",
   "metadata": {},
   "source": [
    "## Variable motion lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2d204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import random\n",
    "import codecs as cs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e21233d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f803a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionCollator():\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.bos = torch.LongTensor(([0]))\n",
    "        self.eos = torch.LongTensor(([2]))\n",
    "        self.pad = torch.LongTensor(([1]))\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        \n",
    "\n",
    "        pad_batch_inputs = []\n",
    "        pad_batch_mask = []\n",
    "        motion_lengths = []\n",
    "        names = []\n",
    "        max_len = max([sample.shape[0] for sample, name in samples])\n",
    "\n",
    "\n",
    "        for inp,name in samples:\n",
    "            n,d = inp.shape\n",
    "            diff = max_len - n\n",
    "            mask = torch.BoolTensor([1]*n + [0]*diff)\n",
    "            padded = torch.concatenate((torch.tensor(inp) , torch.ones((diff,d))*self.pad))\n",
    "            pad_batch_inputs.append(padded)\n",
    "            pad_batch_mask.append(mask)\n",
    "            motion_lengths.append(n)\n",
    "            names.append(name)\n",
    "\n",
    "    \n",
    "        batch = {\n",
    "            \"motion\": torch.stack(pad_batch_inputs , 0),\n",
    "            \"motion_length\": torch.Tensor(motion_lengths),\n",
    "            \"motion_mask\" : torch.stack(pad_batch_mask , 0),\n",
    "            \"names\" : np.array(names)\n",
    "\n",
    "        }\n",
    "\n",
    "   \n",
    "        return batch    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f75b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator\n",
    "from core.datasets import vqa_motion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa75fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 864.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_ds = vqa_motion_dataset.VQMotionDataset(\"t2m\",split = \"render\", data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b358c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f1f1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 23.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95b501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "761b8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator(200)\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                4,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f9ae5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123220e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fd13502",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c02e19f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 92, 263])\n",
      "tensor([81., 82., 68., 92.])\n"
     ]
    }
   ],
   "source": [
    "gt_motion = batch[\"motion\"]\n",
    "print(gt_motion.shape)\n",
    "print(batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf528414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 140\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aa2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c03c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "38ba246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_len = int(batch.get(\"motion_lengths\" , [gt_motion.shape[1]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a78e538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 97, 263])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_motion[:,:motion_len,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea5f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4e7ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import MotionTransformer, MotionDecoder, LinearEmbedding, VQMotionModel\n",
    "from core.quantization.core_vq import VectorQuantization\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n",
    "from core.models.loss import ReConsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69b56531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8bbc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b73c8266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "pred_motion , indices, commit_loss = model(batch[\"motion\"].cuda() , mask = batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ead5b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93fd453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnc = ReConsLoss(\"l1\", 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79098dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1719, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abecb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2821, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fnc.forward_vel(pred_motion, batch[\"motion\"].cuda() ,  batch[\"motion_mask\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2ff08a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionEncoder = MotionTransformer(\n",
    "        inp_dim =263,\n",
    "        max_seq_len = 200,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        attn_layers = Encoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "dd6b943b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    }
   ],
   "source": [
    "encoded_motion = motionEncoder(batch[\"motion\"] , mask = batch[\"motion_mask\"])\n",
    "print(encoded_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c45277c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization(\n",
    "        dim = 768,\n",
    "        codebook_dim = 128,\n",
    "        codebook_size = 1024,\n",
    "        decay = 0.95,\n",
    "        commitment_weight = 1,\n",
    "        kmeans_init = True,\n",
    "        threshold_ema_dead_code = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c560ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:350: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion)\n",
    "print(quantized_enc_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "46a0be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = vq.codebook.t()\n",
    "embed.pow(2).sum(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "686c6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoded_motion[:,:,:128]\n",
    "shape = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "098f7dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3801,  0.7024,  1.5966,  ...,  0.7535, -1.5004, -0.4283],\n",
       "        [-0.2908,  0.5800,  1.6096,  ...,  0.4755, -1.5223, -0.3614],\n",
       "        [-0.2561,  0.4115,  1.5109,  ...,  0.1626, -1.4867, -0.2438],\n",
       "        ...,\n",
       "        [ 0.5579,  0.1785,  0.0483,  ..., -0.8444, -0.6883,  1.2005],\n",
       "        [ 0.5358,  0.1656,  0.1023,  ..., -0.8245, -0.6591,  1.2180],\n",
       "        [ 0.5378,  0.0897,  0.1818,  ..., -0.8273, -0.6676,  1.2187]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rearrange(x, \"... d -> (...) d\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a94a1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = -(\n",
    "    x.pow(2).sum(1, keepdim=True)\n",
    "    - 2 * x @ embed\n",
    "    + embed.pow(2).sum(0, keepdim=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "01f8da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "8049f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = dist.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5628c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bb2b807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = rearrange(batch[\"motion_mask\"], \"... -> (...)\")\n",
    "inp_mask.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "116e3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.finfo(dist.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d192c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "eb18c634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6291e+02, -1.6781e+02, -1.2629e+02,  ..., -3.8359e+11,\n",
       "        -3.0852e+11, -4.7003e+11], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a2ee54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = (dist + dist.masked_fill(~inp_mask[...,None] , mask_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fde61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "8a80a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind2 = dist2.max(dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "bc4b4929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_ind2[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7937985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot = F.one_hot(embed_ind2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "c80bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_ind = vq._codebook.postprocess_emb(embed_ind, shape)\n",
    "quantize = vq._codebook.dequantize(embed_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "eeb5793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "031c3f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "8191be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 128])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "34a9eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_onehot2 = (embed_onehot * inp_mask[...,None] ).type(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "4dd40b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428, 1024])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_onehot2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "4e8de320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.T@embed_onehot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "cd151efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e67863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "55cbce93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 768])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "c4eb9e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12fad71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 107])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/quantization/core_vq.py:375: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantized_enc_motion, indices, commit_loss = vq(encoded_motion , batch[\"motion_mask\"])\n",
    "print(indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f341fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionDecoder(\n",
    "    dim = 768,\n",
    "    logit_dim = 263,\n",
    "    attn_layers = Decoder(\n",
    "            dim = 768,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "253fae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_motion = motionDecoder(quantized_enc_motion, inp_mask = batch[\"motion_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e479fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "cf9f2239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([380, 263])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_motion[batch[\"motion_mask\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "bb1de763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107, 263])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "36335a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88422.6406, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(batch[\"motion\"] * batch[\"motion_mask\"][...,None] , pred_motion*batch[\"motion\"] * batch[\"motion_mask\"][...,None], reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1396e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "bc200fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1263)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"] .numel()/(batch[\"motion_mask\"].sum()*263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "39280ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112564"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "cf080128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 107])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9d3480dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99940)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_mask\"].sum()*263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350dde29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55a6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fba5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022932fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379864c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2f0f42",
   "metadata": {},
   "source": [
    "## Decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "522459ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Class(object):\n",
    "      \n",
    "    def __init__(self, my_dict):\n",
    "          \n",
    "        for key in my_dict:\n",
    "            setattr(self, key, my_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1854299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_folder', default=\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\", help=\"folder with train and test data\")\n",
    "parser.add_argument('--pretrained', default='')\n",
    "parser.add_argument('--resume', default=True, type = bool)\n",
    "parser.add_argument('--output_dir', default=\"./checkpoints/vq_768_128\")\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--fp16', default=True, type=bool)\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='aist', help=\"t2m or kit or aist\")\n",
    "parser.add_argument('--var_len', default=False, type=bool)\n",
    "\n",
    "\n",
    "parser.add_argument('--train_bs', default=64, type=int,)\n",
    "parser.add_argument('--eval_bs', default=64, type=int,)\n",
    "parser.add_argument('--gradient_accumulation_steps', default=4, type=int,)\n",
    "\n",
    "parser.add_argument('--motion_dim', type=int, default=263, help='Input motion dimension dimension')\n",
    "parser.add_argument('--enc_dec_dim', type=int, default=768, help='Encoder and Decoder dimension')\n",
    "parser.add_argument('--depth', type=int, default=12, help='Encoder Decoder depth')\n",
    "parser.add_argument('--heads', type=int, default=10, help='Encoder Decoder number of heads')\n",
    "parser.add_argument('--codebook_dim', type=int, default=128, help='codeboook dimension')\n",
    "parser.add_argument('--codebook_size', type=int, default=1024, help='number of codebook embeddings')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"--commit\", type=float, default=1, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss_vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons_loss', type=str, default='l1_smooth', help='reconstruction loss')\n",
    "parser.add_argument('--max_seq_length', type=int, default=200, help='max sequence length')\n",
    "\n",
    "parser.add_argument(\"--num_train_iters\",  default=500000,type=int)\n",
    "parser.add_argument(\"--save_steps\",  default=5000,type=int)\n",
    "parser.add_argument(\"--logging_steps\",  default=10,type=int)\n",
    "parser.add_argument(\"--wandb_every\",  default=100,type=int)\n",
    "parser.add_argument(\"--evaluate_every\",  default=10000,type=int)\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument('--warmup_steps', default=4000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--learning_rate', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "parser.add_argument('--lr_scheduler_type', default=\"cosine\", help=\"learning rate schedule type\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2ac6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aad762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d27dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "argss = Dict2Class(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9474025e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b4439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.vqvae import VQMotionModel\n",
    "from configs.config import cfg\n",
    "\n",
    "model = VQMotionModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "012bc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/vq_768_128/vqvae_motion.pt\", map_location = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd994df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pkg[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77090528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d83144",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python VQ_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "3062d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "6f266128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inds = torch.randint(0,1024,(1,40))\n",
    "quantized, out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "37add0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[1:2].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "d0f07dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([177., 165., 114.,  74.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "85e9654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0577)\n",
      "tensor(1.9838)\n",
      "\n",
      "\n",
      "tensor(0.1757)\n",
      "tensor(2.9448)\n",
      "\n",
      "\n",
      "tensor(0.2407)\n",
      "tensor(1.6494)\n",
      "\n",
      "\n",
      "tensor(0.1197)\n",
      "tensor(0.4883)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(F.mse_loss(batch[\"motion\"][i,:40,:].cpu() , out_motion[i,:40,:].cpu()))\n",
    "    print(F.mse_loss(batch[\"motion\"][i,40:int(batch[\"motion_length\"][i]),:].cpu() , out_motion[i,40:int(batch[\"motion_length\"][i]),:].cpu()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8afe2",
   "metadata": {},
   "source": [
    "## Music codes to motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46c343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "155dadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9329ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4de37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c7ebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "msc2s = msc[:,:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2824c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_motion_features = model.motionDecoder(torch.Tensor(msc2s.T)[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f6f9d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 263])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_motion_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7224126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(decoded_motion_features.detach().cpu()), \"mus_pred\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe7fc869",
   "metadata": {},
   "source": [
    "## VQ_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69d0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/motion_vqvae\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/motion_vqvae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb06dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "54fad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"t2m\"\n",
    "exp_name = \"vq_768_768\"\n",
    "out_dir = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals\"\n",
    "out_dir = os.path.join(out_dir, f'{exp_name}')\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711a7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils_model.get_logger(out_dir)\n",
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6510837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84939b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42824b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/vqvae_motion.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c92c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200000.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc53c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bbbc5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1522.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-320-c759a1ca7f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnb_joints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdataname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kit'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_TM_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/datasets/dataset_TM_eval.py\u001b[0m in \u001b[0;36mDATALoader\u001b[0;34m(dataset_name, is_test, batch_size, w_vectorizer, num_workers, unit_length)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 num_workers = 0, unit_length = 4) : \n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     val_loader = torch.utils.data.DataLoader(Text2MotionDataset(dataset_name, is_test, w_vectorizer, unit_length=unit_length),\n\u001b[0m\u001b[1;32m    222\u001b[0m                                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                                               \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/core/datasets/dataset_TM_eval.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, is_test, w_vectorizer, feat_bias, max_text_len, unit_length)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_name_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mname_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_name_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "\n",
    "# checkpoint_dir = \"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints\"\n",
    "# dataset_opt_path = f'{checkpoint_dir}/kit/Comp_v6_KLD005/opt.txt' if dataname == 'kit' else f'{checkpoint_dir}/t2m/Comp_v6_KLD005/opt.txt'\n",
    "\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "\n",
    "\n",
    "##### ---- Dataloader ---- #####\n",
    "nb_joints = 21 if dataname == 'kit' else 22\n",
    "\n",
    "val_loader = dataset_TM_eval.DATALoader(\"aist\", True, 4, w_vectorizer, unit_length=2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c0c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0e6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e37606a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "31963e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fid = []\n",
    "div = []\n",
    "top1 = []\n",
    "top2 = []\n",
    "top3 = []\n",
    "matching = []\n",
    "repeat_time = 2\n",
    "for i in tqdm(range(repeat_time)):\n",
    "    best_fid, best_iter, best_div, best_top1, best_top2, best_top3, best_matching, writer, logger = eval_trans.evaluation_vqvae(out_dir, val_loader, model, logger, writer, 0, best_fid=1000, best_iter=0, best_div=100, best_top1=0, best_top2=0, best_top3=0, best_matching=100, eval_wrapper=eval_wrapper, draw=False, save=False, savenpy=(i==0))\n",
    "    fid.append(best_fid)\n",
    "    div.append(best_div)\n",
    "    top1.append(best_top1)\n",
    "    top2.append(best_top2)\n",
    "    top3.append(best_top3)\n",
    "    matching.append(best_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fc87f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48a7639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07952023417818133\n",
      "div:  9.660172\n",
      "top1:  0.8640006454388985\n",
      "top2:  0.9694402610441767\n",
      "top3:  0.9935366465863454\n",
      "matching:  3.024575046017313\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', np.mean(fid))\n",
    "print('div: ', np.mean(div))\n",
    "print('top1: ', np.mean(top1))\n",
    "print('top2: ', np.mean(top2))\n",
    "print('top3: ', np.mean(top3))\n",
    "print('matching: ', np.mean(matching))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b78e2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:\n",
      "fid:  0.07902298641742789\n",
      "div:  9.737120151519775\n",
      "top1:  0.8569277108433735\n",
      "top2:  0.9676204819277109\n",
      "top3:  0.992039586919105\n",
      "matching:  3.050672186836729\n"
     ]
    }
   ],
   "source": [
    "print('final result:')\n",
    "print('fid: ', sum(fid)/repeat_time)\n",
    "print('div: ', sum(div)/repeat_time)\n",
    "print('top1: ', sum(top1)/repeat_time)\n",
    "print('top2: ', sum(top2)/repeat_time)\n",
    "print('top3: ', sum(top3)/repeat_time)\n",
    "print('matching: ', sum(matching)/repeat_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b81a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df310cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "49b4b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 220000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "1c5e6d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0117, requires_grad=True)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"total_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd88f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [00:53<00:00, 21.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "nb_sample = 0\n",
    "\n",
    "draw_org = []\n",
    "draw_pred = []\n",
    "draw_text = []\n",
    "\n",
    "\n",
    "motion_annotation_list = []\n",
    "motion_pred_list = []\n",
    "\n",
    "R_precision_real = 0\n",
    "R_precision = 0\n",
    "\n",
    "nb_sample = 0\n",
    "matching_score_real = 0\n",
    "matching_score_pred = 0\n",
    "for batch in tqdm(val_loader):\n",
    "    word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "    motion = motion.to(torch.float32)\n",
    "    denorm = val_loader.dataset.inv_transform(motion.detach().cpu())\n",
    "    \n",
    "    max_len = motion.shape[1]\n",
    "    mask = []\n",
    "    for n in m_length:\n",
    "        diff = max_len - n\n",
    "        mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "    mask = torch.stack(mask , 0)\n",
    "#     print(mask.shape)\n",
    "\n",
    "\n",
    "    motion = motion.cuda()\n",
    "    et, em = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, denorm, m_length)\n",
    "    bs, seq = motion.shape[0], motion.shape[1]\n",
    "\n",
    "    num_joints = 21 if motion.shape[-1] == 251 else 22\n",
    "#     for n in m_length:\n",
    "#         pred_pose_eval[:,:n] = 1\n",
    "\n",
    "#     pred_pose_eval = torch.zeros((bs, seq, motion.shape[-1])).cuda()\n",
    "    pred_pose, ind, loss_commit = model(motion)\n",
    "    pred_pose = pred_pose.cpu()*mask[...,None]\n",
    "    pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu())\n",
    "\n",
    "#     for i in range(bs):\n",
    "#         pose = val_loader.dataset.inv_transform(motion[i:i+1, :m_length[i], :].detach().cpu().numpy())\n",
    "#         pose_xyz = recover_from_ric(torch.from_numpy(pose).float().cuda(), num_joints)\n",
    "\n",
    "\n",
    "#         pred_pose, ind, loss_commit = net(motion[i:i+1, :m_length[i]])\n",
    "#         pred_denorm = val_loader.dataset.inv_transform(pred_pose.detach().cpu().numpy())\n",
    "#         pred_xyz = recover_from_ric(torch.from_numpy(pred_denorm).float().cuda(), num_joints)\n",
    "\n",
    "#         pred_pose_eval[i:i+1,:m_length[i],:] = pred_pose\n",
    "\n",
    "#         if i < min(4, bs):\n",
    "#             draw_org.append(pose_xyz)\n",
    "#             draw_pred.append(pred_xyz)\n",
    "#             draw_text.append(caption[i])\n",
    "\n",
    "    et_pred, em_pred = eval_wrapper.get_co_embeddings(word_embeddings, pos_one_hots, sent_len, (pred_denorm).detach().cpu(), m_length)\n",
    "\n",
    "    motion_pred_list.append(em_pred)\n",
    "    motion_annotation_list.append(em)\n",
    "\n",
    "    temp_R, temp_match = calculate_R_precision(et.cpu().numpy(), em.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision_real += temp_R\n",
    "    matching_score_real += temp_match\n",
    "    temp_R, temp_match = calculate_R_precision(et_pred.cpu().numpy(), em_pred.cpu().numpy(), top_k=3, sum_all=True)\n",
    "    R_precision += temp_R\n",
    "    matching_score_pred += temp_match\n",
    "    nb_sample += bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c00269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "39879e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, token, name = batch\n",
    "motion = motion.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c16137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ebbe0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 22, 18, 15])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35823a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([136, 172, 108, 132])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5c6591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 196\n",
    "mask = []\n",
    "for n in m_length:\n",
    "    diff = max_len - n\n",
    "    mask.append(torch.BoolTensor([1]*n + [0]*diff))\n",
    "mask = torch.stack(mask , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffe8e2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80630389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose, ind, loss_commit = model(motion.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5d88a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pose = pred_pose.cpu()*mask[...,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59710b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pose[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3275c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_annotation_np = torch.cat(motion_annotation_list, dim=0).cpu().numpy()\n",
    "motion_pred_np = torch.cat(motion_pred_list, dim=0).cpu().numpy()\n",
    "gt_mu, gt_cov  = calculate_activation_statistics(motion_annotation_np)\n",
    "mu, cov= calculate_activation_statistics(motion_pred_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cb5c840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.667021077708341"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gt_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6b984e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5662530162371695"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dacdbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23019442"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_mu-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9980a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.109602005790272"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(gt_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2476b235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.78282411570007"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa0e5eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6446423427524115"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((gt_cov-cov)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91659bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "diversity_real = calculate_diversity(motion_annotation_np, 300 if nb_sample > 300 else 100)\n",
    "diversity = calculate_diversity(motion_pred_np, 300 if nb_sample > 300 else 100)\n",
    "\n",
    "R_precision_real = R_precision_real / nb_sample\n",
    "R_precision = R_precision / nb_sample\n",
    "\n",
    "matching_score_real = matching_score_real / nb_sample\n",
    "matching_score_pred = matching_score_pred / nb_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74e15670",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = calculate_frechet_distance(gt_mu, gt_cov, mu, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "beac57a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  1.3705201 1.3291297 fid:  0.002796184943571589\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbdb04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity:  8.298933 8.4250345 fid:  0.11668709473214278\n"
     ]
    }
   ],
   "source": [
    "print(\"diversity: \" ,diversity_real , diversity, \"fid: \", fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc3f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fdffe3",
   "metadata": {},
   "source": [
    "## Mean comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd290f",
   "metadata": {},
   "source": [
    "### HumanML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f7167f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Mean.npy\")\n",
    "std_hml = np.load(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/Std.npy\")\n",
    "print(mean_hml.shape , std_hml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e334da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbcd1984",
   "metadata": {},
   "source": [
    "### T2M Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f2f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/mean.npy\")\n",
    "std_t2m = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/Comp_v6_KLD005/meta/std.npy\")\n",
    "print(mean_t2m.shape , std_t2m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bbe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "707fda7f",
   "metadata": {},
   "source": [
    "### T2MGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43cd0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263,) (263,)\n"
     ]
    }
   ],
   "source": [
    "mean_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/mean.npy\")\n",
    "std_gpt = np.load(\"/srv/scratch/sanisetty3/music_motion/T2M-GPT/checkpoints/t2m/VQVAEV3_CB1024_CMT_H1024_NRES3/meta/std.npy\")\n",
    "print(mean_gpt.shape , std_gpt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b8270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83d748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b936b260",
   "metadata": {},
   "source": [
    "### MDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29ae96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f61808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e38297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80f22be",
   "metadata": {},
   "source": [
    "## Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a217836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*train_ds.std+train_ds.mean, 22)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, 22, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1429086d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 40.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D//\")\n",
    "collate_fn = MotionCollator(200)\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                1,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0942e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 200\n"
     ]
    }
   ],
   "source": [
    "train_loader.dataset.set_stage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "30f99782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a0b0daa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 109, 263])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d23e270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = model.encode(batch[\"motion\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49b8c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,1024,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8ba72877",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = model.decode(ind.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb567a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(batch[\"motion\"][0:1].detach().cpu()), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a822e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f90fc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc323539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c54d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf462024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
     ]
    }
   ],
   "source": [
    "%cd /coc/scratch/sanisetty3/music_motion/T2M-GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80e08f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.vqvae as vqvae\n",
    "import options.option_vq as option_vq\n",
    "import utils.utils_model as utils_model\n",
    "from dataset import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from options.get_eval_option import get_opt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01d14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Optimal Transport AutoEncoder training for AIST',\n",
    "                                 add_help=True,\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "## dataloader  \n",
    "parser.add_argument('--dataname', type=str, default='t2m', help='dataset directory')\n",
    "parser.add_argument('--batch-size', default=128, type=int, help='batch size')\n",
    "parser.add_argument('--window-size', type=int, default=64, help='training motion length')\n",
    "\n",
    "## optimization\n",
    "parser.add_argument('--total-iter', default=200000, type=int, help='number of total iterations to run')\n",
    "parser.add_argument('--warm-up-iter', default=1000, type=int, help='number of total iterations for warmup')\n",
    "parser.add_argument('--lr', default=2e-4, type=float, help='max learning rate')\n",
    "parser.add_argument('--lr-scheduler', default=[50000, 400000], nargs=\"+\", type=int, help=\"learning rate schedule (iterations)\")\n",
    "parser.add_argument('--gamma', default=0.05, type=float, help=\"learning rate decay\")\n",
    "\n",
    "parser.add_argument('--weight-decay', default=0.0, type=float, help='weight decay')\n",
    "parser.add_argument(\"--commit\", type=float, default=0.02, help=\"hyper-parameter for the commitment loss\")\n",
    "parser.add_argument('--loss-vel', type=float, default=0.1, help='hyper-parameter for the velocity loss')\n",
    "parser.add_argument('--recons-loss', type=str, default='l2', help='reconstruction loss')\n",
    "\n",
    "## vqvae arch\n",
    "parser.add_argument(\"--code-dim\", type=int, default=512, help=\"embedding dimension\")\n",
    "parser.add_argument(\"--nb-code\", type=int, default=512, help=\"nb of embedding\")\n",
    "parser.add_argument(\"--mu\", type=float, default=0.99, help=\"exponential moving average to update the codebook\")\n",
    "parser.add_argument(\"--down-t\", type=int, default=2, help=\"downsampling rate\")\n",
    "parser.add_argument(\"--stride-t\", type=int, default=2, help=\"stride size\")\n",
    "parser.add_argument(\"--width\", type=int, default=512, help=\"width of the network\")\n",
    "parser.add_argument(\"--depth\", type=int, default=3, help=\"depth of the network\")\n",
    "parser.add_argument(\"--dilation-growth-rate\", type=int, default=3, help=\"dilation growth rate\")\n",
    "parser.add_argument(\"--output-emb-width\", type=int, default=512, help=\"output embedding width\")\n",
    "parser.add_argument('--vq-act', type=str, default='relu', choices = ['relu', 'silu', 'gelu'], help='dataset directory')\n",
    "parser.add_argument('--vq-norm', type=str, default=None, help='dataset directory')\n",
    "\n",
    "## quantizer\n",
    "parser.add_argument(\"--quantizer\", type=str, default='ema_reset', choices = ['ema', 'orig', 'ema_reset', 'reset'], help=\"eps for optimal transport\")\n",
    "parser.add_argument('--beta', type=float, default=1.0, help='commitment loss in standard VQ')\n",
    "\n",
    "## resume\n",
    "parser.add_argument(\"--resume-pth\", type=str, default=None, help='resume pth for VQ')\n",
    "parser.add_argument(\"--resume-gpt\", type=str, default=None, help='resume pth for GPT')\n",
    "\n",
    "\n",
    "## output directory \n",
    "parser.add_argument('--out-dir', type=str, default='output_vqfinal/', help='output directory')\n",
    "parser.add_argument('--results-dir', type=str, default='visual_results/', help='output directory')\n",
    "parser.add_argument('--visual-name', type=str, default='baseline', help='output directory')\n",
    "parser.add_argument('--exp-name', type=str, default='exp_debug', help='name of the experiment, will create a file inside out-dir')\n",
    "## other\n",
    "parser.add_argument('--print-iter', default=200, type=int, help='print frequency')\n",
    "parser.add_argument('--eval-iter', default=1000, type=int, help='evaluation frequency')\n",
    "parser.add_argument('--seed', default=123, type=int, help='seed for initializing training.')\n",
    "\n",
    "parser.add_argument('--vis-gt', action='store_true', help='whether visualize GT motions')\n",
    "parser.add_argument('--nb-vis', default=20, type=int, help='nb of visualizations')\n",
    "\n",
    "\n",
    "args=parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37fb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12977d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vqvae.HumanVQVAE(args, ## use args to define different parameters in different quantizers\n",
    "                       args.nb_code,\n",
    "                       args.code_dim,\n",
    "                       args.output_emb_width,\n",
    "                       args.down_t,\n",
    "                       args.stride_t,\n",
    "                       args.width,\n",
    "                       args.depth,\n",
    "                       args.dilation_growth_rate,\n",
    "                       args.vq_act,\n",
    "                       args.vq_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3ab72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./pretrained/VQVAE/net_last.pth\", map_location='cpu')\n",
    "net.load_state_dict(ckpt['net'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da8a5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4429148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,512,(1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b22e3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net.forward_decoder(indices.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28ff9a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480, 263])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0047f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out[0:1].detach().cpu()), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ea85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b3f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b96371f",
   "metadata": {},
   "source": [
    "## Motion trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1024828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad717183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b516c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc0b02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_paths =  glob(\"/srv/scratch/sanisetty3/music_motion/AIST/music/*.npy\" )\n",
    "msc = np.load(music_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2d8f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 128)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca80d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 96, 151, 239, 379, 600]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenss = list(np.array(np.logspace(np.log(60), np.log(600), 6, base=np.exp(1)) + 1 , dtype = np.uint))\n",
    "lenss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40c7750b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200000, 220000, 240000, 260000, 280000, 300000]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_steps = list(np.linspace(200000,300000,6 , dtype = np.uint))\n",
    "stage_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70fdfc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage = np.searchsorted(stage_steps , 60000) - 1\n",
    "stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd02de94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenss[stage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f037a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,100000,40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5bf991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,1,10), np.logspace(np.log(60), np.log(200), 10, base=np.exp(1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ee6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg\n",
    "model = VQMotionModel(cfg.vqvae).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb613de",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 220000\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/checkpoints/vqvae_motion.{step}.pt\", map_location = 'cpu')\n",
    "model.load_state_dict(pkg[\"model\"])\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d13a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 151.57M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8971f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.motionEncoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.motionDecoder.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe92e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training params: 0.00M\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total training params: %.2fM\" % (total / 1e6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e738563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:02<00:00, 1572.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 3980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"test\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "collate_fn = MotionCollator()\n",
    "train_loader = DATALoader(train_ds,\n",
    "                                                1,\n",
    "                                                #sampler=sampler,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fbade46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 263])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7bbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.loss import ReConsLoss\n",
    "\n",
    "loss_fnc = ReConsLoss(\"l1_smooth\", 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2782b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3980/3980 [01:38<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052648645 0.047441807\n"
     ]
    }
   ],
   "source": [
    "losses =[]\n",
    "losses_vel = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        gt_motion = batch[\"motion\"].cuda()\n",
    "        pred_motion , indices, commit_loss = model(gt_motion)\n",
    "        loss_motion = loss_fnc(pred_motion, gt_motion).detach().cpu()\n",
    "        loss_vel = loss_fnc.forward_vel(pred_motion, gt_motion).detach().cpu()\n",
    "#         loss = loss_motion.detach().cpu() + 0.02 * commit_loss + 0.5 * loss_vel.detach().cpu()\n",
    "        losses.append(loss_motion)\n",
    "        losses_vel.append(loss_vel)\n",
    "    \n",
    "print(np.mean(losses) , np.mean(losses_vel))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6a654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\", map_location = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8482b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optim', 'steps', 'total_loss'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f97ed03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200000.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkg[\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65967432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc05821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQVarLenMotionDatasetConditional,MotionCollatorConditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "25593722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1904/1904 [00:02<00:00, 786.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = VQVarLenMotionDatasetConditional(\"aist\", data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\", split = \"train\", max_length_seconds = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f49e221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionCollator = MotionCollatorConditional(dataset_name = \"aist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "40ffd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DATALoader(ds , batch_size = 4, shuffle = False,collate_fn=motionCollator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "17bb1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 228\n"
     ]
    }
   ],
   "source": [
    "dl.dataset.set_stage(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ccb960bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9624e4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['motion', 'motion_lengths', 'motion_mask', 'names', 'condition'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5156b079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 179, 128])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"condition\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c46101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 179, 263])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6a487503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([132., 106., 179., 140.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "85fadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[\"motion\"]\n",
    "inp, target = x[:, :-1], x[:, 1:]\n",
    "seq = x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7257aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.randn(inp.shape, device = x.device)\n",
    "rand[:, 0] = -torch.finfo(rand.dtype).max # first token should not be masked out\n",
    "num_mask = min(int(seq * 0.2), seq - 1)\n",
    "indices = rand.topk(num_mask, dim = -1).indices\n",
    "mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0d617012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 178])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0801dbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 502, 100])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mask = min(int(seq * 0.2), seq - 1)\n",
    "indices = rand.topk(num_mask, dim = -1).indices\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3f5c5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Autoregresive decoder\n",
    "##x mask context c_mask \n",
    "##Causal\n",
    "##n_tokens = 1024+2\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, Decoder, exists, default, always,ScaledSinusoidalEmbedding,AbsolutePositionalEmbedding, l2norm\n",
    "from x_transformers import AutoregressiveWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aab3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerWrapper(\n",
    "    **dec_transformer_kwargs,\n",
    "    attn_layers = Decoder(dim = dim, cross_attend = True, **dec_kwargs)\n",
    ")\n",
    "\n",
    "\n",
    "decoder = AutoregressiveWrapper(decoder, ignore_index=ignore_index, pad_value=pad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b71442c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MotionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_tokens,\n",
    "        max_seq_len,\n",
    "        attn_layers,\n",
    "        emb_dim = None,\n",
    "        cond_dim = None,\n",
    "        inp_dim = None,\n",
    "        token_emb_type = \"linear\",\n",
    "        max_mem_len = 0.,\n",
    "        shift_mem_down = 0,\n",
    "        emb_dropout = 0.,\n",
    "        post_emb_norm = False,\n",
    "        num_memory_tokens = None,\n",
    "        tie_embedding = False,\n",
    "        logits_dim = None,\n",
    "        use_abs_pos_emb = True,\n",
    "        scaled_sinu_pos_emb = False,\n",
    "        l2norm_embed = False,\n",
    "        emb_frac_gradient = 1. # GLM-130B and Cogview successfully used this, set at 0.1\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "        emb_dim = default(emb_dim, dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.cond_emb_dim = cond_dim\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "\n",
    "\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        \n",
    "        if token_emb_type == \"linear\":\n",
    "            self.token_emb = LinearEmbedding(inp_dim, emb_dim, l2norm_embed = l2norm_embed)\n",
    "        else:\n",
    "            self.token_emb = TokenEmbedding2(emb_dim, num_tokens, l2norm_embed = l2norm_embed)\n",
    "\n",
    "        if not (use_abs_pos_emb and not attn_layers.has_pos_emb):\n",
    "            self.pos_emb = always(0)\n",
    "        elif scaled_sinu_pos_emb:\n",
    "            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n",
    "        else:\n",
    "            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len, l2norm_embed = l2norm_embed)\n",
    "\n",
    "        self.emb_frac_gradient = emb_frac_gradient # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290\n",
    "\n",
    "        self.post_emb_norm = nn.LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n",
    "        \n",
    "        if exists(cond_dim):\n",
    "            self.project_cond_emb = nn.Linear(self.cond_emb_dim, dim) if self.cond_emb_dim != dim else nn.Identity()\n",
    "        \n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.init_()\n",
    "        logits_dim = default(logits_dim, num_tokens)\n",
    "        self.logits_dim = logits_dim\n",
    "        if exists(logits_dim):\n",
    "            self.to_logits = nn.Linear(dim, logits_dim)\n",
    "\n",
    "\n",
    "    def init_(self):\n",
    "        if self.l2norm_embed:\n",
    "            nn.init.normal_(self.token_emb.emb.weight, std = 1e-5)\n",
    "            if not isinstance(self.pos_emb, always):\n",
    "                nn.init.normal_(self.pos_emb.emb.weight, std = 1e-5)\n",
    "            return\n",
    "\n",
    "        nn.init.kaiming_normal_(self.token_emb.emb.weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context = None,\n",
    "        mask = None,\n",
    "        context_mask = None,\n",
    "        attn_mask = None,\n",
    "        self_attn_context_mask = None,\n",
    "        \n",
    "        return_embeddings = False,\n",
    "        return_logits_and_embeddings = False,\n",
    "        return_intermediates = False,\n",
    "        return_attn = False,\n",
    "        pos = None,\n",
    "        prepend_embeds = None,\n",
    "        sum_embeds = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if len(x.shape)>2:\n",
    "            b, n,d, device, emb_frac_gradient = *x.shape, x.device, self.emb_frac_gradient\n",
    "        else:\n",
    "            b, n, device, emb_frac_gradient = *x.shape, x.device, self.emb_frac_gradient\n",
    "        return_hiddens = return_attn\n",
    "\n",
    "        if not exists(self.logits_dim):\n",
    "            return_embeddings = True\n",
    "\n",
    "        # absolute positional embedding\n",
    "\n",
    "        external_pos_emb = exists(pos) and pos.dtype != torch.long\n",
    "        pos_emb = self.pos_emb(x, pos = pos) if not external_pos_emb else pos\n",
    "        x = self.token_emb(x) + pos_emb\n",
    "\n",
    "        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training\n",
    "\n",
    "        if exists(sum_embeds):\n",
    "            x = x + sum_embeds\n",
    "\n",
    "        # post embedding norm, purportedly leads to greater stabilization\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "\n",
    "        # whether to append embeds, as in PaLI, for image embeddings\n",
    "\n",
    "        if exists(prepend_embeds):\n",
    "            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n",
    "            assert prepend_dim == x.shape[-1], 'prepended embeddings need to have same dimensions as model dimensions'\n",
    "\n",
    "            x = torch.cat((prepend_embeds, x), dim = -2)\n",
    "\n",
    "        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n",
    "\n",
    "        if emb_frac_gradient < 1:\n",
    "            assert emb_frac_gradient > 0\n",
    "            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n",
    "\n",
    "        # embedding dropout\n",
    "\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x = self.project_emb(x)\n",
    "        \n",
    "        if exists(context):\n",
    "            context = self.project_cond_emb(context)\n",
    "            \n",
    "        print(x.shape,context.shape , mask.shape , context_mask.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        if return_hiddens:\n",
    "            x, intermediates = self.attn_layers(x,context, mask,context_mask,attn_mask,self_attn_context_mask, return_hiddens = True, **kwargs)\n",
    "        else:\n",
    "            x = self.attn_layers(x,context, mask,context_mask,attn_mask,self_attn_context_mask, **kwargs)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_logits_and_embeddings:\n",
    "            out = (self.to_logits(x), x)\n",
    "        elif return_embeddings:\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.to_logits(x)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return out, intermediates\n",
    "\n",
    "        if return_attn:\n",
    "            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n",
    "            return out, attn_maps\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c3768a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim,dim, l2norm_embed = False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Linear(input_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear_emb = self.emb(x)\n",
    "        return l2norm(linear_emb) if self.l2norm_embed else linear_emb\n",
    "\n",
    "class TokenEmbedding2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed = False , pad_idx = 0):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim , padding_idx = pad_idx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "019fde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "motionDecoder = MotionTransformer(\n",
    "        max_seq_len = 1200,\n",
    "        num_tokens = 1026,\n",
    "        cond_dim  =128,\n",
    "        scaled_sinu_pos_emb = True,\n",
    "        token_emb_type = \"token\",\n",
    "        attn_layers = Decoder(\n",
    "            cross_attend = True,\n",
    "            dim = 768,\n",
    "            depth = 12,\n",
    "            heads = 8,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e921caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AutoregressiveWrapper(motionDecoder, pad_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "acb738ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.as_tensor(abs(batch[\"motion\"][:,:,0]*10 - 10 + 1) , dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0d01ab09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  7,  8,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 13,\n",
       "        14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 11, 11, 11, 10,\n",
       "        10,  9,  9,  9,  8,  8,  8,  7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  7,  7,  7,  7,  7,  7,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,\n",
       "         5,  5,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  5,  5,  5,  5,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "75896dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0 , 1026 , (4,179))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9d3908a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = x[:, :-1], x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f32f634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(inp[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c33c7aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(batch[\"motion_mask\"][0][:-1] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.randn(inp.shape, device = x.device)\n",
    "rand[:, 0] = -torch.finfo(rand.dtype).max # first token should not be masked out\n",
    "num_mask = min(int(seq * 0.2), seq - 1)\n",
    "indices = rand.topk(num_mask, dim = -1).indices\n",
    "mask = ~torch.zeros_like(inp).scatter(1, indices, 1.).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070b3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ddbc096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb = TokenEmbedding2(768 , 1026 , pad_idx = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0b2df56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 179, 768])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5659dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 178, 768]) torch.Size([4, 178, 768]) torch.Size([4, 178]) torch.Size([4, 178])\n"
     ]
    }
   ],
   "source": [
    "out = motionDecoder(x = inp, mask = batch[\"motion_mask\"][:,:-1] , context = batch[\"condition\"][:,:-1] , context_mask = batch[\"motion_mask\"][:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bf74f664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 178, 1026])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193acff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8bbd48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e09e3c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1026, 178])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fd8ed9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 178])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cecb5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(\n",
    "            out.reshape(-1,1026),\n",
    "            target.reshape(-1),\n",
    "            ignore_index = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7e77396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.1186, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44467ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
