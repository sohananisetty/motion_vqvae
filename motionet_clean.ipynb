{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f73016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f35de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e7dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf00bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, mean ,std , j = 22):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*std+mean, j)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, j, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7219ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = MotionRegressorModel(args = cfg.motion_trans , ignore_index=1025 ,pad_value=1025 ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbdb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model = VQMotionModel(cfg.vqvae).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d22054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([295000.])\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = VQMotionModel(cfg.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f8a57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c2e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1169.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "train_loader = DATALoader(train_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1dadd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 400 - 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 2111.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQVarLenMotionDataset(\"aist\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , num_stages = 6 ,min_length_seconds=20, max_length_seconds=30)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d49cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 400 - 600\n"
     ]
    }
   ],
   "source": [
    "aist_loader.dataset.set_stage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7bf4d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 191, 263])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for aist_batch in aist_loader:\n",
    "    break\n",
    "aist_batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gBR_sFM_cAll_d06_mBR3_ch17 589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff88bbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gBR_sFM_cAll_d06_mBR3_ch17'], dtype='<U26')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aist_batch[\"names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a37e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c5581",
   "metadata": {},
   "source": [
    "## Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bfd2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 191])\n"
     ]
    }
   ],
   "source": [
    "ind = vqvae_model.encode(aist_batch[\"motion\"].cuda())\n",
    "print(ind.shape)\n",
    "quant , out_motion = vqvae_model.decode(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3436703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.empty(aist_batch[\"motion\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07aca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20b36c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = vqvae_model.encode(aist_batch[\"motion\"][:,:400].cuda())\n",
    "quant , out_motion = vqvae_model.decode(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da2bbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = vqvae_model.decode(ind[:,400:].to(torch.long).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5c0012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:,400:] = out_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9cbf08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(aist_batch[\"motion\"][0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50ae294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out[0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_motion_ind_400\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3af4681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,1024,(1,60))\n",
    "quant , out_motion = vqvae_model.decode(indices.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19fc3133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ee420",
   "metadata": {},
   "source": [
    "## Music Eval stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbd89ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "from aist_features import calculate_fid_scores\n",
    "from aist_features.calculate_fid_scores import calculate_avg_distance, extract_feature,calculate_frechet_feature_distance,calculate_frechet_distance\n",
    "from aist_features.features import kinetic,manual\n",
    "from aist_features.calculate_beat_scores import motion_peak_onehot,alignment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1978b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43a14e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_music import evaluate_music_motion_vqvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80f473e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch[\"motion_lengths\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f15365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:01<00:00, 1282.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = 400)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7ed828ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1910 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for aist_batch in tqdm(aist_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f3ca320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [46:44<00:00,  1.47s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  0.028328037164357056 Diversity_k: 9.260323240545652\n",
      "FID_g:  1.2220429949585352 Diversity_g: 7.300148475695237\n",
      "\n",
      "Beat score on real data: 0.250\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.246\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.028328037164357056, 1.2220429949585352, 100, 100, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_music_motion_vqvae(aist_loader,vqvae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09025d",
   "metadata": {},
   "source": [
    "## Generating token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e74d7211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 1513.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb206fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 14.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(aist_loader):\n",
    "    \n",
    "    n = int(batch[\"motion_lengths\"])\n",
    "    name = str(batch[\"names\"][0])\n",
    "    if n< 400:\n",
    "        ind = vqvae_model.encode(batch[\"motion\"].cuda())\n",
    "    else:\n",
    "        ind = vqvae_model.encode(batch[\"motion\"][:,:400].cuda())\n",
    "    \n",
    "    np.save(os.path.join(\"/srv/scratch/sanisetty3/music_motion/AIST/joint_indices_max_400\" , name+\".npy\"),ind.cpu().numpy()[0])\n",
    "        \n",
    "#         quant , out_motion = vqvae_model.decode(ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c25d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7357972d",
   "metadata": {},
   "source": [
    "## Motion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ad2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional,VQVarLenMotionDatasetConditional, TransMotionDatasetConditional\n",
    "cfg = get_cfg_defaults()\n",
    "cfg.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdeaae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg2 = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\", map_location = 'cpu')\n",
    "# print(pkg2[\"steps\"])\n",
    "# vqvae_model_prev.load_state_dict(pkg2[\"model\"])\n",
    "# vqvae_model_prev =vqvae_model_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe738a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae95881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "trans_model = MotionRegressorModel(args = cfg.motion_trans , ignore_index=1025 ,pad_value=1025 ).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0f6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg3 = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/const_len/\", map_location = 'cpu')\n",
    "# trans_model.load_state_dict(pkg3[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b4f77a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([295000.])\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = VQMotionModel(cfg.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "681e653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 1076.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = TransMotionDatasetConditional(\"aist\", split = \"render\",data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , datafolder=\"joint_indices_max_400\", window_size = 400,force_len=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd3859eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn2 = MotionCollatorConditional(dataset_name = \"aist\" , bos = 1024, pad = 1025, eos = 1026)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "652cc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DATALoader(train_ds , batch_size = 1,collate_fn=collate_fn2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df7e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eedcade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg_batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3b1da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion torch.Size([1, 240])\n",
      "motion_lengths torch.Size([1])\n",
      "motion_mask torch.Size([1, 240])\n",
      "names (1,)\n",
      "condition torch.Size([1, 239, 128])\n",
      "condition_mask torch.Size([1, 239])\n"
     ]
    }
   ],
   "source": [
    "for k,v in reg_batch.items():\n",
    "    print(k , v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dec4a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, target = reg_batch[\"motion\"][:, :-1], reg_batch[\"motion\"][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1fde6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = vqvae_model.decode(target[target<1024][None,...].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dedb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu(),mean = train_ds.mean , std = train_ds.std), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12002216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[234, 587, 137,  19, 115, 182, 117,  29,   0, 113]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e89d0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "gen_motion_indices = trans_model.generate(start_tokens = inp[:,:1], seq_len=100 , context = reg_batch[\"condition\"], context_mask = reg_batch[\"condition_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "143a963a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 234,  851,  496,  413,  997,  505,  681,  796,  585,  158,  413,  264,\n",
       "          250,  850,  524,  244,  157,   74,  270,  844,   72,  817,  468,  740,\n",
       "          851,  806,  608,  263,  924,  740,  845,  924,  608,  960,  314,  700,\n",
       "          352,  804,  157,   82,  846,   40,  616,   97,  967,  650,  162,  997,\n",
       "          740,  314,  851,  616,  180,  966,  250,  314,  642,  960,   52,  779,\n",
       "          393,  665,  496,  772,  264,  270,  426,  524,  920,  608,  817,  772,\n",
       "          806,  208,  997,  585,  214,  287,  560,  817,  844,  112,  833,   72,\n",
       "          985,  848,  800,  266,  264,  266,  814,  985,  740,  180,  496,  985,\n",
       "          112, 1022,  353,   97,   97]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_motion_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "958392d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = trans_model(motion = inp , mask = reg_batch[\"motion_mask\"][:,:-1]  , \\\n",
    "        context = batch[\"condition\"], context_mask = reg_batch[\"condition_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae50ae4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 213, 1027])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3d60039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = torch.nn.CrossEntropyLoss(ignore_index=1025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "603a84e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([213.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reg_batch[\"motion_lengths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba8570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fc0f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits[0][:int(reg_batch[\"motion_lengths\"][0])], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1c5ed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([213, 1027])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b6f9f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ ,indx =torch.max(probs, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff369653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(indx == target[0][:int(reg_batch[\"motion_lengths\"][0])].flatten(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ea9acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.Categorical(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e345c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09a7cb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([614, 672,  90, 735, 735,  40, 107, 349, 349,  51, 786, 258, 173, 246,\n",
       "         12, 216, 463, 303, 114, 114, 140, 593, 905, 278, 278, 358, 212, 140,\n",
       "        216, 252, 472, 472, 593, 675, 675,  51, 472, 553, 135, 221, 216, 205,\n",
       "        252, 463, 419, 586, 114,  41,  58, 104, 612, 903, 303, 246, 540, 205,\n",
       "        252, 634,  40,  59, 240, 240,  51, 258, 258, 757, 840,  12, 840, 252,\n",
       "        463, 485, 114, 840, 593, 757, 612, 903, 405, 787, 212, 205, 430, 485,\n",
       "        699, 886,  51, 349, 349,  59, 472, 553, 135, 409, 882, 205, 303, 463,\n",
       "        787, 988, 114, 268, 258, 877, 612, 405, 303, 429, 540, 485, 485, 603,\n",
       "        798,  59, 240,  51, 634, 798, 757, 634, 246, 391, 840, 252, 252, 634,\n",
       "        840, 840, 593, 877, 612, 903, 463, 212, 140, 205, 485, 485,  10, 593,\n",
       "        240, 240, 240, 472, 553, 135, 421, 882, 429, 429, 252, 813, 840,   4,\n",
       "        111, 586, 104, 405, 405, 114, 136, 121, 840, 485, 593, 798,  51, 349,\n",
       "        240,  59, 798, 786, 258, 315, 391,  12, 485, 303,  41, 840, 421, 593,\n",
       "        757, 612, 903, 405, 114, 140, 429, 485, 252, 617, 593,  59, 349, 675,\n",
       "         51, 214,  45, 553, 472, 593, 444, 904, 485, 813, 135, 175, 205, 823,\n",
       "         38, 564])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][:int(reg_batch[\"motion_lengths\"][0])][target[0][:int(reg_batch[\"motion_lengths\"][0])] < 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aefbb81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 568,   79, 1006,  518,  571,  592,  956,  880,  978,   89,  465,  712,\n",
       "         858,  314,  136,  134,  429,  383,  312,  142,  475,  684,  786,  434,\n",
       "         484,  259,  169,  635,  281,   98,   13,  680,  793,  465,  984,  750,\n",
       "         812,   55,  346,  356,  864,  988,  825,  845,  714,   39,  901,  747,\n",
       "         537,  587, 1004,  827,  274,  632, 1008,  592,  257,  494,  113,  456,\n",
       "         605,  184,  188, 1005,  984,  484,  663,  576,  221,  537,  690,  366,\n",
       "         916,  314,   94,  156,  237,   99,  439,  370,  817,  483,  958,  306,\n",
       "         276,  476,  654,  367,  350,   25,  175,  362,   40,  873,  283,  610,\n",
       "         575,  967,  586,  357,  934,  969,  199,  234,   36,   65,  752,   45,\n",
       "        1007,  608, 1026,  632,  766,  273,  579,  847,  883,  778,  779,  268,\n",
       "        1014,  964,   89,  642,  160,   36,  645,  908,  313,  311,  942,  265,\n",
       "         498,  846,  561,   51,  294,  264,   27,  196,  617,  365,  539,  353,\n",
       "         357,  724,  424,  267,  390,  702,  625,  354,  599,  456,  952,  969,\n",
       "        1018,   27,  540,  611,  227,  879,  523,  654,  448,  113,  865,  394,\n",
       "         133,   65,  596,  415,  328,  814,  192,  335,  719,  413,  490,  312,\n",
       "         989,  391,  284,  700,  311,  880,  300,  206,  100,  525,   82,  918,\n",
       "         895,   91,  332,  943,  945,  313,  669,  924,  631,  288,  687,  826,\n",
       "         487,  810,  750,  929,  375,  552,  938,   57,  116])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indx = dist.sample()\n",
    "pred_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ae4c934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 568,   79, 1006,  518,  571,  592,  956,  880,  978,   89,  465,  712,\n",
       "         858,  314,  136,  134,  429,  383,  312,  142,  475,  684,  786,  434,\n",
       "         484,  259,  169,  635,  281,   98,   13,  680,  793,  465,  984,  750,\n",
       "         812,   55,  346,  356,  864,  988,  825,  845,  714,   39,  901,  747,\n",
       "         537,  587, 1004,  827,  274,  632, 1008,  592,  257,  494,  113,  456,\n",
       "         605,  184,  188, 1005,  984,  484,  663,  576,  221,  537,  690,  366,\n",
       "         916,  314,   94,  156,  237,   99,  439,  370,  817,  483,  958,  306,\n",
       "         276,  476,  654,  367,  350,   25,  175,  362,   40,  873,  283,  610,\n",
       "         575,  967,  586,  357,  934,  969,  199,  234,   36,   65,  752,   45,\n",
       "        1007,  608, 1026])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indx[:111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4e0dd6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_indx == 1025).nonzero().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3d154e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_indx == 1026).nonzero().flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5eeb3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(*(pred_indx == 1025).nonzero().flatten().tolist() , *(pred_indx == 1026).nonzero().flatten().tolist() , 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d6f6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c49315a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1910 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 22, 3) (158, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1910 [00:01<42:51,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 22, 3) (2, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/utils.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.linalg.norm(average_acceleration / current_window)\n",
      "\r",
      "  0%|          | 2/1910 [00:03<1:05:07,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 22, 3) (158, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1910 [00:05<54:48,  1.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 22, 3) (399, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1910 [00:08<1:18:44,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399, 22, 3) (399, 22, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1910 [00:11<1:28:19,  2.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ff103e5a31e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_music\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_music_motion_trans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_music_motion_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvqvae_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/eval_music.py\u001b[0m in \u001b[0;36mevaluate_music_motion_trans\u001b[0;34m(val_loader, net, trans, audio_feature_dir, best_fid_k, best_fid_g, best_div_k, best_div_g, best_beat_align)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mreal_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mreal_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mresult_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/aist_metrics/calculate_fid_scores.py\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(keypoints3d, mode)\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_kinetic_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m       \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_manual_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not support!\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/manual.py\u001b[0m in \u001b[0;36mextract_manual_features\u001b[0;34m(positions)\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m         pose_features.append(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_nmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neck\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lhip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rhip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lwrist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m         pose_features.append(\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/manual.py\u001b[0m in \u001b[0;36mf_nmove\u001b[0;34m(self, j1, j2, j3, j4, range)\u001b[0m\n\u001b[1;32m    216\u001b[0m         ]\n\u001b[1;32m    217\u001b[0m         return feat_utils.velocity_direction_above_threshold_normal(\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mj1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj1_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj4_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/utils.py\u001b[0m in \u001b[0;36mvelocity_direction_above_threshold_normal\u001b[0;34m(j1, j1_prev, j2, j3, p, p_prev, threshold, time_per_frame)\u001b[0m\n\u001b[1;32m     83\u001b[0m ):\n\u001b[1;32m     84\u001b[0m     velocity = (\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj1_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[0mj31\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.eval_music import evaluate_music_motion_trans\n",
    "evaluate_music_motion_trans(dl, vqvae_model, trans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d46b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2167c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb7198f9",
   "metadata": {},
   "source": [
    "## T2M Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4733deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from utils.eval_trans import evaluation_vqvae_loss,evaluation_vqvae\n",
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b7f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [00:04<00:00, 912.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "tm_eval = dataset_TM_eval.DATALoader(\"t2m\", True, 20, w_vectorizer, unit_length=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "602d3252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:42<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 0 :, FID. 0.0637, Diversity Real. 9.4620, Diversity. 9.4266, R_precision_real. [0.61616379 0.79482759 0.86982759], R_precision. [0.60172414 0.78405172 0.86077586], matching_score_real. 2.9635313979510602, matching_score_pred. 3.0240239735307366\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluation_vqvae_loss(val_loader = tm_eval, net= vqvae_model,nb_iter= 0, eval_wrapper = eval_wrapper,save = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad4eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281848d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844bcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ed4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
