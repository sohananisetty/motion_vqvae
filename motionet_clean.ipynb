{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcaaaca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d978f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1c9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from core.datasets.vqa_motion_dataset import VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n",
    "from einops import rearrange, reduce, pack, unpack\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120cfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "import visualize.plot_3d_global as plot_3d\n",
    "from glob import glob\n",
    "def to_xyz(motion, mean ,std , j = 22):\n",
    "    motion_xyz = recover_from_ric(motion.cpu().float()*std+mean, j)\n",
    "    motion_xyz = motion_xyz.reshape(motion.shape[0],-1, j, 3)\n",
    "    return motion_xyz\n",
    "\n",
    "            \n",
    "def sample_render(motion_xyz , name , save_path):\n",
    "    print(f\"render start\")\n",
    "    \n",
    "    gt_pose_vis = plot_3d.draw_to_batch(motion_xyz.numpy(),None, [os.path.join(save_path,name + \".gif\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12fb7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "75437290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([295000.])\n"
     ]
    }
   ],
   "source": [
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5556bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = MotionCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50424dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 60 - 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 37.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = VQVarLenMotionDataset(\"t2m\", split = \"render\" , max_length_seconds = 10, data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D\")\n",
    "train_loader = DATALoader(train_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc774b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 400 - 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 39.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQVarLenMotionDataset(\"aist\", split = \"render\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , num_stages = 6 ,min_length_seconds=20, max_length_seconds=30)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32fea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing range to: 400 - 600\n"
     ]
    }
   ],
   "source": [
    "aist_loader.dataset.set_stage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da01534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 159, 263])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for aist_batch in aist_loader:\n",
    "    break\n",
    "aist_batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99107113",
   "metadata": {},
   "source": [
    "## Trans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e491b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_trans = get_cfg_defaults()\n",
    "cfg_trans.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6ca3b38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([195000.])\n"
     ]
    }
   ],
   "source": [
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()\n",
    "pkg_trans = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist/trans_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg_trans[\"steps\"])\n",
    "trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58013d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b565e8",
   "metadata": {},
   "source": [
    "## Encode Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fb8e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 191])\n"
     ]
    }
   ],
   "source": [
    "ind = vqvae_model.encode(aist_batch[\"motion\"].cuda())\n",
    "print(ind.shape)\n",
    "quant , out_motion = vqvae_model.decode(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f010f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.empty(aist_batch[\"motion\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bf733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c188c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = vqvae_model.encode(aist_batch[\"motion\"][:,:400].cuda())\n",
    "quant , out_motion = vqvae_model.decode(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41c3dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant , out_motion = vqvae_model.decode(ind[:,400:].to(torch.long).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5411013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:,400:] = out_motion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d67a53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(aist_batch[\"motion\"][0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_og_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "241a5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out[0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_motion_ind_400\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ebcdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randint(0,1024,(1,60))\n",
    "quant , out_motion = vqvae_model.decode(indices.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90379db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f95c09",
   "metadata": {},
   "source": [
    "## Music Eval stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df396922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ad8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "from utils.aist_metrics import calculate_fid_scores\n",
    "from utils.aist_metrics.calculate_fid_scores import calculate_avg_distance, extract_feature,calculate_frechet_feature_distance,calculate_frechet_distance\n",
    "from utils.aist_metrics.features import kinetic,manual\n",
    "from utils.aist_metrics.calculate_beat_scores import motion_peak_onehot,alignment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088f2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.datasets.vqa_motion_dataset import MotionCollatorConditional, TransMotionDatasetConditional,VQMotionDataset,DATALoader,VQVarLenMotionDataset,MotionCollator,VQFullMotionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14fb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_music import evaluate_music_motion_vqvae, evaluate_music_motion_generative,evaluate_music_motion_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f24604a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for aist_batch in tqdm(aist_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b85a62",
   "metadata": {},
   "source": [
    "### Const len trained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c1c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([85000.])\n"
     ]
    }
   ],
   "source": [
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans , ignore_index=1025 ,pad_value=1025 ).eval()\n",
    "pkg_trans = torch.load(f\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/const_len/trans_768_768_aist/vqvae_motion.pt\", map_location = 'cpu')\n",
    "print(pkg_trans[\"steps\"])\n",
    "trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ea564d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2763786008230453"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.23/0.243 * 0.292"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365a3a3",
   "metadata": {},
   "source": [
    "## Evaluate Music Motion transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a9ea9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist/var_len_768_768_aist.yaml\"\n",
    "encodec_sine = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_sine_aist/var_len_768_768_sine_aist.yaml\"\n",
    "librosa = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_35/var_len_768_768_aist_35.yaml\"\n",
    "encodec_prob50 = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_mask_prob50/trans_768_768_albi_aist_mask_prob50.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c307d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed8a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "da714b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([195000.])\n"
     ]
    }
   ],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "trans_option = \"encodec\"\n",
    "\n",
    "cfg_trans = get_cfg_defaults()\n",
    "cfg_trans.merge_from_file(encodec)\n",
    "\n",
    "\n",
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()\n",
    "pkg_trans = torch.load(f\"{os.path.dirname(encodec)}/trans_motion_best_fid.pt\", map_location = 'cpu')\n",
    "print(pkg_trans[\"steps\"])\n",
    "trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dcda749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoding_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/music\"\n",
    "audio_features_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/audio_features/\"\n",
    "use35 = False\n",
    "if trans_option == \"librosa\":\n",
    "    audio_encoding_dir = audio_features_dir\n",
    "    use35 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40f82831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81752e",
   "metadata": {},
   "source": [
    "## Evaluate Music Motion Generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67202e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae814a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_music import evaluate_music_motion_vqvae, evaluate_music_motion_generative,evaluate_music_motion_generative2,evaluate_music_motion_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b667473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1700.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"test\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e41c4914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/400 [00:00<00:06, 64.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:06<00:00, 60.42it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.73it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.44it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.62it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.56it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.68it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.08it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 59.90it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.57it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.68it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.56it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.67it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.68it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.73it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.83it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.69it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.64it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.73it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.77it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.50it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.84it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.92it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.72it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 59.70it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.90it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.70it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.62it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.64it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.86it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.47it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.65it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.47it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.52it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.48it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.95it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.58it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.90it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.55it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 60.52it/s]\n",
      "100%|██████████| 400/400 [00:06<00:00, 59.43it/s]\n",
      "100%|██████████| 40/40 [05:09<00:00,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  6.403103264620739 Diversity_k: 10.598105856088491\n",
      "FID_g:  11.891712829203748 Diversity_g: 7.263086085441785\n",
      "\n",
      "Beat score on real data: 0.244\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.212\n",
      "\n",
      "\\PFC score on real data: 2.113\n",
      "\n",
      "\\PFC score on generated data: 2.808\n",
      "\n",
      "best_fid_k 6.403103264620739\n",
      "best_fid_g 11.891712829203748\n",
      "best_div_k 10.598105856088491\n",
      "best_div_g 7.263086085441785\n",
      "best_beat_align 0.24413006020223502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.eval_music import evaluate_music_motion_generative\n",
    "print(\"pretrained mix\")\n",
    "best_fid_k = []\n",
    "best_fid_g = []\n",
    "best_div_k = []\n",
    "best_div_g = []\n",
    "best_beat_align = []\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    a,b,c,d,e = evaluate_music_motion_generative(aist_loader , vqvae_model= vqvae_model ,net = trans_model,use35=use35)\n",
    "    best_fid_k.append(a)\n",
    "    best_fid_g.append(b)\n",
    "    best_div_k.append(c)\n",
    "    best_div_g.append(d)\n",
    "    best_beat_align.append(e)\n",
    "\n",
    "    \n",
    "print(\"best_fid_k\" , np.mean(best_fid_k))\n",
    "print(\"best_fid_g\" , np.mean(best_fid_g))\n",
    "print(\"best_div_k\" , np.mean(best_div_k))\n",
    "print(\"best_div_g\" , np.mean(best_div_g))\n",
    "print(\"best_beat_align\" , np.mean(best_beat_align))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa18a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "FID_k:  5.848359006888529 Diversity_k: 9.930449794347469\n",
    "FID_g:  10.19790933574938 Diversity_g: 7.301737963236295\n",
    "\n",
    "Beat score on real data: 0.244\n",
    "\n",
    "\n",
    "Beat score on generated data: 0.233\n",
    "\n",
    "\\PFC score on real data: 2.113\n",
    "\n",
    "\\PFC score on generated data: 1.169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c34132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:01<00:00, 1568.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds_train = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader_train = DATALoader(aist_ds_train,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbdf1438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/900 [00:00<00:14, 62.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:20<00:00, 42.96it/s]\n",
      "100%|██████████| 900/900 [00:20<00:00, 43.03it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.70it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.74it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.69it/s]it]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.81it/s]it]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.37it/s]]  \n",
      "100%|██████████| 900/900 [00:21<00:00, 42.73it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.74it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.77it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.46it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.69it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.79it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.13it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.60it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.56it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.75it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.85it/s]]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.64it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.51it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.42it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.66it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.72it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.61it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.67it/s]s]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.61it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.68it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.33it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.43it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.21it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.10it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.07it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.13it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.28it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.59it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.57it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.56it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.52it/s]t]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.70it/s]s]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.78it/s]s]\n",
      "100%|██████████| 900/900 [00:21<00:00, 42.53it/s]t]\n",
      " 95%|█████████▌| 1818/1910 [18:01<00:54,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  6.394353653410235 Diversity_k: 10.558077923262992\n",
      "FID_g:  8.632308692625102 Diversity_g: 7.294414699949869\n",
      "\\PFC score on real data: 1.677\n",
      "\n",
      "\\PFC score on generated data: 1.239\n",
      "\n",
      "\n",
      "Beat score on real data: 0.170\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.164\n",
      "\n",
      "best_fid_k 6.394353653410235\n",
      "best_fid_g 8.632308692625102\n",
      "best_div_k 10.558077923262992\n",
      "best_div_g 7.294414699949869\n",
      "best_beat_align 0.17011690074139812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"pretrained mix\")\n",
    "best_fid_k = []\n",
    "best_fid_g = []\n",
    "best_div_k = []\n",
    "best_div_g = []\n",
    "best_beat_align = []\n",
    "seq_len = 900\n",
    "for i in range(1):\n",
    "\n",
    "    a,b,c,d,e,f,g = evaluate_music_motion_generative2(aist_loader_train , vqvae_model= vqvae_model ,net = trans_model, use35 = use35,seq_len = seq_len)\n",
    "    best_fid_k.append(a)\n",
    "    best_fid_g.append(b)\n",
    "    best_div_k.append(c)\n",
    "    best_div_g.append(d)\n",
    "    best_beat_align.append(e)\n",
    "\n",
    "    \n",
    "print(\"best_fid_k\" , np.mean(best_fid_k))\n",
    "print(\"best_fid_g\" , np.mean(best_fid_g))\n",
    "print(\"best_div_k\" , np.mean(best_div_k))\n",
    "print(\"best_div_g\" , np.mean(best_div_g))\n",
    "print(\"best_beat_align\" , np.mean(best_beat_align))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eadc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972fa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0744c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7536746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee9777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f9d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b00d0af",
   "metadata": {},
   "source": [
    "## AICHOREO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b24e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.motion_process import recover_from_ric\n",
    "from utils.aist_metrics.calculate_fid_scores import calculate_avg_distance, extract_feature,calculate_frechet_feature_distance,calculate_frechet_distance\n",
    "from utils.aist_metrics.features import kinetic,manual\n",
    "from utils.aist_metrics.calculate_beat_scores import motion_peak_onehot,alignment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a8b38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [01:47<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  7.528682831845714 Diversity_k: 10.279027991012589\n",
      "FID_g:  7.848392292979781 Diversity_g: 7.421904608008726\n",
      "\n",
      "Beat score on real data: 0.310\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_fid_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-f79ffd4f6e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mbest_fid_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFID_k\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFID_k\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_fid_k\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_fid_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mbest_fid_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFID_g\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFID_g\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_fid_g\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_fid_g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mbest_div_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDist_k\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDist_k\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_div_k\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_div_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_fid_k' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_features = {\"kinetic\": [], \"manual\": []}\n",
    "real_features = {\"kinetic\": [], \"manual\": []}\n",
    "\n",
    "mean = aist_loader.dataset.mean\n",
    "std = aist_loader.dataset.std\n",
    "\n",
    "beat_scores_real = []\n",
    "beat_scores_pred = []\n",
    "\n",
    "real_pfc = []\n",
    "pred_pfc = []\n",
    "\n",
    "seq_len =800\n",
    "\n",
    "audio_dir = audio_encoding_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/music\"\n",
    "audio_feature_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/audio_features/\"\n",
    "\n",
    "\n",
    "# smpl_motions_aist = glob(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/*.npy\")\n",
    "# for i in glob(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/hml/*.npy\"):\n",
    "#     smpl_motion = np.load(i)[120: , 6:]\n",
    "# #     print(smpl_motion.shape)\n",
    "#     seq_name = os.path.basename(i).split(\".\")[0]\n",
    "#     smpl_motions_aist.append({\"motion\":smpl_motion , \"name\" : seq_name })\n",
    "\n",
    "\n",
    "\n",
    "for i,path in enumerate(tqdm(smpl_motions_aist)):\n",
    "    \n",
    "    motion_name = os.path.basename(path)\n",
    "    \n",
    "    gt_motion = torch.Tensor(np.load(f\"/srv/scratch/sanisetty3/music_motion/AIST/new_joint_vecs/{motion_name[:-9]}.npy\")[None,...])\n",
    "#     print(gt_motion.shape)\n",
    "    out_motion = torch.Tensor(np.load(f\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/hml/{motion_name}\")[None,...])\n",
    "#     print(out_motion.shape)\n",
    "\n",
    "    keypoints3d_gt = recover_from_ric(gt_motion[0,:seq_len] , 22).detach().cpu().numpy()\n",
    "    keypoints3d_pred = recover_from_ric(out_motion[0,:seq_len] , 22).detach().cpu().numpy()\n",
    "\n",
    "    real_features[\"kinetic\"].append(extract_feature(keypoints3d_gt, \"kinetic\"))\n",
    "    real_features[\"manual\"].append(extract_feature(keypoints3d_gt, \"manual\"))\n",
    "\n",
    "    result_features[\"kinetic\"].append(extract_feature(keypoints3d_pred, \"kinetic\"))\n",
    "    result_features[\"manual\"].append(extract_feature(keypoints3d_pred, \"manual\"))\n",
    "\n",
    "#     real_pfc.append(calc_physical_score(keypoints3d_gt))\n",
    "#     pred_pfc.append(calc_physical_score(keypoints3d_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    motion_beats = motion_peak_onehot(keypoints3d_gt[:seq_len])\n",
    "    # get real data music beats\n",
    "    audio_name = motion_name.split(\"_\")[-3]\n",
    "\n",
    "    audio_feature = np.load(os.path.join(audio_feature_dir, f\"{audio_name}.npy\"))\n",
    "    audio_beats = audio_feature[:seq_len, -1] # last dim is the music beats\n",
    "    # get beat alignment scores\n",
    "    beat_score = alignment_score(audio_beats, motion_beats, sigma=1)\n",
    "    beat_scores_real.append(beat_score)\n",
    "\n",
    "\n",
    "    motion_beats = motion_peak_onehot(keypoints3d_pred[:seq_len])\n",
    "    beat_score_pred = alignment_score(audio_beats, motion_beats, sigma=1)\n",
    "    beat_scores_pred.append(beat_score_pred)\n",
    "\n",
    "\n",
    "FID_k, Dist_k = calculate_frechet_feature_distance(real_features[\"kinetic\"], result_features[\"kinetic\"])\n",
    "FID_g, Dist_g = calculate_frechet_feature_distance(real_features[\"manual\"], result_features[\"manual\"])\n",
    "\n",
    "\n",
    "print(\"FID_k: \",FID_k,\"Diversity_k:\",Dist_k)\n",
    "print(\"FID_g: \",FID_g,\"Diversity_g:\",Dist_g)\n",
    "\n",
    "# print (\"\\PFC score on real data: %.3f\\n\" % (np.mean(real_pfc)))\n",
    "# print (\"\\PFC score on generated data: %.3f\\n\" % (np.mean(pred_pfc)))\n",
    "\n",
    "\n",
    "print (\"\\nBeat score on real data: %.3f\\n\" % (np.mean(beat_scores_real)))\n",
    "print (\"\\nBeat score on generated data: %.3f\\n\" % (np.mean(beat_scores_pred)))\n",
    "\n",
    "\n",
    "\n",
    "best_fid_k = FID_k if FID_k < best_fid_k else best_fid_k\n",
    "best_fid_g = FID_g if FID_g < best_fid_g else best_fid_g\n",
    "best_div_k = Dist_k if Dist_k > best_div_k else best_div_k\n",
    "best_div_g = Dist_g if Dist_g > best_div_g else best_div_g\n",
    "\n",
    "best_beat_align = np.mean(beat_scores_real) if np.mean(beat_scores_real) > best_beat_align else best_beat_align \n",
    "\n",
    "\n",
    "\n",
    "# return best_fid_k, best_fid_g,best_div_k,best_div_g,best_beat_align , np.mean(real_pfc), np.mean(pred_pfc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05033cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf05ef6d",
   "metadata": {},
   "source": [
    "## Transformed HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cdeed768",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints3d_hml = np.load(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/keypoints3D_hml60.npy\").reshape(-1,22,3)\n",
    "keypoints3d_aist = np.load(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/keypoints3D_aist.npy\")[:52756,:22,:]\n",
    "\n",
    "from pycpd import RigidRegistration, DeformableRegistration\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4d955506",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RigidRegistration(X=keypoints3d_aist[:100].reshape(-1,3), Y=keypoints3d_hml[:100].reshape(-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "44188cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TY, (s_reg, R_reg, t_reg) = reg.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "40f10e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.646927100911472e-05"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(TY - keypoints3d_aist[:100].reshape(-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8c022950",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'points_to_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-56abaccc52c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_point_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoints_to_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'points_to_transform' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5d0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f12d8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1387.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"test\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "val_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a1f10c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in aist_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "16a7d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints3d_gt = recover_from_ric(batch[\"motion\"][0,:100] , 22).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2fcf3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/paper_renders/smpl/keypoints3d_gt.npy\" , reg.transform_point_cloud(Y=keypoints3d_gt.reshape(-1,3)).reshape(-1,22,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cdf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5d88a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:08<00:00, 45.42it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.73it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.42it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.57it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.31it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.58it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.29it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.51it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.41it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.47it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.03it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.45it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.43it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.43it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.43it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.43it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.32it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.39it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.39it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.44it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.38it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.40it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.36it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.37it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.30it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.43it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.33it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.37it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.38it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.29it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.41it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.39it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.37it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.47it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.41it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.44it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.47it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.49it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.48it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.33it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.26it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.38it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.37it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.45it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.39it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.46it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.40it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.48it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.47it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.35it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.40it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.33it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.44it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.29it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.39it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.38it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.41it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.40it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.34it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.32it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.47it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.46it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.42it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.34it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.42it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.36it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.24it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.38it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 45.47it/s]\n",
      "100%|██████████| 400/400 [00:10<00:00, 39.37it/s]\n",
      "100%|██████████| 40/40 [13:24<00:00, 20.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  6.9577679600177476 Diversity_k: 10.06756729009824\n",
      "FID_g:  11.167501952145543 Diversity_g: 7.1934799310488575\n",
      "\n",
      "Beat score on real data: 0.244\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_fid_k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-b2fbd4f6fb57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBeat score on generated data: %.3f\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeat_scores_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbest_fid_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFID_k\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFID_k\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_fid_k\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_fid_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mbest_fid_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFID_g\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFID_g\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_fid_g\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_fid_g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mbest_div_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDist_k\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDist_k\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_div_k\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbest_div_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_fid_k' is not defined"
     ]
    }
   ],
   "source": [
    "result_features = {\"kinetic\": [], \"manual\": []}\n",
    "real_features = {\"kinetic\": [], \"manual\": []}\n",
    "\n",
    "mean = val_loader.dataset.mean\n",
    "std = val_loader.dataset.std\n",
    "\n",
    "beat_scores_real = []\n",
    "beat_scores_pred = []\n",
    "\n",
    "for i,aist_batch in enumerate(tqdm(val_loader)):\n",
    "\n",
    "\n",
    "    mot_len = aist_batch[\"motion_lengths\"][0]\n",
    "    motion_name = aist_batch[\"names\"][0]\n",
    "\n",
    "    music_name = motion_name.split('_')[-2]\n",
    "    music_encoding=  np.load(os.path.join(audio_encoding_dir , music_name + \".npy\"))\n",
    "\n",
    "    gen_motion_indices = torch.randint(0 , 1024 , (1,1))\n",
    "\n",
    "    while gen_motion_indices.shape[1]<=seq_len:\n",
    "\n",
    "       \n",
    "        gen_motion_indices = trans_model.generate(start_tokens =gen_motion_indices.cuda(),\\\n",
    "                                                    seq_len=400 , \\\n",
    "                                                    context = torch.Tensor(music_encoding)[None,...].cuda(), \\\n",
    "                                                    context_mask=torch.ones((1 ,music_encoding.shape[0]) , dtype = torch.bool).cuda(),\\\n",
    "                                                    )\n",
    "\n",
    "        gen_motion_indices = gen_motion_indices[gen_motion_indices<1024][None,...]\n",
    "\n",
    "    try:\n",
    "        out_motion = torch.zeros((aist_batch[\"motion\"].shape[0] ,gen_motion_indices.shape[-1] , aist_batch[\"motion\"].shape[-1]))\n",
    "        for i in range(0 , seq_len, 200):\n",
    "            quant , out_motion_= vqvae_model.decode(gen_motion_indices[:,i:i+200])\n",
    "            out_motion[:,i:i+200] = out_motion_\n",
    "\n",
    "        # quant , out_motion = vqvae_model.module.decode(gen_motion_indices[:,:mot_len])\n",
    "    except:\n",
    "        # quant , out_motion = vqvae_model.decode(gen_motion_indices[:,:mot_len])\n",
    "        out_motion = torch.zeros((aist_batch[\"motion\"].shape[0] ,gen_motion_indices.shape[-1] , aist_batch[\"motion\"].shape[-1]))\n",
    "        for i in range(0 , seq_len, 200):\n",
    "            quant , out_motion_= vqvae_model.decode(gen_motion_indices[:,i:i+200])\n",
    "            out_motion[:,i:i+200] = out_motion_\n",
    "\n",
    "\n",
    "    keypoints3d_gt = recover_from_ric(aist_batch[\"motion\"][0,:mot_len] , 22).detach().cpu().numpy()\n",
    "    keypoints3d_gt = reg.transform_point_cloud(Y=keypoints3d_gt.reshape(-1,3)).reshape(-1,22,3)\n",
    "    \n",
    "    \n",
    "    keypoints3d_pred = recover_from_ric(out_motion[0,:mot_len] , 22).detach().cpu().numpy()\n",
    "    keypoints3d_pred = reg.transform_point_cloud(Y=keypoints3d_pred.reshape(-1,3)).reshape(-1,22,3)\n",
    "    \n",
    "    \n",
    "    real_features[\"kinetic\"].append(extract_feature(keypoints3d_gt, \"kinetic\"))\n",
    "    real_features[\"manual\"].append(extract_feature(keypoints3d_gt, \"manual\"))\n",
    "\n",
    "    result_features[\"kinetic\"].append(extract_feature(keypoints3d_pred, \"kinetic\"))\n",
    "    result_features[\"manual\"].append(extract_feature(keypoints3d_pred, \"manual\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    motion_beats = motion_peak_onehot(keypoints3d_gt[:mot_len])\n",
    "    # get real data music beats\n",
    "    audio_name = motion_name.split(\"_\")[-2]\n",
    "\n",
    "    audio_feature = np.load(os.path.join(audio_feature_dir, f\"{audio_name}.npy\"))\n",
    "    audio_beats = audio_feature[:mot_len, -1] # last dim is the music beats\n",
    "    # get beat alignment scores\n",
    "    beat_score = alignment_score(audio_beats, motion_beats, sigma=1)\n",
    "    beat_scores_real.append(beat_score)\n",
    "\n",
    "\n",
    "    motion_beats = motion_peak_onehot(keypoints3d_pred[:mot_len])\n",
    "    beat_score_pred = alignment_score(audio_beats, motion_beats, sigma=1)\n",
    "    beat_scores_pred.append(beat_score_pred)\n",
    "\n",
    "\n",
    "FID_k, Dist_k = calculate_frechet_feature_distance(real_features[\"kinetic\"], result_features[\"kinetic\"])\n",
    "FID_g, Dist_g = calculate_frechet_feature_distance(real_features[\"manual\"], result_features[\"manual\"])\n",
    "\n",
    "\n",
    "print(\"FID_k: \",FID_k,\"Diversity_k:\",Dist_k)\n",
    "print(\"FID_g: \",FID_g,\"Diversity_g:\",Dist_g)\n",
    "\n",
    "\n",
    "print (\"\\nBeat score on real data: %.3f\\n\" % (np.mean(beat_scores_real)))\n",
    "print (\"\\nBeat score on generated data: %.3f\\n\" % (np.mean(beat_scores_pred)))\n",
    "\n",
    "best_fid_k = FID_k if FID_k < best_fid_k else best_fid_k\n",
    "best_fid_g = FID_g if FID_g < best_fid_g else best_fid_g\n",
    "best_div_k = Dist_k if Dist_k > best_div_k else best_div_k\n",
    "best_div_g = Dist_g if Dist_g > best_div_g else best_div_g\n",
    "\n",
    "best_beat_align = np.mean(beat_scores_real) if np.mean(beat_scores_real) > best_beat_align else best_beat_align \n",
    "\n",
    "\n",
    "\n",
    "return best_fid_k, best_fid_g,best_div_k,best_div_g,best_beat_align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9c831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a03868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce64cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56400896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad058c8",
   "metadata": {},
   "source": [
    "## Sinusoidal pos emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f0b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f2797d1",
   "metadata": {},
   "source": [
    "FID_k:  3.8717503038434415 Diversity_k: 10.580599220288105\n",
    "FID_g:  14.323117971786502 Diversity_g: 7.208645957555526\n",
    "\n",
    "Beat score on real data: 0.244\n",
    "\n",
    "\n",
    "Beat score on generated data: 0.196"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8adb76",
   "metadata": {},
   "source": [
    "### Albi\n",
    "\n",
    "FID_k:  5.974822501678858 Diversity_k: 9.894211104435799\n",
    "FID_g:  10.945797702730552 Diversity_g: 7.33098736114991\n",
    "\n",
    "Beat score on real data: 0.244\n",
    "\n",
    "\n",
    "Beat score on generated data: 0.207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ea200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c4a7a42",
   "metadata": {},
   "source": [
    "## Style motion generative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f59388c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_music import evaluate_music_motion_generative_style, evaluate_music_motion_generative_style2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5618bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:01<00:00, 1390.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3d92e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=torch.device('cuda'), jit=False)  # Must set jit=False for training\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fe653da",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec_style = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/generator/var_len/trans_768_768_albi_aist_style/var_len_768_768_aist_style.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3341b8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([95000.])\n"
     ]
    }
   ],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "\n",
    "cfg_trans = get_cfg_defaults()\n",
    "cfg_trans.merge_from_file(encodec_style)\n",
    "\n",
    "\n",
    "\n",
    "trans_model = MotionRegressorModel(args = cfg_trans.motion_trans,pad_value=1025 ).eval()\n",
    "pkg_trans = torch.load(f\"{os.path.dirname(encodec_style)}/trans_motion.pt\", map_location = 'cpu')\n",
    "print(pkg_trans[\"steps\"])\n",
    "trans_model.load_state_dict(pkg_trans[\"model\"])\n",
    "trans_model =trans_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "edcac810",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_music_motion_generative_style' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-e4704d46183b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_music_motion_generative_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maist_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvqvae_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvqvae_model\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_music_motion_generative_style' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_music_motion_generative_style(aist_loader , vqvae_model= vqvae_model ,net = trans_model,clip_model=clip_model,seq_len = 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "385cb921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:22<00:00, 84.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49d861f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:21<00:00, 37.43it/s]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.46it/s]]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.45it/s]t]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.42it/s]t]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.37it/s]t]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.33it/s]t]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.35it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.35it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.34it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.25it/s]]  \n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.23it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.33it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.29it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.32it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.26it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.31it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.26it/s]]  \n",
      "100%|██████████| 800/800 [00:21<00:00, 37.30it/s]it]\n",
      "100%|██████████| 800/800 [00:21<00:00, 37.27it/s]it]\n",
      " 35%|███▌      | 673/1910 [18:37<34:14,  1.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  3.522845989252062 Diversity_k: 10.215643210963504\n",
      "FID_g:  8.57888238870914 Diversity_g: 7.215440534382331\n",
      "\n",
      "Beat score on real data: 0.172\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.522845989252062,\n",
       " 8.57888238870914,\n",
       " 10.215643210963504,\n",
       " 7.215440534382331,\n",
       " 0.1715415597282555)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_music_motion_generative_style2(aist_loader , vqvae_model= vqvae_model ,net = trans_model,clip_model=clip_model,seq_len = 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08c3ecbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25995348837209303"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.243*0.184 / 0.172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35535c27",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b9964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoding_dir = \"/srv/scratch/sanisetty3/music_motion/AIST/music\"\n",
    "\n",
    "genre_dict = {\n",
    "\"mBR\" : \"Break\",\n",
    "\"mPO\" : \"Pop\",\n",
    "\"mLO\" : \"Lock\",\n",
    "\"mMH\" : \"Middle Hip-hop\",\n",
    "\"mLH\" : \"LA style Hip-hop\",\n",
    "\"mHO\" : \"House\",    \n",
    "\"mWA\" : \"Waack\",\n",
    "\"mKR\" : \"Krump\",\n",
    "\"mJS\" : \"Street Jazz\",\n",
    "\"mJB\" : \"Ballet Jazz\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4899a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7de8c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ballet Jazz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i,aist_batch in enumerate(tqdm(aist_loader)):\n",
    "    break\n",
    "motion_name = aist_batch[\"names\"][0]\n",
    "\n",
    "music_name = motion_name.split('_')[-2]\n",
    "music_encoding=  np.load(os.path.join(audio_encoding_dir , music_name + \".npy\"))\n",
    "\n",
    "print(genre_dict.get(music_name[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb7e9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ballet Jazz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:07<00:00, 56.84it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mot_len = aist_batch[\"motion_lengths\"][0]\n",
    "motion_name = aist_batch[\"names\"][0]\n",
    "\n",
    "music_name = motion_name.split('_')[-2]\n",
    "music_encoding=  np.load(os.path.join(audio_encoding_dir , music_name + \".npy\"))\n",
    "\n",
    "print(genre_dict.get(music_name[:3]))\n",
    "\n",
    "genre = (genre_dict.get(music_name[:3])) if style is None else style\n",
    "\n",
    "text = clip.tokenize([genre], truncate=True).cuda()\n",
    "style_embeddings = clip_model.encode_text(text).cpu().float().reshape(-1) if clip_model is not None else None\n",
    "gen_motion_indices = torch.randint(0 , 1024 , (1,1))\n",
    "gen_motion_indices = trans_model.generate(start_tokens =gen_motion_indices.cuda(),\\\n",
    "                                        seq_len=400 , \\\n",
    "                                        context = torch.Tensor(music_encoding)[None,...].cuda(), \\\n",
    "                                        context_mask=torch.ones((1 ,music_encoding.shape[0]) , dtype = torch.bool).cuda(),\\\n",
    "                                        style_context = torch.Tensor(style_embeddings.reshape(-1))[None,...].cuda(),\n",
    "                                        )\n",
    "gen_motion_indices = gen_motion_indices[gen_motion_indices<1024][None,...]\n",
    "\n",
    "quant , out_motion = vqvae_model.decode(gen_motion_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbb48571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 147, 263])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aist_batch[\"motion\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73af7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slow'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0df6c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(aist_batch[\"motion\"][0:1].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"style_gt\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87719b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(out_motion[:,:mot_len].detach().cpu(),mean = aist_ds.mean , std = aist_ds.std), \"style_none\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/style/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32409bf",
   "metadata": {},
   "source": [
    "### Music VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d272e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_path_mix = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/checkpoints/vqvae_motion.295000.pt\"\n",
    "load_path_hml = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768/vqvae_motion.pt\"\n",
    "load_path_aist = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9109ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec336a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from utils.eval_music import evaluate_music_motion_vqvae\n",
    "\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "load_path = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_mix/vqvae_motion_best_fid.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570c7d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([295000.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"{load_path_mix}\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e0e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ead9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1496.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"test\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db9b4276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained only t2m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:41<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  3.2996737750179364 Diversity_k: 10.26604298215646\n",
      "FID_g:  10.78302279919913 Diversity_g: 7.181474344852643\n",
      "FID_k_real:  -7.86550347697812e-06 Diversity_k_real: 10.195780532558759\n",
      "FID_g_real:  -1.9184653865522705e-13 Diversity_g_real: 7.348854861503992\n",
      "\n",
      "Beat score on real data: 0.245\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.2996737750179364,\n",
       " 10.78302279919913,\n",
       " 10.26604298215646,\n",
       " 7.181474344852643,\n",
       " 0.24494051462936942)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pretrained only t2m\")\n",
    "evaluate_music_motion_vqvae(aist_loader,vqvae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cee809d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(\"pretrained mix\")\n",
    "# best_fid_k = []\n",
    "# best_fid_g = []\n",
    "# best_div_k = []\n",
    "# best_div_g = []\n",
    "# best_beat_align = []\n",
    "\n",
    "# for i in range(20):\n",
    "\n",
    "#     a,b,c,d,e = evaluate_music_motion_vqvae(aist_loader,vqvae_model)\n",
    "#     best_fid_k.append(a)\n",
    "#     best_fid_g.append(b)\n",
    "#     best_div_k.append(c)\n",
    "#     best_div_g.append(d)\n",
    "#     best_beat_align.append(e)\n",
    "\n",
    "    \n",
    "# print(\"best_fid_k\" , np.mean(best_fid_k))\n",
    "# print(\"best_fid_g\" , np.mean(best_fid_g))\n",
    "# print(\"best_div_k\" , np.mean(best_div_k))\n",
    "# print(\"best_div_g\" , np.mean(best_div_g))\n",
    "# print(\"best_beat_align\" , np.mean(best_beat_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "905d603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:39<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  2.635362356342995 Diversity_k: 10.163608189500295\n",
      "FID_g:  7.295345718653849 Diversity_g: 7.234946262225127\n",
      "FID_k_real:  -7.757004908626186e-06 Diversity_k_real: 10.205963216454554\n",
      "FID_g_real:  -1.903529778246593e-09 Diversity_g_real: 7.344472836225461\n",
      "\n",
      "Beat score on real data: 0.244\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.635362356342995,\n",
       " 7.295345718653849,\n",
       " 10.163608189500295,\n",
       " 7.234946262225127,\n",
       " 0.244130060202235)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"mix\")\n",
    "evaluate_music_motion_vqvae(aist_loader,vqvae_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5165ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "727139c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained only t2m, finetuned only aist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:01<00:37,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-88106f13813e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pretrained only t2m, finetuned only aist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_music_motion_vqvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maist_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvqvae_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/eval_music.py\u001b[0m in \u001b[0;36mevaluate_music_motion_vqvae\u001b[0;34m(val_loader, net, audio_feature_dir, best_fid_k, best_fid_g, best_div_k, best_div_g, best_beat_align)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mkeypoints3d_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecover_from_ric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_motion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmot_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mreal_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mreal_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/coc/scratch/sanisetty3/music_motion/motion_vqvae/utils/aist_metrics/calculate_fid_scores.py\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(keypoints3d, mode)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"kinetic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_kinetic_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"manual\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_manual_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/kinetic.py\u001b[0m in \u001b[0;36mextract_kinetic_features\u001b[0;34m(positions)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_kinetic_energy_horizontal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_kinetic_energy_vertical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_energy_expenditure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             ]\n\u001b[1;32m     47\u001b[0m         )\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/kinetic.py\u001b[0m in \u001b[0;36maverage_energy_expenditure\u001b[0;34m(self, joint)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             val += feat_utils.calc_average_acceleration(\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             )\n\u001b[1;32m    110\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/aist_plusplus/features/utils.py\u001b[0m in \u001b[0;36mcalc_average_acceleration\u001b[0;34m(positions, i, joint_idx, sliding_window, frame_time)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoint_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mframe_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0maverage_acceleration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mframe_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mcurrent_window\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_acceleration\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcurrent_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"pretrained only t2m, finetuned only aist\")\n",
    "evaluate_music_motion_vqvae(aist_loader,vqvae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26b752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b755169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f091bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [29:56<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID_k:  0.010750366559051372 Diversity_k: 9.172959109891172\n",
      "FID_g:  1.2350136226828567 Diversity_g: 7.381343867734843\n",
      "\n",
      "Beat score on real data: 0.249\n",
      "\n",
      "\n",
      "Beat score on generated data: 0.250\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.010750366559051372,\n",
       " 1.2350136226828567,\n",
       " 9.172959109891172,\n",
       " 7.381343867734843,\n",
       " 0.24940255611332512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Mixture\n",
    "evaluate_music_motion_vqvae(aist_loader,vqvae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cbf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929e37dd",
   "metadata": {},
   "source": [
    "## Generating token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334185f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.05, 135.15, 204.0, 127.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.11*255, 0.53*255, 0.8*255, 0.5*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d06f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039cc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cda3fd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [00:01<00:00, 1244.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aist_ds = VQFullMotionDataset(\"aist\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/AIST\" , window_size = -1)\n",
    "aist_loader = DATALoader(aist_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "128f1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1910/1910 [01:47<00:00, 17.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(aist_loader):\n",
    "    \n",
    "    n = int(batch[\"motion_lengths\"])\n",
    "    name = str(batch[\"names\"][0])\n",
    "    if n< 400:\n",
    "        ind = vqvae_model.encode(batch[\"motion\"].cuda())\n",
    "    else:\n",
    "#         ind = vqvae_model.encode(batch[\"motion\"][:,:400].cuda())\n",
    "        inds = []\n",
    "        for i in range(0 , n, 200):\n",
    "            ii = vqvae_model.encode(batch[\"motion\"][:,i:i+200].cuda())\n",
    "            inds.append(ii[0])\n",
    "#             print(ii.shape)\n",
    "        \n",
    "        ind = torch.concatenate(inds)[None,...]\n",
    "        \n",
    "#     print(ind.shape)\n",
    "    \n",
    "    np.save(os.path.join(\"/srv/scratch/sanisetty3/music_motion/AIST/joint_indices\" , name+\".npy\"),ind.cpu().numpy()[0])\n",
    "        \n",
    "#         quant , out_motion = vqvae_model.decode(ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e1e7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eefd3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_list = glob(\"/srv/scratch/sanisetty3/music_motion/AIST/joint_indices/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f23214d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(mot_list[0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a68d41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for i in mot_list:\n",
    "    lens.append(np.load(i).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef30e7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa63c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55b6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23384/23384 [08:53<00:00, 43.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motions 23384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hlm_ds = VQFullMotionDataset(\"t2m\", split = \"train\" , data_root = \"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/\" , window_size = -1)\n",
    "hlm_loader = DATALoader(hlm_ds,1,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d262e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914ddffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23384 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(hlm_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "429fe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 M003397\n"
     ]
    }
   ],
   "source": [
    "n = int(batch[\"motion_lengths\"])\n",
    "name = str(batch[\"names\"][0])\n",
    "print(n,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9daa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = vqvae_model.encode(batch[\"motion\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c5d478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 199])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06dcf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23384 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/joint_indices/007648.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7e92969d732f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/joint_indices\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/srv/share2/sanisetty3/miniconda3/envs/ai-choreo/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/joint_indices/007648.npy'"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(hlm_loader):\n",
    "    \n",
    "    n = int(batch[\"motion_lengths\"])\n",
    "    name = str(batch[\"names\"][0])\n",
    "    if n< 400:\n",
    "        ind = vqvae_model.encode(batch[\"motion\"].cuda())\n",
    "    else:\n",
    "        #ind = vqvae_model.encode(batch[\"motion\"][:,:400].cuda())\n",
    "#         out_motion = torch.zeros((batch[\"motion\"].shape[0] ,gen_motion_indices.shape[-1] , aist_batch[\"motion\"].shape[-1]))\n",
    "        inds = []\n",
    "        for i in range(0 , n, 200):\n",
    "            inds.append(vqvae_model.encode(batch[\"motion\"].cuda()))\n",
    "        \n",
    "        ind = torch.stack(inds)\n",
    "    \n",
    "    np.save(os.path.join(\"/srv/scratch/sanisetty3/music_motion/HumanML3D/HumanML3D/joint_indices\" , name+\".npy\"), ind.cpu().numpy()[0])\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14428964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "507abd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render start\n"
     ]
    }
   ],
   "source": [
    "sample_render(to_xyz(batch[\"motion\"][0:1].detach().cpu(),mean = hlm_ds.mean , std = hlm_ds.std), \"rnd_motion\" , \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/evals/decode_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc99328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a0b19e9",
   "metadata": {},
   "source": [
    "## T2M Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f912be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils_model as utils_model\n",
    "from core.datasets import dataset_TM_eval\n",
    "import utils.eval_trans as eval_trans\n",
    "from core.models.evaluator_wrapper import EvaluatorModelWrapper\n",
    "from utils.word_vectorizer import WordVectorizer\n",
    "from utils.eval_trans import evaluation_vqvae_loss,evaluation_vqvae\n",
    "from utils.eval_trans import calculate_R_precision,calculate_activation_statistics,calculate_diversity,calculate_frechet_distance\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdecf405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluation Model Wrapper (Epoch 28) Completed!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4384/4384 [02:23<00:00, 30.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4648 4648\n",
      "Pointer Pointing at 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_vectorizer = WordVectorizer('/srv/scratch/sanisetty3/music_motion/T2M-GPT/glove', 'our_vab')\n",
    "eval_wrapper = EvaluatorModelWrapper(cfg.eval_model)\n",
    "tm_eval = dataset_TM_eval.DATALoader(\"t2m\", True, 20, w_vectorizer, unit_length=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5f5af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646f4266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([275000.])\n"
     ]
    }
   ],
   "source": [
    "from configs.config import cfg, get_cfg_defaults\n",
    "from core.models.vqvae import VQMotionModel\n",
    "from core.models.motion_regressor import MotionRegressorModel\n",
    "\n",
    "load_path = \"/srv/scratch/sanisetty3/music_motion/motion_vqvae/checkpoints/var_len/vq_768_768_aist/vqvae_motion.pt\"\n",
    "cfg_vq = get_cfg_defaults()\n",
    "cfg_vq.merge_from_file(\"/srv/scratch/sanisetty3/music_motion/motion_vqvae/configs/var_len_768_768_aist_vq.yaml\")\n",
    "\n",
    "vqvae_model = VQMotionModel(cfg_vq.vqvae).eval()\n",
    "pkg = torch.load(f\"{load_path}\", map_location = 'cpu')\n",
    "print(pkg[\"steps\"])\n",
    "vqvae_model.load_state_dict(pkg[\"model\"])\n",
    "vqvae_model =vqvae_model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253031f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75682d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:43<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 0 :, FID. 0.0668, Diversity Real. 9.5584, Diversity. 9.9187, R_precision_real. [0.60193966 0.78189655 0.86810345], R_precision. [0.59439655 0.77801724 0.85991379], matching_score_real. 2.9862875124503825, matching_score_pred. 3.028119134902954\n"
     ]
    }
   ],
   "source": [
    "### Pretrained on t2m only\n",
    "metrics = evaluation_vqvae_loss(val_loader = tm_eval, net= vqvae_model,nb_iter= 0, eval_wrapper = eval_wrapper,save = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1454baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained on t2m only and finetuned on aist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:43<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 0 :, FID. 3.2204, Diversity Real. 9.3818, Diversity. 7.4288, R_precision_real. [0.59698276 0.78728448 0.86551724], R_precision. [0.43512931 0.64073276 0.75625   ], matching_score_real. 2.9870332890543443, matching_score_pred. 4.043809700012207\n"
     ]
    }
   ],
   "source": [
    "print(\"pretrained on t2m only and finetuned on aist\")\n",
    "metrics = evaluation_vqvae_loss(val_loader = tm_eval, net= vqvae_model,nb_iter= 0, eval_wrapper = eval_wrapper,save = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872905c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701a9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad4ce29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:42<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> \t Eva. Iter 0 :, FID. 0.0637, Diversity Real. 9.4620, Diversity. 9.4266, R_precision_real. [0.61616379 0.79482759 0.86982759], R_precision. [0.60172414 0.78405172 0.86077586], matching_score_real. 2.9635313979510602, matching_score_pred. 3.0240239735307366\n"
     ]
    }
   ],
   "source": [
    "## Pretrained on a mix of aist and t2m\n",
    "metrics = evaluation_vqvae_loss(val_loader = tm_eval, net= vqvae_model,nb_iter= 0, eval_wrapper = eval_wrapper,save = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fe5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4defa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1221bf6",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c048d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from render_final import render, saveSMPL\n",
    "from core.datasets.vqa_motion_dataset import TransMotionDatasetConditionalFull\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "73b9bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions_list = glob(\"/srv/scratch/sanisetty3/clean/mint/evals/eval60/hml/*.npy\" , recursive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d01b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/Std.npy\")\n",
    "mean = np.load(\"/srv/scratch/sanisetty3/music_motion/AIST/Mean.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "12ae7196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/scratch/sanisetty3/clean/mint/evals/eval60/hml'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(motions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d7fac409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gJS_sBM_cAll_d02_mJS0_ch09_mJS0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/44 [00:31<22:33, 31.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gMH_sBM_cAll_d24_mMH5_ch09_mMH5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 2/44 [01:00<21:25, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gBR_sBM_cAll_d06_mBR5_ch04_mBR5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 3/44 [01:31<20:59, 30.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gWA_sFM_cAll_d27_mWA5_ch20_mWA5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 4/44 [02:01<20:31, 30.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLO_sFM_cAll_d15_mLO4_ch21_mLO4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█▏        | 5/44 [02:30<19:37, 30.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gWA_sBM_cAll_d27_mWA2_ch04_mWA2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▎        | 6/44 [02:57<18:30, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJS_sBM_cAll_d03_mJS2_ch06_mJS2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 7/44 [03:28<18:18, 29.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gPO_sBM_cAll_d12_mPO5_ch06_mPO5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 8/44 [03:56<17:25, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLH_sBM_cAll_d16_mLH1_ch07_mLH1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 9/44 [04:23<16:39, 28.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJS_sBM_cAll_d03_mJS5_ch10_mJS5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 10/44 [04:49<15:44, 27.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gPO_sBM_cAll_d12_mPO2_ch10_mPO2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 11/44 [05:16<15:05, 27.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gMH_sBM_cAll_d22_mMH0_ch07_mMH0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 12/44 [05:44<14:44, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gHO_sBM_cAll_d19_mHO1_ch09_mHO1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 13/44 [06:12<14:19, 27.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gHO_sBM_cAll_d21_mHO3_ch10_mHO3\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 14/44 [06:38<13:43, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLH_sBM_cAll_d18_mLH2_ch09_mLH2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 15/44 [07:08<13:38, 28.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gHO_sBM_cAll_d20_mHO4_ch05_mHO4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 16/44 [07:37<13:10, 28.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gMH_sBM_cAll_d22_mMH2_ch05_mMH2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 17/44 [08:06<12:53, 28.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLO_sFM_cAll_d13_mLO5_ch06_mLO5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 18/44 [08:35<12:27, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gHO_sFM_cAll_d19_mHO2_ch03_mHO2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 19/44 [09:03<11:49, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLO_sBM_cAll_d14_mLO1_ch04_mLO1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 20/44 [09:28<11:00, 27.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gKR_sBM_cAll_d28_mKR1_ch09_mKR1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 21/44 [10:00<11:03, 28.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gKR_sFM_cAll_d30_mKR3_ch18_mKR3\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 22/44 [10:27<10:22, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJB_sBM_cAll_d08_mJB1_ch03_mJB1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 23/44 [11:01<10:26, 29.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gKR_sBM_cAll_d29_mKR4_ch08_mKR4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▍    | 24/44 [11:32<10:07, 30.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gBR_sBM_cAll_d04_mBR2_ch05_mBR2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 25/44 [12:01<09:25, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gBR_sBM_cAll_d04_mBR3_ch04_mBR3\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 26/44 [12:35<09:18, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gPO_sBM_cAll_d12_mPO4_ch09_mPO4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████▏   | 27/44 [13:03<08:33, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLH_sBM_cAll_d17_mLH5_ch09_mLH5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▎   | 28/44 [13:38<08:28, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gMH_sBM_cAll_d22_mMH1_ch08_mMH1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 29/44 [14:10<07:55, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLO_sBM_cAll_d14_mLO0_ch05_mLO0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 30/44 [14:40<07:18, 31.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJS_sBM_cAll_d02_mJS4_ch04_mJS4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 31/44 [15:07<06:27, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJB_sBM_cAll_d08_mJB0_ch05_mJB0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 32/44 [15:37<05:58, 29.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gMH_sBM_cAll_d24_mMH4_ch06_mMH4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 33/44 [16:05<05:23, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gKR_sFM_cAll_d30_mKR0_ch15_mKR0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 34/44 [16:36<04:58, 29.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gWA_sBM_cAll_d26_mWA1_ch03_mWA1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████▉  | 35/44 [17:03<04:21, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gPO_sFM_cAll_d12_mPO0_ch15_mPO0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 36/44 [17:33<03:54, 29.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gHO_sBM_cAll_d20_mHO0_ch10_mHO0\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 37/44 [18:02<03:25, 29.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gJB_sBM_cAll_d09_mJB2_ch04_mJB2\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 38/44 [18:33<02:58, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gKR_sBM_cAll_d29_mKR5_ch07_mKR5\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▊ | 39/44 [19:00<02:24, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gWA_sBM_cAll_d26_mWA4_ch10_mWA4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 40/44 [19:31<01:58, 29.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gWA_sBM_cAll_d27_mWA3_ch05_mWA3\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 41/44 [19:58<01:26, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gLH_sBM_cAll_d18_mLH3_ch06_mLH3\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 42/44 [20:33<01:01, 30.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gBR_sBM_cAll_d05_mBR1_ch05_mBR1\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 43/44 [21:03<00:30, 30.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n",
      "gBR_sBM_cAll_d06_mBR4_ch05_mBR4\n",
      "cuda:0\n",
      "./body_models/\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "Running SMPLify, it may take a few minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [21:29<00:00, 29.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 6, 1]) dict_keys(['pose', 'betas', 'cam'])\n",
      "torch.Size([1, 6890, 3, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(motions_list):\n",
    "    name =os.path.basename(i).split(\".\")[0]\n",
    "    print(name)\n",
    "    motion = torch.Tensor(np.load(i))\n",
    "    motion_xyz = to_xyz(torch.Tensor(motion) , mean= mean , std = std)\n",
    "    saveSMPL(motion_xyz[0].numpy(), outdir=os.path.dirname(i), name=name+\"smpl\", pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631e46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
